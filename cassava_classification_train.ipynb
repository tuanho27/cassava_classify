{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cassava classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "edyDFX_Ih5-j"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from urllib.request import urlretrieve\n",
    "import pandas as pd\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from  torch.cuda.amp import autocast, GradScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import resample\n",
    "cudnn.benchmark = True\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n",
    "import timm\n",
    "from timm.loss import LabelSmoothingCrossEntropy, SoftTargetCrossEntropy, JsdCrossEntropy\n",
    "from utils import Mixup, RandAugment, AsymmetricLossSingleLabel, SCELoss, fmix\n",
    "from PIL import Image\n",
    "from torchcontrib.optim import SWA\n",
    "from apex import amp\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(SEED):\n",
    "    random.seed(SEED)\n",
    "    os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] =\"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the root directory dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "waXTuqeVh5-q"
   },
   "outputs": [],
   "source": [
    "root = os.path.join(os.environ[\"HOME\"], \"Workspace/datasets/taiyoyuden/cassava\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split files from the dataset into the train and validation sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some files in the dataset are broken, so we will use only those image files that OpenCV could load correctly. We will use 20000 images for training, 4936 images for validation, and 10 images for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QgLoujrNHY5Z"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test_images',\n",
       " 'label_num_to_disease_map.json',\n",
       " 'train_1_pseudo.csv',\n",
       " 'val_1_pseudo.csv',\n",
       " 'external',\n",
       " 'sample_submission.csv',\n",
       " 'train_images',\n",
       " 'label_num_to_disease_map.json.save',\n",
       " 'train.csv']"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to visualize images and their labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a function that will take a list of images' file paths and their labels and visualize them in a grid. Correct labels are colored green, and incorrectly predicted labels are colored red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000015157.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000201771.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100042118.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000723321.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000812911.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         image_id  label\n",
       "0  1000015157.jpg      0\n",
       "1  1000201771.jpg      3\n",
       "2   100042118.jpg      1\n",
       "3  1000723321.jpg      1\n",
       "4  1000812911.jpg      3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2216849948.jpg</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000015157.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000201771.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         image_id  label\n",
       "0  2216849948.jpg      4\n",
       "1  1000015157.jpg      0\n",
       "2  1000201771.jpg      3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cassava Bacterial Blight (CBB)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cassava Brown Streak Disease (CBSD)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cassava Green Mottle (CGM)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cassava Mosaic Disease (CMD)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Healthy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     0\n",
       "0       Cassava Bacterial Blight (CBB)\n",
       "1  Cassava Brown Streak Disease (CBSD)\n",
       "2           Cassava Green Mottle (CGM)\n",
       "3         Cassava Mosaic Disease (CMD)\n",
       "4                              Healthy"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train = pd.read_csv(f'{root}/train.csv')\n",
    "train_external = pd.read_csv(f'{root}/external/train_external.csv')\n",
    "test_external = pd.read_csv(f'{root}/external/test_external.csv')\n",
    "test_external_pseudo = pd.read_csv(f'{root}/external/test_external_pseudo.csv')\n",
    "test = pd.read_csv(f'{root}/sample_submission.csv')\n",
    "label_map = pd.read_json(f'{root}/label_num_to_disease_map.json', \n",
    "                         orient='index')\n",
    "display(train.head())\n",
    "display(test.head())\n",
    "display(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7320,  910, 5440, 5241, 5784, 6315,  516, 4476, 5628, 8372, 1735,\n",
       "        819, 6999, 2483, 5361, 5101, 6470, 1234, 4605, 3435, 6446, 8716,\n",
       "       9324, 2608, 7899, 2097, 2797, 9217,  239, 2784, 3055, 4708, 1949,\n",
       "       7784, 1317, 1578, 3606, 3940, 8888, 5443, 8842, 8483, 7563, 2662,\n",
       "       7091, 9605, 6285, 5536, 7149, 9720])"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map.iloc[1].values\n",
    "np.random.randint(50, 10000, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ho-BaziAHY5q"
   },
   "outputs": [],
   "source": [
    "def visualize_input_image_grid(filepaths, image_name, labels, cols=4):\n",
    "    rows = 5\n",
    "    figure, ax = plt.subplots(nrows=rows, ncols=cols, figsize=(25, 15))\n",
    "    for i, index in enumerate(np.random.randint(0, len(image_name), 20)):\n",
    "        name = image_name.iloc[index]['image_id']\n",
    "        label =  image_name.iloc[index]['label']\n",
    "        image = cv2.imread(f'{filepaths}/train_images/{name}')\n",
    "        if (i == 0): \n",
    "            print(image.shape)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        ax.ravel()[i].imshow(image)\n",
    "        ax.ravel()[i].set_title(label, color='GREEN')\n",
    "        ax.ravel()[i].set_axis_off()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the parameters of the whole process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_name = [\"resnest26d\",\"resnest50d\",\"tf_efficientnet_b3_ns\", \"skresnet34\" ,\"cspresnet50\", \"vit_base_patch16_384\"]\n",
    "weights = [\n",
    "    \"weights/resnest26d/resnest26d_fold0_best_epoch_28_final_1st.pth\",\n",
    "    \"weights/resnest26d/resnest26d_fold0_best_epoch_13_final_mixup.pth\",\n",
    "    \"weights/resnest26d/resnest26d_fold2_best_epoch_3_final_hnm.pth\",\n",
    "    \"weights/resnest26d/resnest26d_fold4_best_epoch_29_1st.pth\",\n",
    "    \"weights/resnest26d/resnest26d_fold4_best_epoch_26_mix.pth\",\n",
    "    \"weights/resnest26d/resnest26d_fold4_best_epoch_12_cutmix.pth\",\n",
    "    \"weights/resnest26d/resnest26d_fold4_best_epoch_3_external.pth\",\n",
    "    \"weights/resnest26d/resnest26d_fold4_best_epoch_21_final_512.pth\",\n",
    "    \"weights/tf_efficientnet_b3_ns/tf_efficientnet_b3_ns_fold1_best_epoch_19_external.pth\",\n",
    "    \"weights/tf_efficientnet_b3_ns/tf_efficientnet_b3_ns_fold1_best_epoch_26_512.pth\",\n",
    "    \"weights/tf_efficientnet_b3_ns/tf_efficientnet_b3_ns_fold1_best_epoch_1_final_512.pth\",\n",
    "    \"weights/resnest50d/resnest50d_fold1_best_epoch_95_final_1st.pth\",\n",
    "#     \"weights/resnest50d/resnest50d_fold1_best_epoch_9_final_512.pth\"\n",
    "    \"weights/resnest50d/resnest50d_fold1_best_epoch_38_clean_1st.pth\",\n",
    "    \"weights/resnest50d/resnest50d_fold0_best_epoch_25.pth\",\n",
    "    \"./weights/resnest50d/resnest50d_fold2_best_epoch_50_final_1st.pth\",\n",
    "    \"./weights/resnest50d/resnest50d_fold4_best_epoch_39.pth\",\n",
    "    \"weights/resnest50d/resnest50d_fold1_best_epoch_95_final_1st.pth\",\n",
    "    \"./weights/resnest50d/resnest50d_fold1_best_epoch_17_final_2nd.pth\",\n",
    "    \"./weights/resnest50d/resnest50d_fold3_best_epoch_2_final_2nd.pth\",\n",
    "    \"./weights/resnest50d/resnest50d_fold0_best_epoch_13_final_2nd.pth\",\n",
    "]\n",
    "model_index = 1\n",
    "ckpt_index = -1\n",
    "fold_ckpt_index = [11,12]\n",
    "fold_ckpt_weight = [1,1]\n",
    "\n",
    "params = {\n",
    "    \"visualize\": False,\n",
    "    \"fold\": 0,\n",
    "    \"distill_soft_label\":True,\n",
    "    \"train_external\": True,\n",
    "    \"train_clean_only\": False,\n",
    "    \"test_external\": False,\n",
    "    \"load_pretrained\": True,\n",
    "    \"fp16\": True,\n",
    "    \"resume\": False,\n",
    "    \"image_size\": 512,\n",
    "    \"num_classes\": 5,\n",
    "    \"model\": models_name[model_index],\n",
    "    \"device\": \"cuda\",\n",
    "    \"lr\": 1e-4,\n",
    "    \"lr_min\":1e-6,\n",
    "    \"batch_size\": 8,\n",
    "    \"num_workers\": 8,\n",
    "    \"epochs\": 20,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"drop_block\": 0.2,\n",
    "    \"drop_rate\": 0.2,\n",
    "    \"mix_up\": True,\n",
    "    \"cutmix\":True,\n",
    "    \"fmix\":True,\n",
    "    \"smooth_label\": 0.1,\n",
    "    \"rand_aug\": False,\n",
    "    \"local_rank\":0,\n",
    "    \"distributed\": False,\n",
    "    \"hard_negative_sample\": False,\n",
    "    \"tta\": True,\n",
    "    \"train_phase\":True,\n",
    "    \"balance_data\":False,\n",
    "    \"kfold_pred\":False\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define dataset with KFolds strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let visualize the dataset with the defined class, the proceed the split dataset with 5 folds\n",
    "Then, we can add the external dataset to each fold and visualize the class distribution of the dataset\n",
    "\n",
    "The class dataset is defined with transform and random augmentation.\n",
    "\n",
    "`__init__` will receive an optional `transform` argument. It is a transformation function of the Albumentations augmentation pipeline. Then in `__getitem__`, the Dataset class will use that function to augment an image and return it along with the correct label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "if params['visualize']:\n",
    "    visualize_input_image_grid(root, train, label_map)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold  label\n",
      "0     0         218\n",
      "      1         438\n",
      "      2         477\n",
      "      3        2631\n",
      "      4         516\n",
      "1     0         218\n",
      "      1         438\n",
      "      2         477\n",
      "      3        2631\n",
      "      4         516\n",
      "2     0         217\n",
      "      1         438\n",
      "      2         477\n",
      "      3        2632\n",
      "      4         515\n",
      "3     0         217\n",
      "      1         438\n",
      "      2         477\n",
      "      3        2632\n",
      "      4         515\n",
      "4     0         217\n",
      "      1         437\n",
      "      2         478\n",
      "      3        2632\n",
      "      4         515\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "folds = train.copy()\n",
    "Fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "for n, (train_index, val_index) in enumerate(Fold.split(folds, folds['label'])):\n",
    "    folds.loc[val_index, 'fold'] = int(n)\n",
    "folds['fold'] = folds['fold'].astype(int)\n",
    "print(folds.groupby(['fold','label']).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>label</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000015157.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000201771.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100042118.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000723321.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000812911.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17112</th>\n",
       "      <td>998910982.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17113</th>\n",
       "      <td>999068805.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17114</th>\n",
       "      <td>999474432.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17115</th>\n",
       "      <td>999616605.jpg</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17116</th>\n",
       "      <td>999998473.jpg</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17117 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             image_id  label  fold\n",
       "0      1000015157.jpg      0     3\n",
       "1      1000201771.jpg      3     2\n",
       "2       100042118.jpg      1     2\n",
       "3      1000723321.jpg      1     1\n",
       "4      1000812911.jpg      3     2\n",
       "...               ...    ...   ...\n",
       "17112   998910982.jpg      1     1\n",
       "17113   999068805.jpg      3     1\n",
       "17114   999474432.jpg      1     1\n",
       "17115   999616605.jpg      4     2\n",
       "17116   999998473.jpg      4     4\n",
       "\n",
       "[17117 rows x 3 columns]"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold = params[\"fold\"]\n",
    "train_idx = folds[folds['fold'] != fold].index\n",
    "val_idx = folds[folds['fold'] == fold].index\n",
    "\n",
    "train_folds = folds.loc[train_idx].reset_index(drop=True)\n",
    "val_folds = folds.loc[val_idx].reset_index(drop=True)\n",
    "train_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>label</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train-cbsd-663.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train-cgm-560.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train-cmd-1576.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train-cbb-443.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train-cgm-293.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5651</th>\n",
       "      <td>train-cmd-1383.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5652</th>\n",
       "      <td>train-cmd-971.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5653</th>\n",
       "      <td>train-cbb-402.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5654</th>\n",
       "      <td>train-cbsd-1029.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5655</th>\n",
       "      <td>train-cmd-2291.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5656 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 image_id  label  fold\n",
       "0      train-cbsd-663.jpg      1     0\n",
       "1       train-cgm-560.jpg      2     0\n",
       "2      train-cmd-1576.jpg      3     0\n",
       "3       train-cbb-443.jpg      0     0\n",
       "4       train-cgm-293.jpg      2     0\n",
       "...                   ...    ...   ...\n",
       "5651   train-cmd-1383.jpg      3     0\n",
       "5652    train-cmd-971.jpg      3     0\n",
       "5653    train-cbb-402.jpg      0     0\n",
       "5654  train-cbsd-1029.jpg      1     0\n",
       "5655   train-cmd-2291.jpg      3     0\n",
       "\n",
       "[5656 rows x 3 columns]"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_external\n",
    "for idx,label in enumerate(train_external['label']):\n",
    "    train_external.loc[idx,'fold'] = fold\n",
    "train_external['fold'] = train_external['fold'].astype(int)\n",
    "train_external"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_data(df1, df2):\n",
    "    merge_df = pd.concat([df1, df2], axis=0) #,how ='outer', on ='image_id')\n",
    "    return merge_df\n",
    "\n",
    "if params[\"train_external\"]:\n",
    "    train_folds = merge_data(train_folds, train_external)\n",
    "if params[\"train_clean_only\"]:\n",
    "    train_folds = train_external\n",
    "#     train_folds = pd.read_csv(f'{root}/train_{params[\"fold\"]}_pseudo.csv')\n",
    "#     val_folds = pd.read_csv(f'{root}/val_{params[\"fold\"]}_pseudo.csv')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>label</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000015157.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000201771.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100042118.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000723321.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000812911.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5651</th>\n",
       "      <td>train-cmd-1383.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5652</th>\n",
       "      <td>train-cmd-971.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5653</th>\n",
       "      <td>train-cbb-402.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5654</th>\n",
       "      <td>train-cbsd-1029.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5655</th>\n",
       "      <td>train-cmd-2291.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22773 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 image_id  label  fold\n",
       "0          1000015157.jpg      0     3\n",
       "1          1000201771.jpg      3     2\n",
       "2           100042118.jpg      1     2\n",
       "3          1000723321.jpg      1     1\n",
       "4          1000812911.jpg      3     2\n",
       "...                   ...    ...   ...\n",
       "5651   train-cmd-1383.jpg      3     0\n",
       "5652    train-cmd-971.jpg      3     0\n",
       "5653    train-cbb-402.jpg      0     0\n",
       "5654  train-cbsd-1029.jpg      1     0\n",
       "5655   train-cmd-2291.jpg      3     0\n",
       "\n",
       "[22773 rows x 3 columns]"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0     1087\n",
      "1     2189\n",
      "2     2386\n",
      "3    13158\n",
      "4     2577\n",
      "Name: image_id, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0     466\n",
      "1    1443\n",
      "2     773\n",
      "3    2658\n",
      "4     316\n",
      "Name: image_id, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0     1335\n",
      "1     3194\n",
      "2     2682\n",
      "3    13185\n",
      "4     2377\n",
      "Name: image_id, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def visualize_class_dis(df, fold, color='maroon'):\n",
    "    x = df.groupby('label').count()\n",
    "    print(x['image_id'])\n",
    "    y = [0,1,2,3,4]\n",
    "\n",
    "    fig = plt.figure(figsize=(10,5))\n",
    "    plt.bar(y, x['image_id'].values, color =color, width = 0.4)\n",
    "    plt.xlabel(\"Label\") \n",
    "    plt.ylabel(\"Number of image\") \n",
    "    plt.title(f\"Class distribution of data fold: {fold}\") \n",
    "\n",
    "    # plt.xticks(x['image_id'])\n",
    "    plt.show()\n",
    "visualize_class_dis(train, 'all data', color='blue')\n",
    "visualize_class_dis(train_external, 'external data', color='purple')\n",
    "visualize_class_dis(train_folds, params['fold'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "if params[\"test_external\"]:\n",
    "    train_folds = merge_data(train_folds, test_external_pseudo)\n",
    "    visualize_class_dis(train_folds, f'fold {fold} add extra pseudo test external data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_data(df, mode=\"undersampling\", val=False):\n",
    "    class_0 = df[df.label==0]\n",
    "    class_1 = df[df.label==1]\n",
    "    class_2 = df[df.label==2]\n",
    "    class_3 = df[df.label==3]\n",
    "    class_4 = df[df.label==4]\n",
    "    if mode == \"undersampling\":\n",
    "        # upsample minority\n",
    "        class_3_downsampled = resample(class_3,\n",
    "                                  replace=True, # sample with replacement\n",
    "                                  n_samples=int(len(class_3)*2/3), # match number in majority class\n",
    "                                  random_state=27) # reproducible results\n",
    "        if val:\n",
    "            return  pd.concat([class_0, class_1, class_2, class_3_downsampled, class_4]) \n",
    "    \n",
    "        class_1_downsampled = resample(class_1,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=int(len(class_1)*0.7), # match number in majority class\n",
    "                          random_state=27) # reproducible results\n",
    "        class_4_upsampled = resample(class_4,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=int(len(class_4)*1.3), # match number in majority class\n",
    "                          random_state=27) # reproducible results\n",
    "        return pd.concat([class_0, class_1_downsampled, class_2, class_3_downsampled, class_4_upsampled]) \n",
    "    else:\n",
    "        class_0_upsampled = resample(class_0,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=int(len(class_3)/4), # match number in majority class\n",
    "                          random_state=27) # reproducible results\n",
    "        class_1_upsampled = resample(class_1,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=int(len(class_3)/3), # match number in majority class\n",
    "                          random_state=27) # reproducible results\n",
    "        class_2_upsampled = resample(class_2,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=int(len(class_3)/3), # match number in majority class\n",
    "                          random_state=27) # reproducible results\n",
    "        class_4_upsampled = resample(class_4,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=int(len(class_3)/3), # match number in majority class\n",
    "                          random_state=27) # reproducible results\n",
    "        return pd.concat([class_0_upsampled, class_1_upsampled, class_2_upsampled, class_3, class_4_upsampled]) \n",
    "\n",
    "    \n",
    "if params[\"balance_data\"]:\n",
    "    train_folds = balance_data(train_folds, mode=\"undersampling\")    \n",
    "    val_folds = balance_data(val_folds, mode=\"undersampling\", val=True)    \n",
    "    \n",
    "#     train_folds = balance_data(train_folds, mode=\"oversamling\")\n",
    "    visualize_class_dis(train_folds, f'fold {fold} after balance data')\n",
    "    visualize_class_dis(val_folds, f'fold {fold} after balance data')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if params[\"distill_soft_label\"]:\n",
    "    for fold_idx in range(5):\n",
    "        if fold_idx==0:\n",
    "            soft_target_distill = pd.read_csv(f'./error_analysis/val_{params[\"model\"]}_{fold_idx}_pred.csv')\n",
    "        else:\n",
    "            soft_target_distill = merge_data(soft_target_distill,  pd.read_csv(f'./error_analysis/val_{params[\"model\"]}_{fold_idx}_pred.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9EtOPkNwh5-8"
   },
   "outputs": [],
   "source": [
    "# Dataset\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, df, transform=None, mosaic_mix = False):\n",
    "        self.df = df\n",
    "        self.file_names = df['image_id'].values\n",
    "        self.labels = df['label'].values\n",
    "        self.transform = transform\n",
    "        self.mosaic_mix = mosaic_mix\n",
    "        self.rand_aug_fn = RandAugment()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_name = self.file_names[idx]\n",
    "        file_path = f'{root}/train_images/{file_name}'\n",
    "        image = cv2.imread(file_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        label = torch.tensor(self.labels[idx]).long()\n",
    "        if params[\"rand_aug\"]:\n",
    "            image = np.array(self.rand_aug_fn(Image.fromarray(image)))\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "\n",
    "        return image, label, file_name\n",
    "    \n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, df, transform=None, valid_test=False, fcrops=False):\n",
    "        self.df = df\n",
    "        self.file_names = df['image_id'].values\n",
    "        self.transform = transform\n",
    "        self.valid_test = valid_test\n",
    "        self.fcrops = fcrops\n",
    "        if self.valid_test:\n",
    "            self.labels = df['label'].values  \n",
    "        else:\n",
    "            assert ValueError(\"Test data does not have annotation, plz check!\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_name = self.file_names[idx]\n",
    "        if self.valid_test:\n",
    "            file_path = f'{root}/train_images/{file_name}'\n",
    "            #file_path = f'{root}/external/extraimages/{file_name}'\n",
    "        else:\n",
    "            file_path = f'{root}/test_images/{file_name}'\n",
    "        image = cv2.imread(file_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        if isinstance(self.transform, list):\n",
    "            outputs = {'images':[],\n",
    "                       'labels':[],\n",
    "                       'image_ids':[]}\n",
    "            if self.fcrops:\n",
    "                for trans in self.transform:\n",
    "                    image_aug = transforms.ToPILImage()(image)\n",
    "                    image_aug = trans(image_aug)\n",
    "                    outputs[\"images\"].append(image_aug)\n",
    "                    del image_aug\n",
    "            else:\n",
    "                for trans in self.transform:\n",
    "                    augmented = trans(image=image)\n",
    "                    image_aug = augmented['image']\n",
    "                    outputs[\"images\"].append(image_aug)\n",
    "                    del image_aug\n",
    "\n",
    "            if self.valid_test:\n",
    "                label = torch.tensor(self.labels[idx]).long()\n",
    "                outputs['labels'] = len(self.transform)*[label]\n",
    "                outputs['image_ids'].append(file_name)\n",
    "                \n",
    "            else:\n",
    "                outputs['labels'] = len(self.transform)*[-1]\n",
    "                \n",
    "            return outputs\n",
    "        else:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image'] \n",
    "        return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Albumentations to define transformation functions for the train and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mm_NP-c4h5-_"
   },
   "outputs": [],
   "source": [
    "train_transform = A.Compose(\n",
    "    [\n",
    "        A.RandomResizedCrop(height=params[\"image_size\"], width=params[\"image_size\"], p=1),\n",
    "        A.OneOf([\n",
    "            A.RandomRotate90(p=0.5),\n",
    "            A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),], p=1.\n",
    "        ),\n",
    "#         A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.IAAAffine(rotate=0.2, shear=0.2,p=0.5),\n",
    "        A.CoarseDropout(max_holes=20, max_height=int(params[\"image_size\"]/15), max_width=int(params[\"image_size\"]/15), p=0.5),\n",
    "#         A.IAAAdditiveGaussianNoise(p=1.),\n",
    "        A.MedianBlur(p=0.5),\n",
    "        A.Equalize(p=0.2),\n",
    "        A.GridDistortion(p=0.2),\n",
    "#         A.RandomGridShuffle(grid=(100, 100), p=0.5),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "val_transform = A.Compose(\n",
    "    [\n",
    "        A.CenterCrop(height=params[\"image_size\"], width=params[\"image_size\"], p=1),\n",
    "        A.Resize(params[\"image_size\"],params[\"image_size\"]),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "if params[\"cutmix\"]:\n",
    "    mixup_fn = Mixup(mixup_alpha=1., cutmix_alpha=1., label_smoothing=params[\"smooth_label\"], num_classes=params[\"num_classes\"])\n",
    "else:\n",
    "    mixup_fn = Mixup(mixup_alpha=1., label_smoothing=params[\"smooth_label\"], num_classes=params[\"num_classes\"])\n",
    "\n",
    "train_dataset = TrainDataset(train_folds, transform=train_transform)\n",
    "val_dataset = TrainDataset(val_folds, transform=val_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also let's define a function that takes a dataset and visualizes different augmentations applied to the same image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_augmentations(dataset, idx=3, samples=10, cols=5):\n",
    "    dataset = copy.deepcopy(dataset)\n",
    "    dataset.transform = A.Compose([t for t in dataset.transform if not isinstance(t, (A.Normalize, ToTensorV2))])\n",
    "    rows = samples // cols\n",
    "    figure, ax = plt.subplots(nrows=rows, ncols=cols, figsize=(12, 6))\n",
    "    for i in range(samples):\n",
    "        image, _, _ = dataset[idx]\n",
    "        ax.ravel()[i].imshow(image)\n",
    "        ax.ravel()[i].set_axis_off()\n",
    "    plt.tight_layout()\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "if params[\"visualize\"]:\n",
    "    random.seed(SEED)\n",
    "    visualize_augmentations(train_dataset, idx=random.randint(1,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnNormalize(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
    "        Returns:\n",
    "            Tensor: Normalized image.\n",
    "        \"\"\"\n",
    "        for t, m, s in zip(tensor, self.mean, self.std):\n",
    "            t.mul_(s).add_(m)\n",
    "            # The normalize code -> t.sub_(m).div_(s)\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Mixup CutMix Aug\n",
    "def visualize_mix_augmentations(dataset, idx=[4,5,1000,3], cols=4):\n",
    "    dataset = copy.deepcopy(dataset)\n",
    "    rows = 1\n",
    "    figure, ax = plt.subplots(nrows=rows, ncols=cols, figsize=(20, 15))\n",
    "    images = []\n",
    "    labels = []\n",
    "    for i in idx:\n",
    "        image, label, _ = dataset[i]\n",
    "        images.append(image)\n",
    "        labels.append(label)\n",
    "        \n",
    "    images, labels = mixup_fn(torch.stack(images).cuda(), torch.stack(labels).float().cuda())\n",
    "    for i, (image,label) in enumerate(zip(images, labels)):\n",
    "        unorm = UnNormalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "        image = (unorm(image).cpu().numpy()*255).astype(int)\n",
    "        ax.ravel()[i].imshow(image.transpose(1,2,0))\n",
    "        ax.ravel()[i].set_title(label, color='GREEN')\n",
    "        ax.ravel()[i].set_axis_off()\n",
    "    plt.tight_layout()\n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "if params[\"visualize\"] and params[\"mix_up\"]:\n",
    "    random.seed(SEED)\n",
    "    visualize_mix_augmentations(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define helpers for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a few helpers for our training pipeline. `calculate_accuracy` takes model predictions and true labels and will return accuracy for those predictions. `MetricMonitor` helps to track metrics such as accuracy or loss during training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bgTqE0eYSjDh"
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(output, target):\n",
    "#     return torch.true_divide((target == output).sum(dim=0), output.size(0)).item()\n",
    "    if params[\"mix_up\"]:\n",
    "        output = torch.argmax(torch.softmax(output, dim=1), dim=1)\n",
    "#         return accuracy_score(output.cpu(), target.argmax(1).cpu())\n",
    "        return accuracy_score(output.cpu(), target.cpu())\n",
    "    \n",
    "    output = torch.softmax(output, dim=1)\n",
    "    return accuracy_score(output.argmax(1).cpu(), target.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zRhPJiCch5_D"
   },
   "outputs": [],
   "source": [
    "class MetricMonitor:\n",
    "    def __init__(self, float_precision=3):\n",
    "        self.float_precision = float_precision\n",
    "        self.reset()\n",
    "        self.curr_acc = 0.\n",
    "    def reset(self):\n",
    "        self.metrics = defaultdict(lambda: {\"val\": 0, \"count\": 0, \"avg\": 0})\n",
    "\n",
    "    def update(self, metric_name, val):\n",
    "        metric = self.metrics[metric_name]\n",
    "\n",
    "        metric[\"val\"] += val\n",
    "        metric[\"count\"] += 1\n",
    "        metric[\"avg\"] = metric[\"val\"] / metric[\"count\"]\n",
    "        self.curr_acc = metric[\"avg\"]\n",
    "    def __str__(self):\n",
    "        return \" | \".join(\n",
    "            [\n",
    "                \"{metric_name}: {avg:.{float_precision}f}\".format(\n",
    "                    metric_name=metric_name, avg=metric[\"avg\"], float_precision=self.float_precision\n",
    "                )\n",
    "                for (metric_name, metric) in self.metrics.items()\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "if params[\"hard_negative_sample\"]:\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=1, shuffle=True, num_workers=params[\"num_workers\"], pin_memory=True,\n",
    "    )\n",
    "else:\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=params[\"batch_size\"], shuffle=True, num_workers=params[\"num_workers\"], pin_memory=True,\n",
    "    )\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=params[\"batch_size\"], shuffle=False, num_workers=params[\"num_workers\"], pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_hard_sample(train_loader, model, val_criterion, thres):\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(12, 6))\n",
    "    train_loss_list = {'image_id':[],\n",
    "                       'label':[],\n",
    "                       'loss':[],\n",
    "                       'fold':[]}\n",
    "    model.eval()\n",
    "    stream = tqdm(train_loader)\n",
    "    with torch.no_grad():\n",
    "        for i, (images, target, name) in enumerate(stream, start=1):\n",
    "            images = images.to(params[\"device\"], non_blocking=True)\n",
    "            target = target.to(params[\"device\"], non_blocking=True)#.view(-1,params['batch_size'])\n",
    "            output = model(images)\n",
    "            loss = val_criterion(output, target)\n",
    "            if loss > thres:\n",
    "                train_loss_list['image_id'].append(name[0])\n",
    "                train_loss_list['label'].append(int(target[0].cpu().numpy()))\n",
    "                train_loss_list['loss'].append(loss)\n",
    "                train_loss_list['fold'].append(params['fold'])\n",
    "    print(\"Number hard samples:\",len(train_loss_list[\"loss\"]))\n",
    "    #visualize\n",
    "    ax.ravel()[0].plot(train_loss_list['loss'])\n",
    "    ax.ravel()[0].set_title(\"Loss\", color='BLUE')\n",
    "    #ax.ravel()[0].set_axis_off() \n",
    "    \n",
    "    ax.ravel()[1].plot(sorted(train_loss_list['loss']))\n",
    "    ax.ravel()[1].set_title(\"Sort Curve\", color='BLUE')\n",
    "    #ax.ravel()[1].set_axis_off() \n",
    "    \n",
    "    prob = 10 / (abs(sorted(train_loss_list['loss']) - sorted(train_loss_list['loss']).mean()) + 10)\n",
    "    ax.ravel()[2].plot(prob)\n",
    "    ax.ravel()[2].set_title(\"Sort reshape Curve\", color='GREEN')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()  \n",
    "        \n",
    "    return dict(image_id=train_loss_list['image_id'],\n",
    "                label=train_loss_list['label'],\n",
    "                fold=train_loss_list['fold'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define training parameters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a few training parameters such as model architecture, learning rate, batch size, epochs, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def declare_model(name, load_pretrained=False, weight=None):\n",
    "    model = timm.create_model(name,\n",
    "            pretrained=False,\n",
    "            num_classes=params[\"num_classes\"],\n",
    "            drop_rate=params[\"drop_rate\"])\n",
    "    model = model.to(params[\"device\"])\n",
    "    \n",
    "    if params[\"distributed\"]:\n",
    "        assert ValueError(\"No need to implement in a single machine\")\n",
    "    else:\n",
    "        model = torch.nn.DataParallel(model) \n",
    "        \n",
    "    if load_pretrained:\n",
    "        state_dict = torch.load(weight)\n",
    "        print(f\"Load pretrained model: {name} \",state_dict[\"preds\"])\n",
    "        model.load_state_dict(state_dict[\"model\"])\n",
    "        best_acc = state_dict[\"preds\"]   \n",
    "    return model \n",
    "model = declare_model(params[\"model\"], params[\"load_pretrained\"], WEIGHTS[ckpt_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_features = model.fc.in_features\n",
    "# model.fc = nn.Linear(n_features, params['num_classes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create all required objects and functions for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W7HipbJ8h5_I"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pretrained model:  0.8855\n"
     ]
    }
   ],
   "source": [
    "# model = getattr(models, params[\"model\"])(pretrained=False, num_classes=5)\n",
    "model = model.to(params[\"device\"])\n",
    "val_criterion = nn.CrossEntropyLoss().to(params[\"device\"])\n",
    "criterion_fmix = nn.CrossEntropyLoss().to(params[\"device\"])\n",
    "criterion = LabelSmoothingCrossEntropy().to(params[\"device\"])\n",
    "if params[\"mix_up\"]:\n",
    "    criterion = SoftTargetCrossEntropy().to(params[\"device\"])\n",
    "asymetric_criterion = AsymmetricLossSingleLabel().to(params[\"device\"])\n",
    "symetric_criterion = SCELoss(smooth_label=params[\"smooth_label\"]).to(params[\"device\"])\n",
    "    \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=params[\"lr\"])\n",
    "# scheduler = CosineAnnealingLR(optimizer, T_max=10, eta_min=params[\"lr_min\"], last_epoch=-1)\n",
    "scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=1, eta_min=params[\"lr_min\"], last_epoch=-1)\n",
    "\n",
    "if params[\"fp16\"]:\n",
    "    model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\",verbosity=0)\n",
    "\n",
    "if params[\"distributed\"]:\n",
    "    assert ValueError(\"No need to implement in a single machine\")\n",
    "else:\n",
    "    model = torch.nn.DataParallel(model)    \n",
    "if params[\"load_pretrained\"]:\n",
    "    state_dict = torch.load(weights[ckpt_index])\n",
    "    print(\"Load pretrained model: \",state_dict[\"preds\"])\n",
    "    model.load_state_dict(state_dict[\"model\"])\n",
    "    best_acc = state_dict[\"preds\"]\n",
    "    # Hard negative mining based on train data and pretrained model on that data\n",
    "    if params[\"hard_negative_sample\"]:\n",
    "        update_train_data = update_hard_sample(train_loader, model, val_criterion, thres=0.2)\n",
    "        update_train_folds = pd.DataFrame(data=update_train_data)\n",
    "        update_train_folds = pd.concat(5*[update_train_folds])\n",
    "        #check the update distribution when filter data\n",
    "        print(\"Class distribution for the new data\")\n",
    "        visualize_class_dis(update_train_folds, params[\"fold\"])\n",
    "        \n",
    "        #update the training set\n",
    "        update_train_dataset = TrainDataset(update_train_folds, transform=train_transform)\n",
    "        update_train_loader = DataLoader(\n",
    "            update_train_dataset, batch_size=params[\"batch_size\"], shuffle=True, num_workers=params[\"num_workers\"], pin_memory=True,\n",
    "        )                \n",
    "else:\n",
    "    best_acc = 0.85\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HNz-5F7Bh5_Y"
   },
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch, params):\n",
    "    metric_monitor = MetricMonitor()\n",
    "    model.train()\n",
    "    if params[\"hard_negative_sample\"]:\n",
    "        stream = tqdm(update_train_loader)\n",
    "    else:\n",
    "        stream = tqdm(train_loader)\n",
    "    for i, (images, target, _) in enumerate(stream, start=1):\n",
    "        images = images.to(params[\"device\"]) #, non_blocking=True)\n",
    "        target = target.to(params[\"device\"]) #, non_blocking=True) #.view(-1,params['batch_size'])\n",
    "        if params[\"mix_up\"]:\n",
    "            images , target = mixup_fn(images, target)\n",
    "        output = model(images)\n",
    "        if isinstance(output, (tuple, list)):\n",
    "            output = output[0]\n",
    "        loss = criterion(output, target)\n",
    "        if params['gradient_accumulation_steps'] > 1:\n",
    "            loss = loss / params['gradient_accumulation_steps']\n",
    "    \n",
    "        accuracy = calculate_accuracy(output, target)\n",
    "        metric_monitor.update(\"Loss\", loss.item())\n",
    "        metric_monitor.update(\"Accuracy\", accuracy)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "    \n",
    "        optimizer.step()\n",
    "        stream.set_description(\n",
    "            \"Epoch: {epoch}. Train.      {metric_monitor}\".format(epoch=epoch, metric_monitor=metric_monitor)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "chKMqryvh5_a"
   },
   "outputs": [],
   "source": [
    "def validate(val_loader, model, criterion, epoch, params, fold, best_acc):\n",
    "    metric_monitor = MetricMonitor()\n",
    "    model.eval()\n",
    "    stream = tqdm(val_loader)\n",
    "    with torch.no_grad():\n",
    "        for i, (images, target, _) in enumerate(stream, start=1):\n",
    "            images = images.to(params[\"device\"], non_blocking=True)\n",
    "            target = target.to(params[\"device\"], non_blocking=True)#.view(-1,params['batch_size'])\n",
    "            output = model(images)\n",
    "            loss = val_criterion(output, target)\n",
    "            output = torch.softmax(output, dim = 1)\n",
    "            \n",
    "            accuracy = accuracy_score(output.argmax(1).cpu(), target.cpu())\n",
    "\n",
    "            stream.set_description(\n",
    "                \"Epoch: {epoch}. Validation. {metric_monitor}\".format(epoch=epoch, metric_monitor=metric_monitor)\n",
    "            )           \n",
    "            metric_monitor.update(\"Loss\", loss.item())\n",
    "            metric_monitor.update(\"Accuracy\", accuracy)\n",
    "        #to save weight\n",
    "        if (metric_monitor.curr_acc > best_acc): # or epoch == params[\"epochs\"]:\n",
    "            print(f\"Save best weight at acc {round(metric_monitor.curr_acc,4)}, epoch: {epoch}\")\n",
    "#             if not os.path.exists(f\"weights/{params[\"model\"]}\"):\n",
    "#                 os.makedirs(f\"weights/{params[\"model\"]}\")\n",
    "            torch.save({'model': model.state_dict(), \n",
    "                'loss': loss,\n",
    "                'preds': round(metric_monitor.curr_acc,4)},\n",
    "                 f'weights/{params[\"model\"]}/{params[\"model\"]}_fold{fold}_best_epoch_{epoch}.pth')\n",
    "\n",
    "            best_acc = metric_monitor.curr_acc\n",
    "    return best_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "IiA5Zgfch5_c",
    "outputId": "f4fba281-0997-4202-e9d2-6aa3f93373f0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2847 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on fold: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1. Train.      Loss: 0.709 | Accuracy: 0.666: 100%|██████████| 2847/2847 [10:57<00:00,  4.33it/s]\n",
      "Epoch: 1. Validation. Loss: 0.389 | Accuracy: 0.883: 100%|██████████| 535/535 [00:36<00:00, 14.82it/s]\n",
      "  0%|          | 0/2847 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save best weight at acc 0.8827, epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 2. Train.      Loss: 0.697 | Accuracy: 0.665: 100%|██████████| 2847/2847 [10:59<00:00,  4.32it/s]\n",
      "Epoch: 2. Validation. Loss: 0.380 | Accuracy: 0.879: 100%|██████████| 535/535 [00:36<00:00, 14.82it/s]\n",
      "Epoch: 3. Train.      Loss: 0.695 | Accuracy: 0.664: 100%|██████████| 2847/2847 [10:59<00:00,  4.32it/s]\n",
      "Epoch: 3. Validation. Loss: 0.395 | Accuracy: 0.878: 100%|██████████| 535/535 [00:36<00:00, 14.81it/s]\n",
      "Epoch: 4. Train.      Loss: 0.698 | Accuracy: 0.664: 100%|██████████| 2847/2847 [10:59<00:00,  4.32it/s]\n",
      "Epoch: 4. Validation. Loss: 0.391 | Accuracy: 0.876: 100%|██████████| 535/535 [00:36<00:00, 14.82it/s]\n",
      "Epoch: 5. Train.      Loss: 0.699 | Accuracy: 0.673: 100%|██████████| 2847/2847 [10:59<00:00,  4.32it/s]\n",
      "Epoch: 5. Validation. Loss: 0.405 | Accuracy: 0.873: 100%|██████████| 535/535 [00:36<00:00, 14.83it/s]\n",
      "Epoch: 6. Train.      Loss: 0.699 | Accuracy: 0.673: 100%|██████████| 2847/2847 [11:00<00:00,  4.31it/s]\n",
      "Epoch: 6. Validation. Loss: 0.372 | Accuracy: 0.879: 100%|██████████| 535/535 [00:36<00:00, 14.83it/s]\n",
      "Epoch: 7. Train.      Loss: 0.701 | Accuracy: 0.669: 100%|██████████| 2847/2847 [11:00<00:00,  4.31it/s]\n",
      "Epoch: 7. Validation. Loss: 0.377 | Accuracy: 0.879: 100%|██████████| 535/535 [00:36<00:00, 14.81it/s]\n",
      "Epoch: 8. Train.      Loss: 0.693 | Accuracy: 0.678: 100%|██████████| 2847/2847 [10:59<00:00,  4.31it/s]\n",
      "Epoch: 8. Validation. Loss: 0.396 | Accuracy: 0.874: 100%|██████████| 535/535 [00:36<00:00, 14.83it/s]\n",
      "Epoch: 9. Train.      Loss: 0.699 | Accuracy: 0.663: 100%|██████████| 2847/2847 [11:00<00:00,  4.31it/s]\n",
      "Epoch: 9. Validation. Loss: 0.373 | Accuracy: 0.886: 100%|██████████| 535/535 [00:36<00:00, 14.82it/s]\n",
      "  0%|          | 0/2847 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save best weight at acc 0.8855, epoch: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 10. Train.      Loss: 0.698 | Accuracy: 0.668: 100%|██████████| 2847/2847 [10:59<00:00,  4.31it/s]\n",
      "Epoch: 10. Validation. Loss: 0.366 | Accuracy: 0.886: 100%|██████████| 535/535 [00:36<00:00, 14.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save best weight at acc 0.8864, epoch: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 11. Train.      Loss: 0.694 | Accuracy: 0.674: 100%|██████████| 2847/2847 [10:59<00:00,  4.31it/s]\n",
      "Epoch: 11. Validation. Loss: 0.371 | Accuracy: 0.879: 100%|██████████| 535/535 [00:36<00:00, 14.80it/s]\n",
      "Epoch: 12. Train.      Loss: 0.699 | Accuracy: 0.662: 100%|██████████| 2847/2847 [11:01<00:00,  4.30it/s]\n",
      "Epoch: 12. Validation. Loss: 0.381 | Accuracy: 0.880: 100%|██████████| 535/535 [00:36<00:00, 14.81it/s]\n",
      "Epoch: 13. Train.      Loss: 0.695 | Accuracy: 0.672: 100%|██████████| 2847/2847 [11:06<00:00,  4.27it/s]\n",
      "Epoch: 13. Validation. Loss: 0.385 | Accuracy: 0.877: 100%|██████████| 535/535 [00:36<00:00, 14.73it/s]\n",
      "Epoch: 14. Train.      Loss: 0.693 | Accuracy: 0.667: 100%|██████████| 2847/2847 [11:03<00:00,  4.29it/s]\n",
      "Epoch: 14. Validation. Loss: 0.390 | Accuracy: 0.868: 100%|██████████| 535/535 [00:36<00:00, 14.80it/s]\n",
      "Epoch: 15. Train.      Loss: 0.693 | Accuracy: 0.669: 100%|██████████| 2847/2847 [11:00<00:00,  4.31it/s]\n",
      "Epoch: 15. Validation. Loss: 0.366 | Accuracy: 0.883: 100%|██████████| 535/535 [00:36<00:00, 14.82it/s]\n",
      "Epoch: 16. Train.      Loss: 0.698 | Accuracy: 0.664: 100%|██████████| 2847/2847 [11:00<00:00,  4.31it/s]\n",
      "Epoch: 16. Validation. Loss: 0.376 | Accuracy: 0.881: 100%|██████████| 535/535 [00:36<00:00, 14.79it/s]\n",
      "Epoch: 17. Train.      Loss: 0.694 | Accuracy: 0.665: 100%|██████████| 2847/2847 [10:59<00:00,  4.32it/s]\n",
      "Epoch: 17. Validation. Loss: 0.364 | Accuracy: 0.886: 100%|██████████| 535/535 [00:36<00:00, 14.83it/s]\n",
      "Epoch: 18. Train.      Loss: 0.696 | Accuracy: 0.669: 100%|██████████| 2847/2847 [11:00<00:00,  4.31it/s]\n",
      "Epoch: 18. Validation. Loss: 0.385 | Accuracy: 0.877: 100%|██████████| 535/535 [00:36<00:00, 14.79it/s]\n",
      "Epoch: 19. Train.      Loss: 0.696 | Accuracy: 0.669: 100%|██████████| 2847/2847 [10:59<00:00,  4.32it/s]\n",
      "Epoch: 19. Validation. Loss: 0.373 | Accuracy: 0.880: 100%|██████████| 535/535 [00:36<00:00, 14.79it/s]\n",
      "Epoch: 20. Train.      Loss: 0.694 | Accuracy: 0.669: 100%|██████████| 2847/2847 [11:00<00:00,  4.31it/s]\n",
      "Epoch: 20. Validation. Loss: 0.372 | Accuracy: 0.882: 100%|██████████| 535/535 [00:36<00:00, 14.77it/s]\n"
     ]
    }
   ],
   "source": [
    "# best_acc = 0\n",
    "print(f\"Train on fold: {fold}\")\n",
    "if params[\"train_phase\"]:\n",
    "    for epoch in range(1, params[\"epochs\"] + 1):\n",
    "        train(train_loader, model, criterion, optimizer, epoch, params)\n",
    "        best_acc = validate(val_loader, model, criterion, epoch, params, fold, best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save({'model': model.state_dict(), \n",
    "#     'preds': 0.8},\n",
    "#      f'weights/{params[\"model\"]}_fold{fold}_best_epoch_{epoch}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "classification (1) (1).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "traffic_monitor",
   "language": "python",
   "name": "traffic_monitor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
