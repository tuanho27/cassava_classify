{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-20T14:28:46.358671Z",
     "iopub.status.busy": "2020-12-20T14:28:46.357788Z",
     "iopub.status.idle": "2020-12-20T14:28:46.360784Z",
     "shell.execute_reply": "2020-12-20T14:28:46.360259Z"
    },
    "papermill": {
     "duration": 0.032732,
     "end_time": "2020-12-20T14:28:46.360893",
     "exception": false,
     "start_time": "2020-12-20T14:28:46.328161",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "############## DungNB 0.900 ##############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-20T14:28:46.427230Z",
     "iopub.status.busy": "2020-12-20T14:28:46.426412Z",
     "iopub.status.idle": "2020-12-20T14:30:12.663872Z",
     "shell.execute_reply": "2020-12-20T14:30:12.662578Z"
    },
    "papermill": {
     "duration": 86.278467,
     "end_time": "2020-12-20T14:30:12.664041",
     "exception": false,
     "start_time": "2020-12-20T14:28:46.385574",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/cassava-packages/timm-0.3.1-py3-none-any.whl\n",
      "Requirement already satisfied: torch>=1.0 in /opt/conda/lib/python3.7/site-packages (from timm==0.3.1) (1.6.0)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from timm==0.3.1) (0.7.0)\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch>=1.0->timm==0.3.1) (0.18.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch>=1.0->timm==0.3.1) (1.18.5)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.7/site-packages (from torchvision->timm==0.3.1) (8.0.1)\n",
      "Installing collected packages: timm\n",
      "Successfully installed timm-0.3.1\n",
      "Processing /kaggle/input/cassava-packages/vision_transformer_pytorch-1.0.2-py2.py3-none-any.whl\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from vision-transformer-pytorch==1.0.2) (1.18.5)\n",
      "Requirement already satisfied: torch>=1.5.0 in /opt/conda/lib/python3.7/site-packages (from vision-transformer-pytorch==1.0.2) (1.6.0)\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch>=1.5.0->vision-transformer-pytorch==1.0.2) (0.18.2)\n",
      "Installing collected packages: vision-transformer-pytorch\n",
      "Successfully installed vision-transformer-pytorch-1.0.2\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "######################################################################## | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "!pip install ../input/cassava-packages/timm-0.3.1-py3-none-any.whl\n",
    "!pip install ../input/cassava-packages/vision_transformer_pytorch-1.0.2-py2.py3-none-any.whl\n",
    "!cp -r ../input/cassava-packages/pretrainedmodels-0.7.4-py37hc8dfbb8_0.xyz ./pretrainedmodels-0.7.4-py37hc8dfbb8_0.tar.bz2\n",
    "!conda install ./pretrainedmodels-0.7.4-py37hc8dfbb8_0.tar.bz2\n",
    "!rm -rf ./pretrainedmodels-0.7.4-py37hc8dfbb8_0.tar.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2020-12-20T14:30:12.762861Z",
     "iopub.status.busy": "2020-12-20T14:30:12.761945Z",
     "iopub.status.idle": "2020-12-20T14:30:16.922411Z",
     "shell.execute_reply": "2020-12-20T14:30:16.921380Z"
    },
    "papermill": {
     "duration": 4.214163,
     "end_time": "2020-12-20T14:30:16.922566",
     "exception": false,
     "start_time": "2020-12-20T14:30:12.708403",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "import cv2\n",
    "import pandas as pd\n",
    "import pretrainedmodels\n",
    "import timm\n",
    "from vision_transformer_pytorch import VisionTransformer\n",
    "from tqdm import tqdm\n",
    "from albumentations import Resize, Normalize, Compose, CenterCrop\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import random\n",
    "import pickle\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "import lightgbm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "cv2.setNumThreads(0)\n",
    "cv2.ocl.setUseOpenCL(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-20T14:30:16.992973Z",
     "iopub.status.busy": "2020-12-20T14:30:16.992329Z",
     "iopub.status.idle": "2020-12-20T14:30:17.002146Z",
     "shell.execute_reply": "2020-12-20T14:30:17.001604Z"
    },
    "papermill": {
     "duration": 0.046059,
     "end_time": "2020-12-20T14:30:17.002241",
     "exception": false,
     "start_time": "2020-12-20T14:30:16.956182",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TEST_DIR = \"../input/cassava-leaf-disease-classification/test_images\"\n",
    "TEST_DF = pd.read_csv('../input/cassava-leaf-disease-classification/sample_submission.csv')\n",
    "CHECKPOINT_DIR = '../input/cassava-checkpoints'\n",
    "WORKERS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-20T14:30:17.090432Z",
     "iopub.status.busy": "2020-12-20T14:30:17.089535Z",
     "iopub.status.idle": "2020-12-20T14:30:17.101198Z",
     "shell.execute_reply": "2020-12-20T14:30:17.101822Z"
    },
    "papermill": {
     "duration": 0.069068,
     "end_time": "2020-12-20T14:30:17.101990",
     "exception": false,
     "start_time": "2020-12-20T14:30:17.032922",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed=123):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "SEED = 123\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-20T14:30:17.196420Z",
     "iopub.status.busy": "2020-12-20T14:30:17.195558Z",
     "iopub.status.idle": "2020-12-20T14:30:17.201651Z",
     "shell.execute_reply": "2020-12-20T14:30:17.202743Z"
    },
    "papermill": {
     "duration": 0.057075,
     "end_time": "2020-12-20T14:30:17.202916",
     "exception": false,
     "start_time": "2020-12-20T14:30:17.145841",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_dict(obj, name):\n",
    "    with open(name, 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_dict(name):\n",
    "    with open(name, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-20T14:30:17.306829Z",
     "iopub.status.busy": "2020-12-20T14:30:17.305808Z",
     "iopub.status.idle": "2020-12-20T14:30:17.793773Z",
     "shell.execute_reply": "2020-12-20T14:30:17.793199Z"
    },
    "papermill": {
     "duration": 0.542756,
     "end_time": "2020-12-20T14:30:17.793897",
     "exception": false,
     "start_time": "2020-12-20T14:30:17.251141",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SeResnext(nn.Module):\n",
    "    def __init__(self, backbone, pretrained, num_classes, in_features):\n",
    "        super(SeResnext, self).__init__()\n",
    "        if pretrained:\n",
    "            self.base = pretrainedmodels.__dict__[backbone](pretrained='imagenet')\n",
    "        else:\n",
    "            self.base = pretrainedmodels.__dict__[backbone](pretrained=None)\n",
    "        self.base.avg_pool = nn.AdaptiveAvgPool2d(output_size=1)\n",
    "        self.base.last_linear = nn.Identity()\n",
    "        self.fc1 = nn.Linear(in_features, 2048, bias=True)\n",
    "        self.fc2 = nn.Linear(2048, 1024, bias=True)\n",
    "        self.last_linear = nn.Linear(1024, num_classes, bias=True)\n",
    "\n",
    "    @autocast()\n",
    "    def forward(self, x):\n",
    "        x = self.base(x)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.last_linear(x)\n",
    "        return x\n",
    "\n",
    "class ENet(nn.Module):\n",
    "    def __init__(self, backbone, pretrained, num_classes, in_features):\n",
    "        super(ENet, self).__init__()\n",
    "        self.base = timm.create_model(backbone, pretrained=pretrained, num_classes=0)\n",
    "        self.fc1 = nn.Linear(in_features, 2048, bias=True)\n",
    "        self.fc2 = nn.Linear(2048, 1024, bias=True)\n",
    "        self.last_linear = nn.Linear(1024, num_classes, bias=True)\n",
    "\n",
    "    @autocast()\n",
    "    def forward(self, x):\n",
    "        x = self.base(x)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.last_linear(x)\n",
    "        return x\n",
    "\n",
    "class IRV2(nn.Module):\n",
    "    def __init__(self, backbone, pretrained, num_classes, in_features):\n",
    "        super(IRV2, self).__init__()\n",
    "        if pretrained:\n",
    "            self.base = pretrainedmodels.__dict__[backbone](pretrained='imagenet')\n",
    "        else:\n",
    "            self.base = pretrainedmodels.__dict__[backbone](pretrained=None)\n",
    "        self.base.avg_pool = nn.AdaptiveAvgPool2d(output_size=1)\n",
    "        self.base.last_linear = nn.Identity()\n",
    "        self.fc1 = nn.Linear(in_features, 2048, bias=True)\n",
    "        self.fc2 = nn.Linear(2048, 1024, bias=True)\n",
    "        self.last_linear = nn.Linear(1024, num_classes, bias=True)\n",
    "\n",
    "    @autocast()\n",
    "    def forward(self, x):\n",
    "        x = self.base(x)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.last_linear(x)\n",
    "        return x\n",
    "\n",
    "class InceptionV4(nn.Module):\n",
    "    def __init__(self, backbone, pretrained, num_classes, in_features):\n",
    "        super(InceptionV4, self).__init__()\n",
    "        if pretrained:\n",
    "            self.base = pretrainedmodels.__dict__[backbone](pretrained='imagenet')\n",
    "        else:\n",
    "            self.base = pretrainedmodels.__dict__[backbone](pretrained=None)\n",
    "        self.base.avg_pool = nn.AdaptiveAvgPool2d(output_size=1)\n",
    "        self.base.last_linear = nn.Identity()\n",
    "        self.fc1 = nn.Linear(in_features, 2048, bias=True)\n",
    "        self.fc2 = nn.Linear(2048, 1024, bias=True)\n",
    "        self.last_linear = nn.Linear(1024, num_classes, bias=True)\n",
    "\n",
    "    @autocast()\n",
    "    def forward(self, x):\n",
    "        x = self.base(x)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.last_linear(x)\n",
    "        return x\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, backbone, pretrained, num_classes):\n",
    "        super(ViT, self).__init__()\n",
    "        if pretrained:\n",
    "            self.base = VisionTransformer.from_pretrained(backbone, num_classes=num_classes)\n",
    "        else:\n",
    "            self.base = VisionTransformer.from_name(backbone, num_classes=num_classes) \n",
    "    @autocast()\n",
    "    def forward(self, x):\n",
    "        x = self.base(x)\n",
    "        return x\n",
    "    \n",
    "def make_model(backbone, pretrained, num_classes):\n",
    "    if backbone == 'eb0':\n",
    "        return ENet('tf_efficientnet_b0_ns', pretrained, num_classes, in_features=1280)\n",
    "    elif backbone == 'eb1':\n",
    "        return ENet('tf_efficientnet_b1_ns', pretrained, num_classes, in_features=1280)\n",
    "    elif backbone == 'eb2':\n",
    "        return ENet('tf_efficientnet_b2_ns', pretrained, num_classes, in_features=1408)\n",
    "    elif backbone == 'eb3':\n",
    "        return ENet('tf_efficientnet_b3_ns', pretrained, num_classes, in_features=1536)\n",
    "    elif backbone == 'eb4':\n",
    "        return ENet('tf_efficientnet_b4_ns', pretrained, num_classes, in_features=1792)\n",
    "    elif backbone == 'eb5':\n",
    "        return ENet('tf_efficientnet_b5_ns', pretrained, num_classes, in_features=2048)\n",
    "    elif backbone == 'eb6':\n",
    "        return ENet('tf_efficientnet_b6_ns', pretrained, num_classes, in_features=2304)\n",
    "    elif backbone == 'eb7':\n",
    "        return ENet('tf_efficientnet_b7_ns', pretrained, num_classes, in_features=2560)\n",
    "    elif backbone == 'sr101':\n",
    "        return SeResnext('se_resnext101_32x4d', pretrained, num_classes, in_features=2048)\n",
    "    elif backbone == 'iv4':\n",
    "        return InceptionV4('inceptionv4', pretrained, num_classes, in_features=1536)\n",
    "    elif backbone == 'irv2':\n",
    "        return IRV2('inceptionresnetv2', pretrained, num_classes, in_features=1536)\n",
    "    elif backbone == 'vitb16':\n",
    "        return ViT('ViT-B_16', pretrained, num_classes)\n",
    "    else:\n",
    "        raise ValueError('!!! MODELNAME !!!')\n",
    "        \n",
    "class CNNLV2Model(nn.Module):\n",
    "    def __init__(self, num_classes, num_channels):\n",
    "        super(CNNLV2Model, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(num_channels, 256, kernel_size=(1,3), stride=1, padding=0),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 512, kernel_size=(5,1), stride=1, padding=0),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.fc1 = nn.Linear(512, 1024, bias=True)\n",
    "        self.fc2 = nn.Linear(1024, 1024, bias=True)\n",
    "        self.last_linear = nn.Linear(1024, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x).view(-1, 512)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.last_linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-20T14:30:17.871597Z",
     "iopub.status.busy": "2020-12-20T14:30:17.869830Z",
     "iopub.status.idle": "2020-12-20T14:30:17.872336Z",
     "shell.execute_reply": "2020-12-20T14:30:17.872812Z"
    },
    "papermill": {
     "duration": 0.047963,
     "end_time": "2020-12-20T14:30:17.872931",
     "exception": false,
     "start_time": "2020-12-20T14:30:17.824968",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CLDTestDataset(Dataset):\n",
    "    def __init__(self, df, root_dir, image_size):\n",
    "        super(CLDTestDataset,self).__init__()\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.root_dir = root_dir\n",
    "        self.image_size = image_size\n",
    "        self.transform = Compose([\n",
    "            Resize(self.image_size, self.image_size),\n",
    "            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_id = self.df.loc[index, 'image_id']\n",
    "        image_path = os.path.join(self.root_dir, image_id)\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        height, width = image.shape[0:2]\n",
    "        crop_size = int(0.8*min(height, width))\n",
    "        x1 = (width - crop_size)//2\n",
    "        y1 = (height - crop_size)//2\n",
    "        image_cropped = image[y1:y1+crop_size, x1:x1+crop_size, :]\n",
    "        image_cropped = self.transform(image=image_cropped)['image']\n",
    "        \n",
    "        image = self.transform(image=image)['image']\n",
    "        return image, image_cropped, image_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-20T14:30:17.943196Z",
     "iopub.status.busy": "2020-12-20T14:30:17.942503Z",
     "iopub.status.idle": "2020-12-20T14:30:17.946638Z",
     "shell.execute_reply": "2020-12-20T14:30:17.946043Z"
    },
    "papermill": {
     "duration": 0.043935,
     "end_time": "2020-12-20T14:30:17.946754",
     "exception": false,
     "start_time": "2020-12-20T14:30:17.902819",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_checkpoint(checkpoint_dir, backbone='eb6', image_size=512, fold=0):\n",
    "    model = make_model(backbone=backbone, pretrained=False, num_classes=5)\n",
    "    model.cuda()\n",
    "    CHECKPOINT = '{}/{}_{}_fold{}.pth'.format(checkpoint_dir, backbone, image_size, fold)\n",
    "    model.load_state_dict(torch.load(CHECKPOINT))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def load_cnn_lv2_checkpoint(checkpoint_dir, num_models=3, fold=0):\n",
    "    model = CNNLV2Model(num_classes=5, num_channels=num_models)\n",
    "    model.cuda()\n",
    "    CHECKPOINT = '{}/cnn_lv2_fold{}.pth'.format(checkpoint_dir, fold)\n",
    "    model.load_state_dict(torch.load(CHECKPOINT))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def load_lgbm_lv2_checkpoint(checkpoint_dir, fold=0, i=0):\n",
    "    model = lightgbm.Booster(model_file='{}/lgbm_lv2_{}_fold{}.pth'.format(checkpoint_dir, i, fold))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-20T14:30:18.014267Z",
     "iopub.status.busy": "2020-12-20T14:30:18.012484Z",
     "iopub.status.idle": "2020-12-20T14:30:18.015207Z",
     "shell.execute_reply": "2020-12-20T14:30:18.015723Z"
    },
    "papermill": {
     "duration": 0.038799,
     "end_time": "2020-12-20T14:30:18.015840",
     "exception": false,
     "start_time": "2020-12-20T14:30:17.977041",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_ids = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-20T14:30:18.104166Z",
     "iopub.status.busy": "2020-12-20T14:30:18.103353Z",
     "iopub.status.idle": "2020-12-20T14:30:42.121793Z",
     "shell.execute_reply": "2020-12-20T14:30:42.120497Z"
    },
    "papermill": {
     "duration": 24.074899,
     "end_time": "2020-12-20T14:30:42.121917",
     "exception": false,
     "start_time": "2020-12-20T14:30:18.047018",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.10s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eb6_512_f0 = load_checkpoint(CHECKPOINT_DIR, backbone='eb6', image_size=512, fold=0)\n",
    "eb6_512_f1 = load_checkpoint(CHECKPOINT_DIR, backbone='eb6', image_size=512, fold=1)\n",
    "eb6_512_f2 = load_checkpoint(CHECKPOINT_DIR, backbone='eb6', image_size=512, fold=2)\n",
    "eb6_512_f3 = load_checkpoint(CHECKPOINT_DIR, backbone='eb6', image_size=512, fold=3)\n",
    "eb6_512_f4 = load_checkpoint(CHECKPOINT_DIR, backbone='eb6', image_size=512, fold=4)\n",
    "\n",
    "test_dataset = CLDTestDataset(df=TEST_DF, root_dir=TEST_DIR, image_size=512)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=WORKERS)\n",
    "\n",
    "eb6_512_preds = []\n",
    "for images, images_cropped, ids in tqdm(test_loader):\n",
    "    images = images.cuda()\n",
    "    images_cropped = images_cropped.cuda()\n",
    "    image_ids.extend(ids)\n",
    "    pred = []\n",
    "    with torch.cuda.amp.autocast(), torch.no_grad():\n",
    "        pred.append(eb6_512_f0(images))\n",
    "        pred.append(eb6_512_f0(torch.flip(images, dims=(3,)).contiguous()))\n",
    "        pred.append(eb6_512_f0(images_cropped))\n",
    "        \n",
    "        pred.append(eb6_512_f1(images))\n",
    "        pred.append(eb6_512_f1(torch.flip(images, dims=(3,)).contiguous()))\n",
    "        pred.append(eb6_512_f1(images_cropped))\n",
    "        \n",
    "        pred.append(eb6_512_f2(images))\n",
    "        pred.append(eb6_512_f2(torch.flip(images, dims=(3,)).contiguous()))\n",
    "        pred.append(eb6_512_f2(images_cropped))\n",
    "        \n",
    "        pred.append(eb6_512_f3(images))\n",
    "        pred.append(eb6_512_f3(torch.flip(images, dims=(3,)).contiguous()))\n",
    "        pred.append(eb6_512_f3(images_cropped))\n",
    "        \n",
    "        pred.append(eb6_512_f4(images))\n",
    "        pred.append(eb6_512_f4(torch.flip(images, dims=(3,)).contiguous()))\n",
    "        pred.append(eb6_512_f4(images_cropped))\n",
    "        \n",
    "    pred = torch.stack(pred, -1)\n",
    "    eb6_512_preds.append(pred)\n",
    "\n",
    "image_ids = np.array(image_ids)\n",
    "eb6_512_preds = torch.cat(eb6_512_preds).data.cpu().numpy()\n",
    "\n",
    "del eb6_512_f0\n",
    "del eb6_512_f1\n",
    "del eb6_512_f2\n",
    "del eb6_512_f3\n",
    "del eb6_512_f4\n",
    "del test_dataset\n",
    "del test_loader\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-20T14:30:42.203868Z",
     "iopub.status.busy": "2020-12-20T14:30:42.203009Z",
     "iopub.status.idle": "2020-12-20T14:30:52.675703Z",
     "shell.execute_reply": "2020-12-20T14:30:52.676232Z"
    },
    "papermill": {
     "duration": 10.519717,
     "end_time": "2020-12-20T14:30:52.676382",
     "exception": false,
     "start_time": "2020-12-20T14:30:42.156665",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.84s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eb4_640_f0 = load_checkpoint(CHECKPOINT_DIR, backbone='eb4', image_size=640, fold=0)\n",
    "eb4_640_f1 = load_checkpoint(CHECKPOINT_DIR, backbone='eb4', image_size=640, fold=1)\n",
    "eb4_640_f2 = load_checkpoint(CHECKPOINT_DIR, backbone='eb4', image_size=640, fold=2)\n",
    "eb4_640_f3 = load_checkpoint(CHECKPOINT_DIR, backbone='eb4', image_size=640, fold=3)\n",
    "eb4_640_f4 = load_checkpoint(CHECKPOINT_DIR, backbone='eb4', image_size=640, fold=4)\n",
    "\n",
    "test_dataset = CLDTestDataset(df=TEST_DF, root_dir=TEST_DIR, image_size=640)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=WORKERS)\n",
    "\n",
    "eb4_640_preds = []\n",
    "for images, images_cropped, _ in tqdm(test_loader):\n",
    "    images = images.cuda()\n",
    "    images_cropped = images_cropped.cuda()\n",
    "    pred = []\n",
    "    with torch.cuda.amp.autocast(), torch.no_grad():\n",
    "        pred.append(eb4_640_f0(images))\n",
    "        pred.append(eb4_640_f0(torch.flip(images, dims=(3,)).contiguous()))\n",
    "        pred.append(eb4_640_f0(images_cropped))\n",
    "        \n",
    "        pred.append(eb4_640_f1(images))\n",
    "        pred.append(eb4_640_f1(torch.flip(images, dims=(3,)).contiguous()))\n",
    "        pred.append(eb4_640_f1(images_cropped))\n",
    "        \n",
    "        pred.append(eb4_640_f2(images))\n",
    "        pred.append(eb4_640_f2(torch.flip(images, dims=(3,)).contiguous()))\n",
    "        pred.append(eb4_640_f2(images_cropped))\n",
    "        \n",
    "        pred.append(eb4_640_f3(images))\n",
    "        pred.append(eb4_640_f3(torch.flip(images, dims=(3,)).contiguous()))\n",
    "        pred.append(eb4_640_f3(images_cropped))\n",
    "        \n",
    "        pred.append(eb4_640_f4(images))\n",
    "        pred.append(eb4_640_f4(torch.flip(images, dims=(3,)).contiguous()))\n",
    "        pred.append(eb4_640_f4(images_cropped))\n",
    "        \n",
    "    pred = torch.stack(pred, -1)\n",
    "    eb4_640_preds.append(pred)\n",
    "\n",
    "eb4_640_preds = torch.cat(eb4_640_preds).data.cpu().numpy()\n",
    "\n",
    "del eb4_640_f0\n",
    "del eb4_640_f1\n",
    "del eb4_640_f2\n",
    "del eb4_640_f3\n",
    "del eb4_640_f4\n",
    "del test_dataset\n",
    "del test_loader\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-20T14:30:52.765815Z",
     "iopub.status.busy": "2020-12-20T14:30:52.764933Z",
     "iopub.status.idle": "2020-12-20T14:31:09.157057Z",
     "shell.execute_reply": "2020-12-20T14:31:09.157597Z"
    },
    "papermill": {
     "duration": 16.446868,
     "end_time": "2020-12-20T14:31:09.157765",
     "exception": false,
     "start_time": "2020-12-20T14:30:52.710897",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.70s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sr101_448_f0 = load_checkpoint(CHECKPOINT_DIR, backbone='sr101', image_size=448, fold=0)\n",
    "sr101_448_f1 = load_checkpoint(CHECKPOINT_DIR, backbone='sr101', image_size=448, fold=1)\n",
    "sr101_448_f2 = load_checkpoint(CHECKPOINT_DIR, backbone='sr101', image_size=448, fold=2)\n",
    "sr101_448_f3 = load_checkpoint(CHECKPOINT_DIR, backbone='sr101', image_size=448, fold=3)\n",
    "sr101_448_f4 = load_checkpoint(CHECKPOINT_DIR, backbone='sr101', image_size=448, fold=4)\n",
    "\n",
    "test_dataset = CLDTestDataset(df=TEST_DF, root_dir=TEST_DIR, image_size=448)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=WORKERS)\n",
    "\n",
    "sr101_448_preds = []\n",
    "for images, images_cropped, _ in tqdm(test_loader):\n",
    "    images = images.cuda()\n",
    "    images_cropped = images_cropped.cuda()\n",
    "    pred = []\n",
    "    with torch.cuda.amp.autocast(), torch.no_grad():\n",
    "        pred.append(sr101_448_f0(images))\n",
    "        pred.append(sr101_448_f0(torch.flip(images, dims=(3,)).contiguous()))\n",
    "        pred.append(sr101_448_f0(images_cropped))\n",
    "        \n",
    "        pred.append(sr101_448_f1(images))\n",
    "        pred.append(sr101_448_f1(torch.flip(images, dims=(3,)).contiguous()))\n",
    "        pred.append(sr101_448_f1(images_cropped))\n",
    "        \n",
    "        pred.append(sr101_448_f2(images))\n",
    "        pred.append(sr101_448_f2(torch.flip(images, dims=(3,)).contiguous()))\n",
    "        pred.append(sr101_448_f2(images_cropped))\n",
    "        \n",
    "        pred.append(sr101_448_f3(images))\n",
    "        pred.append(sr101_448_f3(torch.flip(images, dims=(3,)).contiguous()))\n",
    "        pred.append(sr101_448_f3(images_cropped))\n",
    "        \n",
    "        pred.append(sr101_448_f4(images))\n",
    "        pred.append(sr101_448_f4(torch.flip(images, dims=(3,)).contiguous()))\n",
    "        pred.append(sr101_448_f4(images_cropped))\n",
    "        \n",
    "    pred = torch.stack(pred, -1)\n",
    "    sr101_448_preds.append(pred)\n",
    "\n",
    "sr101_448_preds = torch.cat(sr101_448_preds).data.cpu().numpy()\n",
    "\n",
    "del sr101_448_f0\n",
    "del sr101_448_f1\n",
    "del sr101_448_f2\n",
    "del sr101_448_f3\n",
    "del sr101_448_f4\n",
    "del test_dataset\n",
    "del test_loader\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-20T14:31:09.252723Z",
     "iopub.status.busy": "2020-12-20T14:31:09.240642Z",
     "iopub.status.idle": "2020-12-20T14:31:27.033077Z",
     "shell.execute_reply": "2020-12-20T14:31:27.032587Z"
    },
    "papermill": {
     "duration": 17.838522,
     "end_time": "2020-12-20T14:31:27.033190",
     "exception": false,
     "start_time": "2020-12-20T14:31:09.194668",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.73it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vitb16_384_f0 = load_checkpoint(CHECKPOINT_DIR, backbone='vitb16', image_size=384, fold=0)\n",
    "vitb16_384_f1 = load_checkpoint(CHECKPOINT_DIR, backbone='vitb16', image_size=384, fold=1)\n",
    "vitb16_384_f2 = load_checkpoint(CHECKPOINT_DIR, backbone='vitb16', image_size=384, fold=2)\n",
    "vitb16_384_f3 = load_checkpoint(CHECKPOINT_DIR, backbone='vitb16', image_size=384, fold=3)\n",
    "vitb16_384_f4 = load_checkpoint(CHECKPOINT_DIR, backbone='vitb16', image_size=384, fold=4)\n",
    "\n",
    "test_dataset = CLDTestDataset(df=TEST_DF, root_dir=TEST_DIR, image_size=384)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=WORKERS)\n",
    "\n",
    "vitb16_384_preds = []\n",
    "for images, images_cropped, _ in tqdm(test_loader):\n",
    "    images = images.cuda()\n",
    "    images_cropped = images_cropped.cuda()\n",
    "    pred = []\n",
    "    with torch.cuda.amp.autocast(), torch.no_grad():\n",
    "        pred.append(vitb16_384_f0(images))\n",
    "        pred.append(vitb16_384_f0(torch.flip(images, dims=(3,)).contiguous()))\n",
    "        pred.append(vitb16_384_f0(images_cropped))\n",
    "        \n",
    "        pred.append(vitb16_384_f1(images))\n",
    "        pred.append(vitb16_384_f1(torch.flip(images, dims=(3,)).contiguous()))\n",
    "        pred.append(vitb16_384_f1(images_cropped))\n",
    "        \n",
    "        pred.append(vitb16_384_f2(images))\n",
    "        pred.append(vitb16_384_f2(torch.flip(images, dims=(3,)).contiguous()))\n",
    "        pred.append(vitb16_384_f2(images_cropped))\n",
    "        \n",
    "        pred.append(vitb16_384_f3(images))\n",
    "        pred.append(vitb16_384_f3(torch.flip(images, dims=(3,)).contiguous()))\n",
    "        pred.append(vitb16_384_f3(images_cropped))\n",
    "        \n",
    "        pred.append(vitb16_384_f4(images))\n",
    "        pred.append(vitb16_384_f4(torch.flip(images, dims=(3,)).contiguous()))\n",
    "        pred.append(vitb16_384_f4(images_cropped))\n",
    "        \n",
    "    pred = torch.stack(pred, -1)\n",
    "    vitb16_384_preds.append(pred)\n",
    "\n",
    "vitb16_384_preds = torch.cat(vitb16_384_preds).data.cpu().numpy()\n",
    "\n",
    "del vitb16_384_f0\n",
    "del vitb16_384_f1\n",
    "del vitb16_384_f2\n",
    "del vitb16_384_f3\n",
    "del vitb16_384_f4\n",
    "del test_dataset\n",
    "del test_loader\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-20T14:31:27.165274Z",
     "iopub.status.busy": "2020-12-20T14:31:27.164319Z",
     "iopub.status.idle": "2020-12-20T14:31:27.748245Z",
     "shell.execute_reply": "2020-12-20T14:31:27.747763Z"
    },
    "papermill": {
     "duration": 0.676267,
     "end_time": "2020-12-20T14:31:27.748351",
     "exception": false,
     "start_time": "2020-12-20T14:31:27.072084",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dung_preds = np.zeros((len(TEST_DF), 5), dtype=np.float64)\n",
    "for fold in range(5):\n",
    "    cnn_x_test = np.stack([eb6_512_preds[:,:,3*fold:3*fold+3], \n",
    "                       eb4_640_preds[:,:,3*fold:3*fold+3], \n",
    "                       sr101_448_preds[:,:,3*fold:3*fold+3], \n",
    "                       vitb16_384_preds[:,:,3*fold:3*fold+3]], axis=-1)\n",
    "\n",
    "    cnn_x_test = torch.from_numpy(cnn_x_test).permute(0,3,1,2).cuda()\n",
    " \n",
    "    cnn_lv2_model = load_cnn_lv2_checkpoint(CHECKPOINT_DIR, num_models=4, fold=fold)\n",
    "    with torch.cuda.amp.autocast(), torch.no_grad():\n",
    "        cnn_preds = cnn_lv2_model(cnn_x_test)\n",
    "        cnn_preds = F.softmax(cnn_preds, 1).data.cpu().numpy()\n",
    "    dung_preds += cnn_preds\n",
    "    \n",
    "    del cnn_lv2_model\n",
    "    del cnn_x_test\n",
    "dung_preds /= 5.0\n",
    "\n",
    "del eb6_512_preds\n",
    "del eb4_640_preds\n",
    "del sr101_448_preds\n",
    "del vitb16_384_preds\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-20T14:31:27.830045Z",
     "iopub.status.busy": "2020-12-20T14:31:27.829386Z",
     "iopub.status.idle": "2020-12-20T14:31:27.833233Z",
     "shell.execute_reply": "2020-12-20T14:31:27.833850Z"
    },
    "papermill": {
     "duration": 0.047122,
     "end_time": "2020-12-20T14:31:27.833989",
     "exception": false,
     "start_time": "2020-12-20T14:31:27.786867",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "############## TuanHo 0.903 ##############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-20T14:31:27.917786Z",
     "iopub.status.busy": "2020-12-20T14:31:27.916922Z",
     "iopub.status.idle": "2020-12-20T14:31:59.491381Z",
     "shell.execute_reply": "2020-12-20T14:31:59.490431Z"
    },
    "papermill": {
     "duration": 31.619168,
     "end_time": "2020-12-20T14:31:59.491522",
     "exception": false,
     "start_time": "2020-12-20T14:31:27.872354",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install ../input/cassavav01/pytorch-image-models > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-20T14:31:59.594517Z",
     "iopub.status.busy": "2020-12-20T14:31:59.577721Z",
     "iopub.status.idle": "2020-12-20T14:32:00.271081Z",
     "shell.execute_reply": "2020-12-20T14:32:00.270066Z"
    },
    "papermill": {
     "duration": 0.73799,
     "end_time": "2020-12-20T14:32:00.271202",
     "exception": false,
     "start_time": "2020-12-20T14:31:59.533212",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cp -r ../input/cassavav01/utils ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-20T14:32:00.360731Z",
     "iopub.status.busy": "2020-12-20T14:32:00.359813Z",
     "iopub.status.idle": "2020-12-20T14:32:00.375945Z",
     "shell.execute_reply": "2020-12-20T14:32:00.375303Z"
    },
    "papermill": {
     "duration": 0.065738,
     "end_time": "2020-12-20T14:32:00.376046",
     "exception": false,
     "start_time": "2020-12-20T14:32:00.310308",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from urllib.request import urlretrieve\n",
    "import pandas as pd\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "cudnn.benchmark = True\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n",
    "import timm\n",
    "from timm.loss import LabelSmoothingCrossEntropy, SoftTargetCrossEntropy, JsdCrossEntropy\n",
    "from utils import Mixup, RandAugment\n",
    "from PIL import Image\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-20T14:32:00.462588Z",
     "iopub.status.busy": "2020-12-20T14:32:00.460828Z",
     "iopub.status.idle": "2020-12-20T14:32:00.463239Z",
     "shell.execute_reply": "2020-12-20T14:32:00.463709Z"
    },
    "papermill": {
     "duration": 0.048912,
     "end_time": "2020-12-20T14:32:00.463838",
     "exception": false,
     "start_time": "2020-12-20T14:32:00.414926",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_everything(SEED):\n",
    "    random.seed(SEED)\n",
    "    os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-20T14:32:00.548773Z",
     "iopub.status.busy": "2020-12-20T14:32:00.547858Z",
     "iopub.status.idle": "2020-12-20T14:32:00.553056Z",
     "shell.execute_reply": "2020-12-20T14:32:00.553727Z"
    },
    "papermill": {
     "duration": 0.05117,
     "end_time": "2020-12-20T14:32:00.553871",
     "exception": false,
     "start_time": "2020-12-20T14:32:00.502701",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train_tfrecords',\n",
       " 'sample_submission.csv',\n",
       " 'test_tfrecords',\n",
       " 'label_num_to_disease_map.json',\n",
       " 'train_images',\n",
       " 'train.csv',\n",
       " 'test_images']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root = os.path.join(os.environ[\"HOME\"], \"/kaggle/input/cassava-leaf-disease-classification\")\n",
    "os.listdir(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-20T14:32:00.643259Z",
     "iopub.status.busy": "2020-12-20T14:32:00.642634Z",
     "iopub.status.idle": "2020-12-20T14:32:00.946396Z",
     "shell.execute_reply": "2020-12-20T14:32:00.945844Z"
    },
    "papermill": {
     "duration": 0.351611,
     "end_time": "2020-12-20T14:32:00.946531",
     "exception": false,
     "start_time": "2020-12-20T14:32:00.594920",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000015157.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000201771.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100042118.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000723321.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000812911.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         image_id  label\n",
       "0  1000015157.jpg      0\n",
       "1  1000201771.jpg      3\n",
       "2   100042118.jpg      1\n",
       "3  1000723321.jpg      1\n",
       "4  1000812911.jpg      3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2216849948.jpg</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         image_id  label\n",
       "0  2216849948.jpg      4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cassava Bacterial Blight (CBB)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cassava Brown Streak Disease (CBSD)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cassava Green Mottle (CGM)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cassava Mosaic Disease (CMD)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Healthy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     0\n",
       "0       Cassava Bacterial Blight (CBB)\n",
       "1  Cassava Brown Streak Disease (CBSD)\n",
       "2           Cassava Green Mottle (CGM)\n",
       "3         Cassava Mosaic Disease (CMD)\n",
       "4                              Healthy"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train = pd.read_csv(f'{root}/train.csv')\n",
    "test = pd.read_csv(f'{root}/sample_submission.csv')\n",
    "label_map = pd.read_json(f'{root}/label_num_to_disease_map.json', \n",
    "                         orient='index')\n",
    "display(train.head())\n",
    "display(test.head())\n",
    "display(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-20T14:32:01.035789Z",
     "iopub.status.busy": "2020-12-20T14:32:01.035144Z",
     "iopub.status.idle": "2020-12-20T14:32:01.039355Z",
     "shell.execute_reply": "2020-12-20T14:32:01.038889Z"
    },
    "papermill": {
     "duration": 0.050488,
     "end_time": "2020-12-20T14:32:01.039474",
     "exception": false,
     "start_time": "2020-12-20T14:32:00.988986",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "models_name = [\"resnest26d\", \"resnest50d\" ,\"tf_efficientnet_b3_ns\", \"vit_base_patch16_384\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-20T14:32:01.135836Z",
     "iopub.status.busy": "2020-12-20T14:32:01.135061Z",
     "iopub.status.idle": "2020-12-20T14:32:01.139279Z",
     "shell.execute_reply": "2020-12-20T14:32:01.138794Z"
    },
    "papermill": {
     "duration": 0.057623,
     "end_time": "2020-12-20T14:32:01.139382",
     "exception": false,
     "start_time": "2020-12-20T14:32:01.081759",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "WEIGHTS = [\n",
    "\n",
    "    \"../input/cassavav05/resnest26d_fold1_best_epoch_7_final_2nd.pth\",\n",
    "    \"../input/cassavav05/resnest26d_fold2_best_epoch_4_final_2nd.pth\",\n",
    "    #\n",
    "    \"../input/cassavav03/tf_efficientnet_b3_ns_fold1_best_epoch_1_final_512.pth\",\n",
    "    #\n",
    "#     \"../input/cassavav06/resnest50d_fold0_best_epoch_13_final_2nd.pth\",\n",
    "    \"../input/cassavav07/resnest50d_fold0_best_epoch_10_final_3rd.pth\",\n",
    "#     \"../input/cassavav08/resnest50d_fold0_best_epoch_16_final_4th.pth\",\n",
    "    \"../input/cassavav06/resnest50d_fold1_best_epoch_17_final_2nd.pth\",\n",
    "    \"../input/cassavav09/resnest50d_fold1_best_epoch_8_final_5th_pseudo.pth\",\n",
    "    \"../input/cassavav06/resnest50d_fold2_best_epoch_22_final_2nd.pth\",\n",
    "#     \"../input/cassavav06/resnest50d_fold3_best_epoch_2_final_2nd.pth\",\n",
    "    \"../input/cassavav07/resnest50d_fold3_best_epoch_1_final_3rd.pth\",\n",
    "#     \"../input/cassavav08/resnest50d_fold3_best_epoch_2_final_4th.pth\",\n",
    "#     \"../input/cassavav06/resnest50d_fold4_best_epoch_10_final_2nd.pth\",\n",
    "    \"../input/cassavav06/resnest50d_fold4_best_epoch_15_final_3rd.pth\",\n",
    "    \"../input/cassavav08/resnest50d_fold4_best_epoch_10_final-4th.pth\",\n",
    "    \"../input/cassavav09/resnest50d_fold4_best_epoch_1_final_5th_pseudo.pth\",\n",
    "]\n",
    "model_index = 1\n",
    "ckpt_index = 0\n",
    "\n",
    "ensemble_models_name = list(2*[\"resnest26d\"] + [\"tf_efficientnet_b3_ns\"] + 8*[\"resnest50d\"])\n",
    "ensemble_ckpt_index = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "params = {\n",
    "    \"visualize\": False,\n",
    "    \"debug\":False,\n",
    "    \"fold\": [0,1,2,3,4],\n",
    "    \"load_pretrained\": True,\n",
    "    \"image_size\": 512,\n",
    "    \"num_classes\": 5,\n",
    "    \"model\": models_name[model_index],\n",
    "    \"device\": \"cuda\",\n",
    "    \"batch_size\": 16,\n",
    "    \"num_workers\": 2,\n",
    "    \"drop_block\": 0.2,\n",
    "    \"drop_rate\": 0.2,\n",
    "    \"tta\": True,\n",
    "    \"kfold_pred\":True,\n",
    "    \"error_fix\":False,\n",
    "    \"ensemble\":True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-20T14:32:01.245882Z",
     "iopub.status.busy": "2020-12-20T14:32:01.245083Z",
     "iopub.status.idle": "2020-12-20T14:32:01.248183Z",
     "shell.execute_reply": "2020-12-20T14:32:01.247721Z"
    },
    "papermill": {
     "duration": 0.060115,
     "end_time": "2020-12-20T14:32:01.248291",
     "exception": false,
     "start_time": "2020-12-20T14:32:01.188176",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, df, transform=None, valid_test=False):\n",
    "        self.df = df\n",
    "        self.file_names = df['image_id'].values\n",
    "        self.transform = transform\n",
    "        self.valid_test = valid_test\n",
    "        if self.valid_test:\n",
    "            self.labels = df['label'].values  \n",
    "        else:\n",
    "            assert ValueError(\"Test data does not have annotation, plz check!\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_name = self.file_names[idx]\n",
    "        file_path = f'{root}/test_images/{file_name}'\n",
    "        image = cv2.imread(file_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        if isinstance(self.transform, list):\n",
    "            outputs = {'images':[],\n",
    "                       'labels':[]}\n",
    "             #image0 = transforms.ToPILImage()(image)\n",
    "             #image0 = self.transform[0](image0)\n",
    "\n",
    "            for trans in self.transform:\n",
    "                augmented = trans(image=image)\n",
    "                image_aug = augmented['image']\n",
    "                outputs[\"images\"].append(image_aug)\n",
    "                del image_aug\n",
    "                \n",
    "            if self.valid_test:\n",
    "                label = torch.tensor(self.labels[idx]).long()\n",
    "                outputs['labels'] = len(self.transform)*[label]\n",
    "            else:\n",
    "                outputs['labels'] = len(self.transform)*[-1]\n",
    "                \n",
    "            return outputs\n",
    "        else:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image'] \n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-20T14:32:01.339618Z",
     "iopub.status.busy": "2020-12-20T14:32:01.338788Z",
     "iopub.status.idle": "2020-12-20T14:32:01.341744Z",
     "shell.execute_reply": "2020-12-20T14:32:01.341093Z"
    },
    "papermill": {
     "duration": 0.051736,
     "end_time": "2020-12-20T14:32:01.341846",
     "exception": false,
     "start_time": "2020-12-20T14:32:01.290110",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class UnNormalize(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
    "        Returns:\n",
    "            Tensor: Normalized image.\n",
    "        \"\"\"\n",
    "        for t, m, s in zip(tensor, self.mean, self.std):\n",
    "            t.mul_(s).add_(m)\n",
    "            # The normalize code -> t.sub_(m).div_(s)\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-20T14:32:01.436274Z",
     "iopub.status.busy": "2020-12-20T14:32:01.435390Z",
     "iopub.status.idle": "2020-12-20T14:32:01.438144Z",
     "shell.execute_reply": "2020-12-20T14:32:01.437684Z"
    },
    "papermill": {
     "duration": 0.05399,
     "end_time": "2020-12-20T14:32:01.438249",
     "exception": false,
     "start_time": "2020-12-20T14:32:01.384259",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def declare_model(name, index=None):\n",
    "    model = timm.create_model(name,\n",
    "            pretrained=False,\n",
    "            num_classes=params[\"num_classes\"],\n",
    "            drop_rate=params[\"drop_rate\"])\n",
    "    model = model.to(params[\"device\"])\n",
    "    model = torch.nn.DataParallel(model) \n",
    "        \n",
    "    if params[\"load_pretrained\"]:\n",
    "        if index == None: \n",
    "            return model\n",
    "        state_dict = torch.load(WEIGHTS[index])\n",
    "        print(f\"Load pretrained model: {name} \",state_dict[\"preds\"])\n",
    "        model.load_state_dict(state_dict[\"model\"])\n",
    "        best_acc = state_dict[\"preds\"]   \n",
    "    return model\n",
    "if not params[\"kfold_pred\"] and not params[\"ensemble\"]:\n",
    "    model = declare_model(params[\"model\"], ckpt_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-20T14:32:01.550002Z",
     "iopub.status.busy": "2020-12-20T14:32:01.549205Z",
     "iopub.status.idle": "2020-12-20T14:32:01.552170Z",
     "shell.execute_reply": "2020-12-20T14:32:01.552649Z"
    },
    "papermill": {
     "duration": 0.070719,
     "end_time": "2020-12-20T14:32:01.552792",
     "exception": false,
     "start_time": "2020-12-20T14:32:01.482073",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "transform_tta0 = A.Compose(\n",
    "    [\n",
    "     A.CenterCrop(height=params[\"image_size\"], width=params[\"image_size\"], p=1),\n",
    "     A.Resize(height=params[\"image_size\"], width=params[\"image_size\"], p=1),\n",
    "     A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),   \n",
    "     ToTensorV2()\n",
    "    ]\n",
    ")\n",
    "\n",
    "transform_tta1 = A.Compose(\n",
    "    [\n",
    "     A.CenterCrop(height=params[\"image_size\"], width=params[\"image_size\"], p=1),\n",
    "     A.Resize(height=params[\"image_size\"], width=params[\"image_size\"], p=1),\n",
    "     A.HorizontalFlip(p=1.),\n",
    "     A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),   \n",
    "     ToTensorV2()\n",
    "    ]\n",
    ")\n",
    "transform_tta2 = A.Compose(\n",
    "    [\n",
    "     A.CenterCrop(height=params[\"image_size\"], width=params[\"image_size\"], p=1),        \n",
    "     A.Resize(height=params[\"image_size\"], width=params[\"image_size\"], p=1),\n",
    "     A.VerticalFlip(p=1.),\n",
    "     A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "     ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "transform_tta3 = A.Compose(\n",
    "    [\n",
    "     A.CenterCrop(height=params[\"image_size\"], width=params[\"image_size\"], p=1),\n",
    "     A.Resize(height=params[\"image_size\"], width=params[\"image_size\"], p=1),\n",
    "     A.RandomRotate90(p=1.),\n",
    "     A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),     \n",
    "     ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "##### Test TTA with Five Crops\n",
    "transform_crop_tta0 = T.Compose([\n",
    "                T.FiveCrop(params[\"image_size\"]),\n",
    "                T.Lambda(lambda crops: ([T.ToTensor()(crop) for crop in crops])),\n",
    "                T.Lambda(lambda norms: torch.stack([T.Normalize(mean=[0.5], std=[0.5])(norm) for norm in norms]))\n",
    "        ]\n",
    ")\n",
    "test_transform_tta = [transform_tta0, transform_tta1, transform_tta2, transform_tta3]\n",
    "\n",
    "#debug\n",
    "if params['debug']:\n",
    "    print(\"In debug mode\")\n",
    "    test = pd.concat([test,test, test])\n",
    "\n",
    "if params[\"tta\"]:\n",
    "    test_pred_dataset = TestDataset(test, transform=test_transform_tta)\n",
    "else:\n",
    "    test_pred_dataset = TestDataset(test, transform=test_transform_tta[0])\n",
    "    \n",
    "test_pred_loader = DataLoader(\n",
    "    test_pred_dataset, batch_size=params[\"batch_size\"], shuffle=False, num_workers=params['num_workers'], pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-20T14:32:01.646493Z",
     "iopub.status.busy": "2020-12-20T14:32:01.645863Z",
     "iopub.status.idle": "2020-12-20T14:32:01.651913Z",
     "shell.execute_reply": "2020-12-20T14:32:01.651401Z"
    },
    "papermill": {
     "duration": 0.055993,
     "end_time": "2020-12-20T14:32:01.652009",
     "exception": false,
     "start_time": "2020-12-20T14:32:01.596016",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2216849948.jpg</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         image_id  label\n",
       "0  2216849948.jpg      4"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-20T14:32:01.749110Z",
     "iopub.status.busy": "2020-12-20T14:32:01.748372Z",
     "iopub.status.idle": "2020-12-20T14:32:01.751383Z",
     "shell.execute_reply": "2020-12-20T14:32:01.752115Z"
    },
    "papermill": {
     "duration": 0.05723,
     "end_time": "2020-12-20T14:32:01.752240",
     "exception": false,
     "start_time": "2020-12-20T14:32:01.695010",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def visualize_tta(data, pred):     \n",
    "    unorm = UnNormalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "    if params[\"tta\"]:\n",
    "        figure, ax = plt.subplots(nrows=1, ncols=num_tta, figsize=(12, 6))\n",
    "        for i, image in enumerate(data[\"images\"]):\n",
    "            image = (unorm(image[0]).cpu().numpy()*255).astype(int)\n",
    "            ax.ravel()[i].imshow(image.transpose(2,1,0))\n",
    "            ax.ravel()[i].set_title(str(pred), color='GREEN')\n",
    "            ax.ravel()[i].set_axis_off()\n",
    "        plt.tight_layout()\n",
    "        plt.show() \n",
    "    else:\n",
    "        image = (unorm(data).cpu().numpy()*255).astype(int)\n",
    "        imgplot = plt.imshow(image[0].transpose(2,1,0))\n",
    "        imgplot = plt.title(str(pred), color='GREEN')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-20T14:32:01.843473Z",
     "iopub.status.busy": "2020-12-20T14:32:01.842644Z",
     "iopub.status.idle": "2020-12-20T14:32:01.845059Z",
     "shell.execute_reply": "2020-12-20T14:32:01.845508Z"
    },
    "papermill": {
     "duration": 0.050638,
     "end_time": "2020-12-20T14:32:01.845633",
     "exception": false,
     "start_time": "2020-12-20T14:32:01.794995",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gmean(input_x, dim):\n",
    "    log_x = torch.log(input_x)\n",
    "    return torch.exp(torch.mean(log_x, dim=dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-20T14:32:01.949180Z",
     "iopub.status.busy": "2020-12-20T14:32:01.948274Z",
     "iopub.status.idle": "2020-12-20T14:32:01.951045Z",
     "shell.execute_reply": "2020-12-20T14:32:01.950550Z"
    },
    "papermill": {
     "duration": 0.06265,
     "end_time": "2020-12-20T14:32:01.951149",
     "exception": false,
     "start_time": "2020-12-20T14:32:01.888499",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_tta = len(test_transform_tta)\n",
    "def tta_predict(loader, params, valid_test=False):\n",
    "    outputs = []\n",
    "    preds = []\n",
    "    models = []\n",
    "    for ckpt_idx in params[\"fold\"]:\n",
    "        model = declare_model(params[\"model\"], ckpt_idx)\n",
    "        model.eval()\n",
    "        models.append(model)\n",
    "    #    \n",
    "    stream = tqdm(loader)    \n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(stream, start=1):\n",
    "            if params[\"visualize\"]:\n",
    "                visualize_tta(data, pred)\n",
    "            tta_output = []   \n",
    "            for i, image in enumerate(data[\"images\"]):\n",
    "                kfout = [torch.softmax(model(image), dim=1) for model in models]\n",
    "                tta_output.extend(torch.stack(kfout, dim=0))\n",
    "            tta_output = torch.stack(tta_output, dim=0).mean(dim=0)\n",
    "            \n",
    "            pred = torch.argmax(tta_output, dim=1).cpu().numpy()    \n",
    "            if params[\"error_fix\"]:\n",
    "                topk_output, topk_ids = torch.topk(output, params[\"num_classes\"])\n",
    "                for i in range(len(pred)):\n",
    "                    ## adjust the output prediction\n",
    "                    max_1st = topk_ids[i][0]\n",
    "                    max_2nd = topk_ids[i][1]\n",
    "                    if  max_1st == 0 and max_2nd == 4 and output[i][max_2nd] > 0.2:\n",
    "                        pred[i] = max_2nd\n",
    "                    if max_1st == 3 and max_2nd == 2 and output[i][max_2nd] > 0.2:\n",
    "                        pred[i] = max_2nd\n",
    "\n",
    "            preds.extend(pred)\n",
    "            ## For visualize the TTA \n",
    "            if params[\"visualize\"]:\n",
    "                visualize_tta(data, pred)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-20T14:32:02.049118Z",
     "iopub.status.busy": "2020-12-20T14:32:02.048276Z",
     "iopub.status.idle": "2020-12-20T14:32:02.051319Z",
     "shell.execute_reply": "2020-12-20T14:32:02.050853Z"
    },
    "papermill": {
     "duration": 0.057427,
     "end_time": "2020-12-20T14:32:02.051422",
     "exception": false,
     "start_time": "2020-12-20T14:32:01.993995",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ensemble CV w/ fold predict on validation set\n",
    "def tta_pred_ensemble_model(loader, params, valid_test=False):\n",
    "    stream = tqdm(loader)\n",
    "    models = []\n",
    "    tuan_preds = []\n",
    "    print(f\"Ensemble below models: {ensemble_models_name}\")\n",
    "    for name, ckpt in zip(ensemble_models_name, ensemble_ckpt_index):\n",
    "        print(ckpt)\n",
    "        m = declare_model(name, ckpt)  \n",
    "        models.append(m.eval())\n",
    "        del m\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(stream, start=1):\n",
    "            tta_output = []   \n",
    "            for i, image in enumerate(data[\"images\"]):\n",
    "                kfout = [torch.softmax(model(image), dim=1) for model in models]\n",
    "                tta_output.extend(torch.stack(kfout, dim=0))\n",
    "            tta_output = torch.stack(tta_output, dim=0).mean(dim=0)\n",
    "            \n",
    "            tta_output = F.softmax(tta_output, 1).data.cpu().numpy()\n",
    "            \n",
    "            tuan_preds.append(tta_output)\n",
    "            \n",
    "    tuan_preds = np.vstack(tuan_preds).astype(np.float64)\n",
    "    return tuan_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-20T14:32:02.144111Z",
     "iopub.status.busy": "2020-12-20T14:32:02.143186Z",
     "iopub.status.idle": "2020-12-20T14:32:23.302519Z",
     "shell.execute_reply": "2020-12-20T14:32:23.303200Z"
    },
    "papermill": {
     "duration": 21.207506,
     "end_time": "2020-12-20T14:32:23.303392",
     "exception": false,
     "start_time": "2020-12-20T14:32:02.095886",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble below models: ['resnest26d', 'resnest26d', 'tf_efficientnet_b3_ns', 'resnest50d', 'resnest50d', 'resnest50d', 'resnest50d', 'resnest50d', 'resnest50d', 'resnest50d', 'resnest50d']\n",
      "0\n",
      "Load pretrained model: resnest26d  0.8997\n",
      "1\n",
      "Load pretrained model: resnest26d  0.8952\n",
      "2\n",
      "Load pretrained model: tf_efficientnet_b3_ns  0.897\n",
      "3\n",
      "Load pretrained model: resnest50d  0.8864\n",
      "4\n",
      "Load pretrained model: resnest50d  0.8993\n",
      "5\n",
      "Load pretrained model: resnest50d  0.9005\n",
      "6\n",
      "Load pretrained model: resnest50d  0.8953\n",
      "7\n",
      "Load pretrained model: resnest50d  0.8906\n",
      "8\n",
      "Load pretrained model: resnest50d  0.8916\n",
      "9\n",
      "Load pretrained model: resnest50d  0.8939\n",
      "10\n",
      "Load pretrained model: resnest50d  0.892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:21<00:00, 21.13s/it]\n"
     ]
    }
   ],
   "source": [
    "tuan_preds = tta_pred_ensemble_model(test_pred_loader, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-20T14:32:23.408212Z",
     "iopub.status.busy": "2020-12-20T14:32:23.407365Z",
     "iopub.status.idle": "2020-12-20T14:32:23.410277Z",
     "shell.execute_reply": "2020-12-20T14:32:23.409809Z"
    },
    "papermill": {
     "duration": 0.057441,
     "end_time": "2020-12-20T14:32:23.410378",
     "exception": false,
     "start_time": "2020-12-20T14:32:23.352937",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds = 0.5*dung_preds + 0.5*tuan_preds\n",
    "preds = np.argmax(preds, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-20T14:32:23.518619Z",
     "iopub.status.busy": "2020-12-20T14:32:23.517579Z",
     "iopub.status.idle": "2020-12-20T14:32:23.527872Z",
     "shell.execute_reply": "2020-12-20T14:32:23.528822Z"
    },
    "papermill": {
     "duration": 0.069734,
     "end_time": "2020-12-20T14:32:23.529002",
     "exception": false,
     "start_time": "2020-12-20T14:32:23.459268",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         image_id  label\n",
      "0  2216849948.jpg      4\n"
     ]
    }
   ],
   "source": [
    "sub_df = pd.DataFrame()\n",
    "sub_df['image_id'] = np.array(image_ids)\n",
    "sub_df['label'] = np.array(preds, dtype=int)\n",
    "sub_df.to_csv('submission.csv', index=False)\n",
    "print(sub_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "papermill": {
   "duration": 222.889293,
   "end_time": "2020-12-20T14:32:25.096640",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-12-20T14:28:42.207347",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
