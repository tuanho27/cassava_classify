{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cassava classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "edyDFX_Ih5-j"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from urllib.request import urlretrieve\n",
    "import pandas as pd\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import resample\n",
    "cudnn.benchmark = True\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n",
    "import timm\n",
    "from timm.loss import LabelSmoothingCrossEntropy, SoftTargetCrossEntropy, JsdCrossEntropy\n",
    "from utils import Mixup, RandAugment, AsymmetricLossSingleLabel, SCELoss\n",
    "from PIL import Image\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(SEED):\n",
    "    random.seed(SEED)\n",
    "    os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] =\"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the root directory dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "waXTuqeVh5-q"
   },
   "outputs": [],
   "source": [
    "root = os.path.join(os.environ[\"HOME\"], \"Workspace/datasets/taiyoyuden/cassava\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split files from the dataset into the train and validation sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some files in the dataset are broken, so we will use only those image files that OpenCV could load correctly. We will use 20000 images for training, 4936 images for validation, and 10 images for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QgLoujrNHY5Z"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test_images',\n",
       " 'label_num_to_disease_map.json',\n",
       " 'train_1_pseudo.csv',\n",
       " 'val_1_pseudo.csv',\n",
       " 'external',\n",
       " 'sample_submission.csv',\n",
       " 'train_images',\n",
       " 'label_num_to_disease_map.json.save',\n",
       " 'train.csv']"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to visualize images and their labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a function that will take a list of images' file paths and their labels and visualize them in a grid. Correct labels are colored green, and incorrectly predicted labels are colored red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000015157.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000201771.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100042118.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000723321.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000812911.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         image_id  label\n",
       "0  1000015157.jpg      0\n",
       "1  1000201771.jpg      3\n",
       "2   100042118.jpg      1\n",
       "3  1000723321.jpg      1\n",
       "4  1000812911.jpg      3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2216849948.jpg</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000015157.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000201771.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         image_id  label\n",
       "0  2216849948.jpg      4\n",
       "1  1000015157.jpg      0\n",
       "2  1000201771.jpg      3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cassava Bacterial Blight (CBB)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cassava Brown Streak Disease (CBSD)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cassava Green Mottle (CGM)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cassava Mosaic Disease (CMD)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Healthy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     0\n",
       "0       Cassava Bacterial Blight (CBB)\n",
       "1  Cassava Brown Streak Disease (CBSD)\n",
       "2           Cassava Green Mottle (CGM)\n",
       "3         Cassava Mosaic Disease (CMD)\n",
       "4                              Healthy"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train = pd.read_csv(f'{root}/train.csv')\n",
    "train_external = pd.read_csv(f'{root}/external/train_external.csv')\n",
    "test_external = pd.read_csv(f'{root}/external/test_external.csv')\n",
    "test_external_pseudo = pd.read_csv(f'{root}/external/test_external_pseudo.csv')\n",
    "test = pd.read_csv(f'{root}/sample_submission.csv')\n",
    "label_map = pd.read_json(f'{root}/label_num_to_disease_map.json', \n",
    "                         orient='index')\n",
    "display(train.head())\n",
    "display(test.head())\n",
    "display(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7320,  910, 5440, 5241, 5784, 6315,  516, 4476, 5628, 8372, 1735,\n",
       "        819, 6999, 2483, 5361, 5101, 6470, 1234, 4605, 3435, 6446, 8716,\n",
       "       9324, 2608, 7899, 2097, 2797, 9217,  239, 2784, 3055, 4708, 1949,\n",
       "       7784, 1317, 1578, 3606, 3940, 8888, 5443, 8842, 8483, 7563, 2662,\n",
       "       7091, 9605, 6285, 5536, 7149, 9720])"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map.iloc[1].values\n",
    "np.random.randint(50, 10000, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ho-BaziAHY5q"
   },
   "outputs": [],
   "source": [
    "def visualize_input_image_grid(filepaths, image_name, labels, cols=4):\n",
    "    rows = 5\n",
    "    figure, ax = plt.subplots(nrows=rows, ncols=cols, figsize=(25, 15))\n",
    "    for i, index in enumerate(np.random.randint(0, len(image_name), 20)):\n",
    "        name = image_name.iloc[index]['image_id']\n",
    "        label =  image_name.iloc[index]['label']\n",
    "        image = cv2.imread(f'{filepaths}/train_images/{name}')\n",
    "        if (i == 0): \n",
    "            print(image.shape)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        ax.ravel()[i].imshow(image)\n",
    "        ax.ravel()[i].set_title(label, color='GREEN')\n",
    "        ax.ravel()[i].set_axis_off()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the parameters of the whole process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_name = [\"resnest26d\",\"resnest50d\",\"tf_efficientnet_b3_ns\", \"skresnet34\" ,\"cspresnet50\", \"vit_base_patch16_384\"]\n",
    "weights = [\n",
    "    \"weights/resnest26d/resnest26d_fold0_best_epoch_28_final_1st.pth\",\n",
    "    \"weights/resnest26d/resnest26d_fold0_best_epoch_13_final_mixup.pth\",\n",
    "    \"weights/resnest26d/resnest26d_fold2_best_epoch_3_final_hnm.pth\",\n",
    "    \"weights/resnest26d/resnest26d_fold4_best_epoch_29_1st.pth\",\n",
    "    \"weights/resnest26d/resnest26d_fold4_best_epoch_26_mix.pth\",\n",
    "    \"weights/resnest26d/resnest26d_fold4_best_epoch_12_cutmix.pth\",\n",
    "    \"weights/resnest26d/resnest26d_fold4_best_epoch_3_external.pth\",\n",
    "    \"weights/resnest26d/resnest26d_fold4_best_epoch_21_final_512.pth\",\n",
    "    \"weights/tf_efficientnet_b3_ns/tf_efficientnet_b3_ns_fold1_best_epoch_19_external.pth\",\n",
    "    \"weights/tf_efficientnet_b3_ns/tf_efficientnet_b3_ns_fold1_best_epoch_26_512.pth\",\n",
    "    \"weights/tf_efficientnet_b3_ns/tf_efficientnet_b3_ns_fold1_best_epoch_1_final_512.pth\",\n",
    "    \"weights/resnest50d/resnest50d_fold1_best_epoch_95_final_1st.pth\",\n",
    "#     \"weights/resnest50d/resnest50d_fold1_best_epoch_9_final_512.pth\"\n",
    "    \"weights/resnest50d/resnest50d_fold1_best_epoch_38_clean_1st.pth\",\n",
    "    \"weights/resnest50d/resnest50d_fold0_best_epoch_25.pth\",\n",
    "    \"./weights/resnest50d/resnest50d_fold4_best_epoch_39.pth\",\n",
    "        \n",
    "]\n",
    "model_index = 1\n",
    "ckpt_index = -1\n",
    "fold_ckpt_index = [11,12]\n",
    "fold_ckpt_weight = [1,1]\n",
    "\n",
    "params = {\n",
    "    \"visualize\": False,\n",
    "    \"fold\": 4,\n",
    "    \"train_external\": True,\n",
    "    \"train_clean_only\": False,\n",
    "    \"test_external\": False,\n",
    "    \"load_pretrained\": True,\n",
    "    \"resume\": False,\n",
    "    \"image_size\": 512,\n",
    "    \"num_classes\": 5,\n",
    "    \"model\": models_name[model_index],\n",
    "    \"device\": \"cuda\",\n",
    "    \"lr\": 1e-4,\n",
    "    \"lr_min\":1e-6,\n",
    "    \"batch_size\": 4,\n",
    "    \"num_workers\": 8,\n",
    "    \"epochs\": 30,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"drop_block\": 0.2,\n",
    "    \"drop_rate\": 0.2,\n",
    "    \"mix_up\": True,\n",
    "    \"cutmix\":True,\n",
    "    \"fmix\":False,\n",
    "    \"smooth_label\": 0.1,\n",
    "    \"rand_aug\": False,\n",
    "    \"local_rank\":0,\n",
    "    \"distributed\": False,\n",
    "    \"hard_negative_sample\": False,\n",
    "    \"tta\": True,\n",
    "    \"train_phase\":True,\n",
    "    \"balance_data\":False,\n",
    "    \"kfold_pred\":False\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define dataset with KFolds strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let visualize the dataset with the defined class, the proceed the split dataset with 5 folds\n",
    "Then, we can add the external dataset to each fold and visualize the class distribution of the dataset\n",
    "\n",
    "The class dataset is defined with transform and random augmentation.\n",
    "\n",
    "`__init__` will receive an optional `transform` argument. It is a transformation function of the Albumentations augmentation pipeline. Then in `__getitem__`, the Dataset class will use that function to augment an image and return it along with the correct label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "if params['visualize']:\n",
    "    visualize_input_image_grid(root, train, label_map)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold  label\n",
      "0     0         218\n",
      "      1         438\n",
      "      2         477\n",
      "      3        2631\n",
      "      4         516\n",
      "1     0         218\n",
      "      1         438\n",
      "      2         477\n",
      "      3        2631\n",
      "      4         516\n",
      "2     0         217\n",
      "      1         438\n",
      "      2         477\n",
      "      3        2632\n",
      "      4         515\n",
      "3     0         217\n",
      "      1         438\n",
      "      2         477\n",
      "      3        2632\n",
      "      4         515\n",
      "4     0         217\n",
      "      1         437\n",
      "      2         478\n",
      "      3        2632\n",
      "      4         515\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "folds = train.copy()\n",
    "Fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "for n, (train_index, val_index) in enumerate(Fold.split(folds, folds['label'])):\n",
    "    folds.loc[val_index, 'fold'] = int(n)\n",
    "folds['fold'] = folds['fold'].astype(int)\n",
    "print(folds.groupby(['fold','label']).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>label</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000015157.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000201771.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100042118.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000723321.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000812911.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17113</th>\n",
       "      <td>998910982.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17114</th>\n",
       "      <td>999068805.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17115</th>\n",
       "      <td>999329392.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17116</th>\n",
       "      <td>999474432.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17117</th>\n",
       "      <td>999616605.jpg</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17118 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             image_id  label  fold\n",
       "0      1000015157.jpg      0     3\n",
       "1      1000201771.jpg      3     2\n",
       "2       100042118.jpg      1     2\n",
       "3      1000723321.jpg      1     1\n",
       "4      1000812911.jpg      3     2\n",
       "...               ...    ...   ...\n",
       "17113   998910982.jpg      1     1\n",
       "17114   999068805.jpg      3     1\n",
       "17115   999329392.jpg      3     0\n",
       "17116   999474432.jpg      1     1\n",
       "17117   999616605.jpg      4     2\n",
       "\n",
       "[17118 rows x 3 columns]"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold = params[\"fold\"]\n",
    "train_idx = folds[folds['fold'] != fold].index\n",
    "val_idx = folds[folds['fold'] == fold].index\n",
    "\n",
    "train_folds = folds.loc[train_idx].reset_index(drop=True)\n",
    "val_folds = folds.loc[val_idx].reset_index(drop=True)\n",
    "train_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>label</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train-cbsd-663.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train-cgm-560.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train-cmd-1576.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train-cbb-443.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train-cgm-293.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5651</th>\n",
       "      <td>train-cmd-1383.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5652</th>\n",
       "      <td>train-cmd-971.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5653</th>\n",
       "      <td>train-cbb-402.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5654</th>\n",
       "      <td>train-cbsd-1029.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5655</th>\n",
       "      <td>train-cmd-2291.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5656 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 image_id  label  fold\n",
       "0      train-cbsd-663.jpg      1     4\n",
       "1       train-cgm-560.jpg      2     4\n",
       "2      train-cmd-1576.jpg      3     4\n",
       "3       train-cbb-443.jpg      0     4\n",
       "4       train-cgm-293.jpg      2     4\n",
       "...                   ...    ...   ...\n",
       "5651   train-cmd-1383.jpg      3     4\n",
       "5652    train-cmd-971.jpg      3     4\n",
       "5653    train-cbb-402.jpg      0     4\n",
       "5654  train-cbsd-1029.jpg      1     4\n",
       "5655   train-cmd-2291.jpg      3     4\n",
       "\n",
       "[5656 rows x 3 columns]"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_external\n",
    "for idx,label in enumerate(train_external['label']):\n",
    "    train_external.loc[idx,'fold'] = fold\n",
    "train_external['fold'] = train_external['fold'].astype(int)\n",
    "train_external"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_data(df1, df2):\n",
    "    merge_df = pd.concat([df1, df2], axis=0) #,how ='outer', on ='image_id')\n",
    "    return merge_df\n",
    "\n",
    "if params[\"train_external\"]:\n",
    "    train_folds = merge_data(train_folds, train_external)\n",
    "if params[\"train_clean_only\"]:\n",
    "    train_folds = train_external\n",
    "#     train_folds = pd.read_csv(f'{root}/train_{params[\"fold\"]}_pseudo.csv')\n",
    "#     val_folds = pd.read_csv(f'{root}/val_{params[\"fold\"]}_pseudo.csv')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>label</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000015157.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000201771.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100042118.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000723321.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000812911.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5651</th>\n",
       "      <td>train-cmd-1383.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5652</th>\n",
       "      <td>train-cmd-971.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5653</th>\n",
       "      <td>train-cbb-402.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5654</th>\n",
       "      <td>train-cbsd-1029.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5655</th>\n",
       "      <td>train-cmd-2291.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22774 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 image_id  label  fold\n",
       "0          1000015157.jpg      0     3\n",
       "1          1000201771.jpg      3     2\n",
       "2           100042118.jpg      1     2\n",
       "3          1000723321.jpg      1     1\n",
       "4          1000812911.jpg      3     2\n",
       "...                   ...    ...   ...\n",
       "5651   train-cmd-1383.jpg      3     4\n",
       "5652    train-cmd-971.jpg      3     4\n",
       "5653    train-cbb-402.jpg      0     4\n",
       "5654  train-cbsd-1029.jpg      1     4\n",
       "5655   train-cmd-2291.jpg      3     4\n",
       "\n",
       "[22774 rows x 3 columns]"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0     1087\n",
      "1     2189\n",
      "2     2386\n",
      "3    13158\n",
      "4     2577\n",
      "Name: image_id, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnAAAAFNCAYAAACAH1JNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZhkdX3v8fdHFpGADMpIYAYdVOINGtcOYsw1BpVFhCHEcImoqFxJbjRixCh4VXC57luISyRixCUCEiOoGEVBjbmA9IABAbmMCGEAYWTYURD53j/q11o03dM1011dc5r363nq6XN+Z/ueqpqnPnN+Z0lVIUmSpO54wKgLkCRJ0roxwEmSJHWMAU6SJKljDHCSJEkdY4CTJEnqGAOcJElSxxjgpA1YkqOTfHaE2/92kv/Zhg9K8o05XPdFSZ7Zhud0P5O8Ickn5mp967DdP0lyVZLbkjxpgPl//f7OQ23bJvlukluTvH+GeZ+ZZNVapn8qydvnqK4rkjy7Da/T9yBJJXn0XNQhdY0BThqxJC9IMt5+9K9N8rUkfzjquiarqs9V1e4zzTfoj3tVPbaqvj3buqYKG1X1jqqal2A0yfuAV1bVFlV1/lyuuD/orKdDgZ8BD66qw+eorE5IsqyFvY1HXYs0Vwxw0ggleQ3wIeAdwLbAw4GPAstHWdcwLfAf0UcAF426iGk8Ari4vHu7tCAY4KQRSbIV8FbgFVX1xaq6vap+WVVfrqq/nWaZLyT5aZKbW3fYY/umPTfJxa2L7Ookr23t2yT5SpKbkqxJ8u9Jpvy3n+Q5SX7U1v9hIH3TXpLke204ST6Y5PoktyS5MMnjkhwKHAS8rh1R/HKb/4okr09yAXB7ko2nOKK0WZITW/3nJXlC37bv1VU2cZQvyW8BXwO2b9u7Lcn2k7vikuzbumxvat2Wv9s37Yokr01yQdvvE5NsNs3784Akb0xyZdv3TyfZKskDk9wGbAT8Z5Ifr8f7+6gkZyS5IcnPknwuyaI27TP0wv2X2z6+bqbvw6Ttfgo4uO9zeXar+UNJrmmvDyV54DTLP6l9JrcmORGY8v2ZZtlp92tdJfnbdpT6miQvmzRt7yTnt+/jVUmO7pv83fb3prb/T5vLuqRRMMBJo/M0ej+E/7oOy3wN2Al4GHAe8Lm+accBf1FVWwKPA85o7YcDq4DF9I7yvQG4z1GYJNsAXwTeCGwD/Bh4+jR17A48A/gdYCvgAOCGqjq21fSe1o24T98yfw7sDSyqqrunWOdy4AvAQ4B/Br6UZJNp3wmgqm4H9gKuadvboqqumbRfvwN8Hnh1ew9OoxeENu2b7QBgT2BH4PHAS6bZ5Eva64+BRwJbAB+uqjuraos2zxOq6lGTFxzg/Q3wTmB74HeBHYCj236+CPgvYJ+2j+9py6zt+/BrVfUS7v25fBP438CuwBOBJwC7tNom170p8CXgM/Q+my8Afzppnpsyfbf/tPu1LpLsCbwWeA69fZ7cnXw78GJgEb3v2f9Ksl+b9oz2d1Hb/7Pmqi5pVAxw0ug8FPjZNGFmSlX1yaq6tarupPdj84T0juQB/BLYOcmDq+rGqjqvr3074BHtCN+/T9ON9lzgoqo6uap+Sa9r96fTlPJLYEvgvwGpqkuq6toZyj+mqq6qqp9PM31F37Y/QC/c7jrDOgfxP4CvVtXpbd3vAx4E/MGk2q6pqjXAl+mFmqkcBHygqi6vqtuAI4EDM1i38Frf36pa2Wq8s6pW03sP/mhtK5zh+zCTg4C3VtX1bXtvAV40xXy7ApsAH2rfn5OBcyfVsaiqvjdNjeu8X9M4APinqvphC+5HT9rOt6vqwqq6p6ouoBfap93OHNYljYQBThqdG4BtBvzxJ8lGSd6V5MdJbgGuaJO2aX//lF5IuDLJd5I8rbW/F1gJfCPJ5UmOmGYT2wNXTYy0kHfVVDNW1RnAh4GPANcnOTbJg2fYhSnXNdX0qrqH3lHD7WdYZhDbA1dOWvdVwJK+efqD6h30jqzNuK42vDG9I5uD1DHt+5veVaInpNf9fQvwWX7z2d7HAN+HQeqZvC9Tvd/bA1dPCv1XTjHfdHWu037NUG//d+heNSR5apIzk6xOcjPwl2vbzhzWJY2EAU4anbOAO4H9ZpqxeQG9bsZn0+u2XNbaA1BV51bVcnrdaV8CTmrtt1bV4VX1SGBf4DVJnjXF+q+l143UW2mS/vHJquqYqnoKsDO9rtSJ8/amO0l+ppPn+7f9AGApMNEdegewed+8v70O672G3gn8E+ue2K+rZ1huxnXROy/tbuC6AZad6f19B719+b2qejDwQvrOkeO++7nW78MAptqXa6aY71pgSau3f95BzbRfg7rX+zdFDf8MnArsUFVbAf/Qt52pviNzVZc0EgY4aUSq6mbgzcBHkuyXZPMkmyTZK8l7plhkS3qB7wZ6YeYdExOSbJrefdq2at1ztwD3tGnPS/Lo9gN8M/CriWmTfBV4bJL921HBV3HvoPRrSX6/HfHYhN65R7/oW+d19M4PW1dP6dv2q9u+nt2m/QB4QTvqtCf37uq6DnjoWroOTwL2TvKsVu/hbd3/dz1q/DzwN0l2TLIFvc/gxAG7wWd6f7cEbgNuTrKE3wTiCZPf12m/D+uwL29Msridn/dmekehJjuLXkh9Vft+7k/vfLlBzbRfgzoJeEmSnZNsDhw1xXbWVNUvkuxCL+BOWE3v+zn5/ZuLuqSRMMBJI1RV7wdeQ+/k8dX0uoheSe8I2mSfptdtdDVwMb8JNxNeBFzRuoP+kt45TtA74fub9H6szgI+WlVnTlHLz4A/A95FLxTsBPzHNKU/GPhH4MZW0w30umqhdzHFzu3E9qn2Yzqn0Dtf7ca2L/u3MApwGLAPcFPbr1+vt6p+RC+MXN62ea9uwKq6lN7Rlb+ndx+0fehdDHDXOtQ24ZP0Tub/LvATesH1rwdZcID39y3Ak+mF7K/Su+Ch3zvpBa6b0rvCeKbvw0zeDowDFwAX0rsI4j7372vv0/70Lt5YQ+8zuldt7crO/z7Ndmbar4FU1dfonTd4Br1TAs6YNMtfAW9Nciu9MHpS37J3AP8H+I/2/u06V3VJoxJvCSRJktQtHoGTJEnqGAOcJElSxxjgJEmSOsYAJ0mS1DEGOEmSpI4Z6A7wC8k222xTy5YtG3UZkiRJM1qxYsXPqmrx5Pb7XYBbtmwZ4+Pjoy5DkiRpRkmmfHSdXaiSJEkdY4CTJEnqGAOcJElSxxjgJEmSOsYAJ0mS1DEGOEmSpI4xwEmSJHWMAU6SJKljDHCSJEkdY4CTJEnqGAOcJElSx9zvnoUqSeq+ZNQVrJ+qUVeghcIjcJIkSR1jgJMkSeoYA5wkSVLHGOAkSZI6xgAnSZLUMQY4SZKkjjHASZIkdYwBTpIkqWMMcJIkSR1jgJMkSeoYA5wkSVLHGOAkSZI6xgAnSZLUMQY4SZKkjjHASZIkdYwBTpIkqWMMcJIkSR1jgJMkSeqYoQW4JJ9Mcn2SH/a1vTfJj5JckORfkyzqm3ZkkpVJLk2yR1/7nq1tZZIj+tp3THJOaz8xyabD2hdJkqQNyTCPwH0K2HNS2+nA46rq8cD/A44ESLIzcCDw2LbMR5NslGQj4CPAXsDOwJ+3eQHeDXywqh4N3AgcMsR9kSRJ2mAMLcBV1XeBNZPavlFVd7fRs4GlbXg5cEJV3VlVPwFWAru018qquryq7gJOAJYnCbAbcHJb/nhgv2HtiyRJ0oZklOfAvQz4WhteAlzVN21Va5uu/aHATX1hcKJdkiRpwRtJgEvyv4G7gc/N0/YOTTKeZHz16tXzsUlJkqShmfcAl+QlwPOAg6qqWvPVwA59sy1tbdO13wAsSrLxpPYpVdWxVTVWVWOLFy+ek/2QJEkalXkNcEn2BF4H7FtVd/RNOhU4MMkDk+wI7AR8HzgX2KldcbopvQsdTm3B70zg+W35g4FT5ms/JEmSRmmYtxH5PHAW8Jgkq5IcAnwY2BI4PckPkvwDQFVdBJwEXAz8G/CKqvpVO8ftlcDXgUuAk9q8AK8HXpNkJb1z4o4b1r5IkiRtSPKbXsz7h7GxsRofHx91GZKkWUhGXcH6uZ/95GoOJFlRVWOT230SgyRJUscY4CRJkjrGACdJktQxBjhJkqSOMcBJkiR1jAFOkiSpYwxwkiRJHWOAkyRJ6hgDnCRJUscY4CRJkjrGACdJktQxBjhJkqSOMcBJkiR1jAFOkiSpYwxwkiRJHWOAkyRJ6hgDnCRJUscY4CRJkjrGACdJktQxBjhJkqSOMcBJkiR1jAFOkiSpYwxwkiRJHWOAkyRJ6hgDnCRJUscY4CRJkjrGACdJktQxBjhJkqSOMcBJkiR1jAFOkiSpY4YW4JJ8Msn1SX7Y1/aQJKcnuaz93bq1J8kxSVYmuSDJk/uWObjNf1mSg/van5LkwrbMMUkyrH2RJEnakAzzCNyngD0ntR0BfKuqdgK+1cYB9gJ2aq9DgY9BL/ABRwFPBXYBjpoIfW2el/ctN3lbkiRJC9LQAlxVfRdYM6l5OXB8Gz4e2K+v/dPVczawKMl2wB7A6VW1pqpuBE4H9mzTHlxVZ1dVAZ/uW5ckSdKCNt/nwG1bVde24Z8C27bhJcBVffOtam1ra181RbskSdKCN7KLGNqRs5qPbSU5NMl4kvHVq1fPxyYlSZKGZr4D3HWt+5P29/rWfjWwQ998S1vb2tqXTtE+pao6tqrGqmps8eLFs94JSZKkUZrvAHcqMHEl6cHAKX3tL25Xo+4K3Ny6Wr8O7J5k63bxwu7A19u0W5Ls2q4+fXHfuiRJkha0jYe14iSfB54JbJNkFb2rSd8FnJTkEOBK4IA2+2nAc4GVwB3ASwGqak2StwHntvneWlUTF0b8Fb0rXR8EfK29JEmSFrz0TkW7/xgbG6vx8fFRlyFJmoWu3vnzfvaTqzmQZEVVjU1u90kMkiRJHWOAkyRJ6hgDnCRJUscY4CRJkjrGACdJktQxBjhJkqSOMcBJkiR1jAFOkiSpYwxwkiRJHWOAkyRJ6hgDnCRJUscY4CRJkjrGACdJktQxBjhJkqSOGSjAJfnDJC9tw4uT7DjcsiRJkjSdGQNckqOA1wNHtqZNgM8OsyhJkiRNb5AjcH8C7AvcDlBV1wBbDrMoSZIkTW+QAHdXVRVQAEl+a7glSZIkaW0GCXAnJfk4sCjJy4FvAv843LIkSZI0nY1nmqGq3pfkOcAtwGOAN1fV6UOvTJIkSVOaMcABtMBmaJMkSdoAzBjgktxKO/+tz83AOHB4VV0+jMIkSZI0tUGOwH0IWAX8MxDgQOBRwHnAJ4FnDqs4SZIk3dcgFzHsW1Ufr6pbq+qWqjoW2KOqTgS2HnJ9kiRJmmSQAHdHkgOSPKC9DgB+0aZN7lqVJEnSkA0S4A4CXgRcD1zXhl+Y5EHAK4dYmyRJkqYwyG1ELgf2mWby9+a2HEmSJM1kkKtQNwMOAR4LbDbRXlUvG2JdkiRJmsYgXaifAX4b2AP4DrAUuHWYRUmSJGl6gwS4R1fVm4Dbq+p4YG/gqcMtS5IkSdMZJMD9sv29KcnjgK2Ahw2vJEmSJK3NIAHu2CRbA28CTgUuBt4zm40m+ZskFyX5YZLPJ9ksyY5JzkmyMsmJSTZt8z6wja9s05f1refI1n5pkj1mU5MkSVJXzBjgquoTVXVjVX2nqh5ZVQ+rqn9Y3w0mWQK8ChirqscBG9F7usO7gQ9W1aOBG+ldOEH7e2Nr/2CbjyQ7t+UeC+wJfDTJRutblyRJUlcMchXqIuDFwLL++avqVbPc7oOS/BLYHLgW2A14QZt+PHA08DFgeRsGOBn4cJK09hOq6k7gJ0lWArsAZ82iLkmSpA3eIM9CPQ04G7gQuGe2G6yqq5O8D/gv4OfAN4AVwE1VdXebbRWwpA0vAa5qy96d5Gbgoa397L5V9y9zL0kOBQ4FePjDHz7bXZAkSRqpQQLcZlX1mrnaYDufbjmwI3AT8AV6XaBD057feizA2NiYj/+SJEmdNtB94JK8PMl2SR4y8ZrFNp8N/KSqVlfVL4EvAk8HFiWZCJRLgavb8NXADgBt+lbADf3tUywjSZK0YA0S4O4C3kvv3LIV7TU+i23+F7Brks3buWzPondl65nA89s8BwOntOFT2zht+hlVVa39wHaV6o7ATsD3Z1GXJElSJwzShXo4vZv5/mwuNlhV5yQ5GTgPuBs4n1735leBE5K8vbUd1xY5jt5RwJXAGnpXnlJVFyU5iV74uxt4RVX9ai5qlCRJ2pCldzBrLTMk3wD2q6o75qek4RobG6vx8dkcQJQkjVoy6grWzww/udJ9JFlRVWOT2wc5Anc78IMkZwJ3TjTO8jYikiRJWk+DBLgvtZckSZI2ADMGuPYAe0mSJG0gpg1wSU6qqgOSXAjcp9e+qh4/1MokSZI0pbUdgTus/X3efBQiSZKkwUwb4Krq2vb3yvkrR5IkSTMZ5Ea+kiRJ2oAY4CRJkjpm2gCX5Fvt77vnrxxJkiTNZG0XMWyX5A+AfZOcANzrvtdVdd5QK5MkSdKU1hbg3gy8CVgKfGDStAJ2G1ZRkiRJmt7arkI9GTg5yZuq6m3zWJMkSZLWYpAnMbwtyb7AM1rTt6vqK8MtS5IkSdOZ8SrUJO+kd1Pfi9vrsCTvGHZhkiRJmtogD7PfG3hiVd0DkOR44HzgDcMsTJIkSVMb9D5wi/qGtxpGIZIkSRrMIEfg3gmcn+RMercSeQZwxFCrkiRJ0rQGuYjh80m+Dfx+a3p9Vf10qFVJkiRpWoMcgZt4sP2pQ65FkiRJA/BZqJIkSR1jgJMkSeqYtQa4JBsl+dF8FSNJkqSZrTXAVdWvgEuTPHye6pEkSdIMBrmIYWvgoiTfB26faKyqfYdWlSRJkqY1SIB709CrkCRJ0sAGuQ/cd5I8Atipqr6ZZHNgo+GXJkmSpKkM8jD7lwMnAx9vTUuALw2zKEmSJE1vkNuIvAJ4OnALQFVdBjxsmEVJkiRpeoMEuDur6q6JkSQbAzW8kiRJkrQ2gwS47yR5A/CgJM8BvgB8ebhlSZIkaTqDBLgjgNXAhcBfAKcBb5zNRpMsSnJykh8luSTJ05I8JMnpSS5rf7du8ybJMUlWJrkgyZP71nNwm/+yJAfPpiZJkqSuGOQq1HuSHA+cQ6/r9NKqmm0X6t8B/1ZVz0+yKbA58AbgW1X1riRH0AuOrwf2AnZqr6cCHwOemuQhwFHAWKtrRZJTq+rGWdYmSZK0QRvkKtS9gR8DxwAfBlYm2Wt9N5hkK+AZwHEAVXVXVd0ELAeOb7MdD+zXhpcDn66es4FFSbYD9gBOr6o1LbSdDuy5vnVJkiR1xSA38n0/8MdVtRIgyaOArwJfW89t7kivS/afkjwBWAEcBmxbVde2eX4KbNuGlwBX9S2/qrVN1y5JkrSgDXIO3K0T4a25HLh1FtvcGHgy8LGqehK9x3Md0T9D66KdsytdkxyaZDzJ+OrVq+dqtZIkSSMxbYBLsn+S/YHxJKcleUm7UODLwLmz2OYqYFVVndPGT6YX6K5rXaO0v9e36VcDO/Qtv7S1Tdd+H1V1bFWNVdXY4sWLZ1G6JEnS6K3tCNw+7bUZcB3wR8Az6XV/Pmh9N1hVPwWuSvKY1vQs4GLgVGDiStKDgVPa8KnAi9vVqLsCN7eu1q8DuyfZul2xuntrkyRJWtCmPQeuql46xO3+NfC5dgXq5cBL6YXJk5IcAlwJHNDmPQ14LrASuKPNS1WtSfI2fnM08K1VtWaINUuSJG0QMtMdQZLsSC9wLaMv8FXVvkOtbEjGxsZqfHx81GVIkmYhGXUF62fWN+HS/U6SFVU1Nrl9kKtQv0Tvlh9fBu6Z68IkSZK0bgYJcL+oqmOGXokkSZIGMkiA+7skRwHfAO6caKyq84ZWlSRJkqY1SID7PeBFwG78pgu12rgkSZLm2SAB7s+AR1bVXcMuRpIkSTMb5EkMPwQWDbsQSZIkDWaQI3CLgB8lOZd7nwPXyduISJIkdd0gAe6ooVchSZKkgc0Y4KrqO/NRiCRJkgYzY4BLciu9q04BNgU2AW6vqgcPszBJkiRNbZAjcFtODCcJsBzYdZhFSZIkaXqDXIX6a9XzJWCPIdUjSZKkGQzShbp/3+gDgDHgF0OrSJIkSWs1yFWo+/QN3w1cQa8bVZIkSSMwyDlwL52PQiRJkjSYaQNckjevZbmqqrcNoR5JkiTNYG1H4G6fou23gEOAhwIGOEmSpBGYNsBV1fsnhpNsCRwGvBQ4AXj/dMtJkiRpuNZ6DlyShwCvAQ4CjgeeXFU3zkdhkiRJmtrazoF7L7A/cCzwe1V127xVJUmSpGmt7Ua+hwPbA28ErklyS3vdmuSW+SlPkiRJk63tHLh1ekqDJEmS5ochTZIkqWMMcJIkSR1jgJMkSeoYA5wkSVLHGOAkSZI6xgAnSZLUMQY4SZKkjjHASZIkdczIAlySjZKcn+QrbXzHJOckWZnkxCSbtvYHtvGVbfqyvnUc2dovTbLHaPZEkiRpfo3yCNxhwCV94+8GPlhVjwZuBA5p7YcAN7b2D7b5SLIzcCDwWGBP4KNJNpqn2iVJkkZmJAEuyVJgb+ATbTzAbsDJbZbjgf3a8PI2Tpv+rDb/cuCEqrqzqn4CrAR2mZ89kCRJGp1RHYH7EPA64J42/lDgpqq6u42vApa04SXAVQBt+s1t/l+3T7GMJEnSgjXvAS7J84Drq2rFPG7z0CTjScZXr149X5uVJEkailEcgXs6sG+SK4AT6HWd/h2wKMnGbZ6lwNVt+GpgB4A2fSvghv72KZa5l6o6tqrGqmps8eLFc7s3kiRJ82zeA1xVHVlVS6tqGb2LEM6oqoOAM4Hnt9kOBk5pw6e2cdr0M6qqWvuB7SrVHYGdgO/P025IkiSNzMYzzzJvXg+ckOTtwPnAca39OOAzSVYCa+iFPqrqoiQnARcDdwOvqKpfzX/ZkiRJ8yu9g1n3H2NjYzU+Pj7qMiRJs5CMuoL1cz/7ydUcSLKiqsYmt/skBkmSpI4xwEmSJHWMAU6SJKljDHCSJEkdY4CTJEnqGAOcJElSxxjgJEmSOsYAJ0mS1DEGOEmSpI4xwEmSJHWMAU6SJKljDHCSJEkdY4CTJEnqGAOcJElSxxjgJEmSOsYAJ0mS1DEGOEmSpI4xwEmSJHWMAU6SJKljDHCSJEkdY4CTJEnqGAOcJElSxxjgJEmSOmbjURcgSZLuv5JRV7B+qka7fY/ASZIkdYwBTpIkqWMMcJIkSR3jOXCS7jc810bSQuEROEmSpI4xwEmSJHWMAU6SJKlj5j3AJdkhyZlJLk5yUZLDWvtDkpye5LL2d+vWniTHJFmZ5IIkT+5b18Ft/suSHDzf+6KFL+nmS5K0sI3iCNzdwOFVtTOwK/CKJDsDRwDfqqqdgG+1cYC9gJ3a61DgY9ALfMBRwFOBXYCjJkKfJEnSQjbvAa6qrq2q89rwrcAlwBJgOXB8m+14YL82vBz4dPWcDSxKsh2wB3B6Va2pqhuB04E953FXJEmSRmKk58AlWQY8CTgH2Laqrm2Tfgps24aXAFf1LbaqtU3XPtV2Dk0ynmR89erVc1a/JEnSKIwswCXZAvgX4NVVdUv/tKoqYM7ufFRVx1bVWFWNLV68eK5WK0mSNBIjCXBJNqEX3j5XVV9szde1rlHa3+tb+9XADn2LL21t07VLkiQtaKO4CjXAccAlVfWBvkmnAhNXkh4MnNLX/uJ2NequwM2tq/XrwO5Jtm4XL+ze2iRJkha0UTxK6+nAi4ALk/ygtb0BeBdwUpJDgCuBA9q004DnAiuBO4CXAlTVmiRvA85t8721qtbMzy5IkiSNTup+9pC9sbGxGh8fH3UZ6oiu3lPtfvbPemB+nguHn+XC4We5dklWVNXY5HafxCBJktQxBjhJkqSOMcBJkiR1jAFOkiSpYwxwkiRJHWOAkyRJ6hgDnCRJUscY4CRJkjrGACdJktQxBjhJkqSOMcBJkiR1jAFOkiSpYzYedQELkQ/mlSRJw+QROEmSpI4xwEmSJHWMAU6SJKljDHCSJEkdY4CTJEnqGAOcJElSxxjgJEmSOsYAJ0mS1DEGOEmSpI4xwEmSJHWMAU6SJKljDHCSJEkdY4CTJEnqGAOcJElSxxjgJEmSOsYAJ0mS1DEGOEmSpI7pfIBLsmeSS5OsTHLEqOuRJEkatk4HuCQbAR8B9gJ2Bv48yc6jrUqSJGm4Oh3ggF2AlVV1eVXdBZwALB9xTZIkSUPV9QC3BLiqb3xVa5MkSVqwNh51AfMhyaHAoW30tiSXjrKeWdoG+NkwVpwMY61aCz/LhcXPc+Hws1w4FsJn+YipGrse4K4GdugbX9ra7qWqjgWOna+ihinJeFWNjboOzZ6f5cLi57lw+FkuHAv5s+x6F+q5wE5JdkyyKXAgcOqIa5IkSRqqTh+Bq6q7k7wS+DqwEfDJqrpoxGVJkiQNVacDHEBVnQacNuo65tGC6AoW4Ge50Ph5Lhx+lgvHgv0sU1WjrkGSJEnroOvnwEmSJN3vGOA6xMeGLQxJPpnk+iQ/HHUtmp0kOyQ5M8nFSS5Kctioa9L6S7JZku8n+c/2eb5l1DVpdpJslOT8JF8ZdS1zzQDXET42bEH5FLDnqIvQnLgbOLyqdgZ2BV7hv8tOuxPYraqeADwR2DPJriOuSbNzGHDJqIsYBgNcd/jYsAWiqr4LrBl1HZq9qrq2qs5rw7fS+6HwaTAdVT23tdFN2ssTxTsqyVJgb+ATo65lGAxw3eFjw6QNWJJlwJOAc0ZbiWajdbn9ALgeOL2q/Dy760PA64B7Rl3IMBjgJGmWkmwB/Avw6qq6ZdT1aP1V1a+q6on0nuyzS5LHjbomrbskzwOur6oVo65lWAxw3THQY8Mkza8km9ALb5+rqi+Ouh7Njaq6CTgTz1ftqqcD+ya5gt4pR7sl+exoS5pbBrju8LFh0gYmSYDjgEuq6gOjrkezk2RxkkVt+EHAc4AfjbYqrY+qOrKqllbVMnq/l2dU1QtHXNacMsB1RFXdDUw8NuwS4CQfG9ZNST4PnAU8JsmqJIeMuiatt6cDL6L3v/sftNdzR12U1tt2wJlJLqD3n5ReZccAAAGfSURBVObTq2rB3X5CC4NPYpAkSeoYj8BJkiR1jAFOkiSpYwxwkiRJHWOAkyRJ6hgDnCRJUscY4CRpkiS3zTzXr+c9Oslrh7V+SZqKAU6SJKljDHCSNIAk+yQ5J8n5Sb6ZZNu+yU9IclaSy5K8vG+Zv01ybpILkrxlBGVLWqAMcJI0mO8Bu1bVk+g9W/F1fdMeD+wGPA14c5Ltk+wO7ATsAjwReEqSZ8xzzZIWqI1HXYAkdcRS4MQk2wGbAj/pm3ZKVf0c+HmSM+mFtj8EdgfOb/NsQS/QfXf+Spa0UBngJGkwfw98oKpOTfJM4Oi+aZOfSVhAgHdW1cfnpzxJ9yd2oUrSYLYCrm7DB0+atjzJZkkeCjyT3oPQvw68LMkWAEmWJHnYfBUraWHzCJwk3dfmSVb1jX+A3hG3LyS5ETgD2LFv+gXAmcA2wNuq6hrgmiS/C5yVBOA24IXA9cMvX9JCl6rJR/4lSZK0IbMLVZIkqWMMcJIkSR1jgJMkSeoYA5wkSVLHGOAkSZI6xgAnSZLUMQY4SZKkjjHASZIkdcz/B8DlklDCxCjNAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0     466\n",
      "1    1443\n",
      "2     773\n",
      "3    2658\n",
      "4     316\n",
      "Name: image_id, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmoAAAFNCAYAAACwk0NsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3debQlZX3u8e8jDaIBAaUlzI1KvKJGNARJHEIcEFFAverVKBJFMfdCxIhR9KoNzkmUoNEYUVniEBGHKGoniAga40SDBAT00kFYNLR0IwINKIL87h/1Hth9coZNc/bZ1ae/n7X2OrXfmn5VtWE//VbVrlQVkiRJ6p97jbsASZIkTc2gJkmS1FMGNUmSpJ4yqEmSJPWUQU2SJKmnDGqSJEk9ZVCT7oEkxyb51BjXf3aSl7fhFyX5+hwu+6Ik+7bhOd3OJG9M8tG5Wt7dWO+zk1yZ5KYkjx5i+jv37zzUtl2SbydZm+S9s0y7b5KVM4z/eJK3z32V/ZFkSZJKsmjI6Rf8PtHCZFCTZpHkz5Isb1/uq5L8a5LHj7uuyarq01W132zTDfuFVVUPr6qz72ldU4WKqnpnVc1LAJrkPcCRVbVFVf1oLhec5PIkT7kHizgcuBa4X1UdPUdljczdDUobkvkM6NJsDGrSDJK8BjgBeCewHbAL8I/AweOsa5QW4hfvgF2Bi8ZdxDR2BS6ujeRXyBf450yaMwY1aRpJtgLeChxRVV+sqpur6raq+kpV/fU083wuyc+T3NBOYz18YNwBSS5up7auSvLa1r5tkq8muT7JdUn+PcmU/20meWqSn7TlfwDIwLg/T/KdNpwkf59kdZIbk1yY5BFJDgdeBLyu9RB+pU1/eZLXJ7kAuDnJoil6iDZP8tlW/3lJHjWw7krykIH3H0/y9iS/A/wrsENb301Jdph8KjXJQe1U6/WtN+NhA+MuT/LaJBe07f5sks2n2T/3SvKmJFe0bf9Ekq2S3DvJTcAmwH8m+a/12L8PTvLNJL9Icm2STyfZuo37JF2I/0rbxtfN9nmYtN6PA4cOHJentJpPSHJ1e52Q5N7TzP/odkzWJvksMOX+mU6SlyW5JMkvk5yeZNfW/vokP5gIVUn+dztOmwPfbrNf32r+o5mW1cZVkiOSXApcmtbbmuTodrxWJXnpwPTPSPKj9hm+Msmxd2Obpt0nSbZJ99/cmlbnV5Ps1Ma9A3gC8IG2XR9o7e9rNdyY5NwkT7g7+1hab1Xly5evKV7A/sDtwKIZpjkW+NTA+5cBWwL3puuJO39g3CrgCW14G+AxbfhdwD8Bm7bXE4BMsa5tgbXAc9t0f9Xqe3kb/+fAd9rw04Bzga3pwsbDgO3buI8Db5+07MuB84GdgfsMtD1lYDtvG1j3a4GfAZu28QU8ZGB5d64D2BdYOd1+A34PuBl4alv264AVwGYDdfwQ2AG4P3AJ8BfTHI+XtXkfBGwBfBH45MD4deq8m/v3Ia3GewOL6YLKCZP24VOmqGfKz8MU61/nuND9I+H7wAPb+r4LvG3yPgU2A65o9W7a6r9t0rKuBx4/zXoPbvvsYcAi4E3Ad9u4e7XtPBbYHfgl8Og2bknbn4uGWdbA/j+jHcf7tO24vW3rpsABwC3ANgPb+chWx+8D1wDPmm79A+uZcZ8ADwD+J3Dfdnw+B3xpYP6zJ477QNuL23yLgKOBnwObj/v/U74W/sseNWl6DwCurarbh52hqk6qqrVVdSvdl9uj0vXMQfdFsUeS+1XVL6vqvIH27YFdq+ux+/eqmur01wHARVX1+aq6je6L/+fTlHIb3RfQ/6ALfZdU1apZyn9/VV1ZVb+aZvy5A+s+nq6HYp9ZljmM/wV8rarOaMt+D92X+B9Pqu3qqroO+Aqw5zTLehFwfFVdVlU3AW8AXpDhTrPNuH+rakWr8daqWkO3D/5kpgXO8nmYzYuAt1bV6ra+44BDpphuH7owckL7/HweOGdSHVtX1XemWc9fAO9qn5Hb6U7z75lk16q6A3gJ8CrgNOBva+Zr+6Zd1sA076qq6wY+Z7e17bytqpYBNwEPbXWfXVUXVtUdVXUB8Blm2efD7JOq+kVVfaGqbqmqtcA7ZltuVX2qzXd7Vb2XLnw/dIhapHvEoCZN7xfAtkN+yZNkkyTvTvJfSW6k62GBrqcGun/BHwBckeRbE6eKgL+j64X4epLLkhwzzSp2AK6ceNPC3JVTTVhV3wQ+AHwQWJ3kxCT3m2UTplzWVOPbF/jKVtM9tQNd78fgsq8EdhyYZjCQ3kLXWzbrstrwIrrrC4epY9r9m+6uzFPSnba+EfgUdx3b/2aIz8Mw9Uzelqn29w7AVZPC/RVTTDedXYH3tdPO1wPX0fXC7ghQVZcDZ9H1YH3wniyrmfw5+8WkfwzdeXyTPDbJWe0U5Q10QXCY/TfjPkly3yQfbqfIb6TrNdw6ySbTLTDd6fdL2mns64GthqxFukcMatL0vgfcCjxryOn/jO7Uz1Po/ie+pLUHoKrOqaqD6U5lfQk4tbWvraqjq+pBwEHAa5I8eYrlr6I7NdktNMng+8mq6v1V9QfAHnSnFyeuq5vuYvXZLmIfXPe9gJ2Aq1vTLXSnkSb87t1Y7tV0X/ATy57YrqtmmW/WZdFdN3Y73Smz2cy2f99Jty2PrKr70Z0Ky8D4yds54+dhCFNty9VTTLcK2LHVOzjtsK4EXtl63SZe96mq70J3nRjwR8CZdP+omDDVcZ1xWTPMN51/puvJ27mqtqK7RGCY/TfbPjmarjfsse1YPrG1T0y/To3terTXAc+nOy27NXDDkLVI94hBTZpGVd0AvAX4YJJntX+Fb5rk6Un+dopZtqQLdr+gCy3vnBiRZLN0v3O2VTutdiNwRxv3zCQPaV8qNwC/nRg3ydeAhyd5TuvlexXrBqI7JfnD1huxKd31X78eWOY1dNdw3V1/MLDuV7dt/X4bdz7wZ60XaX/WPY10DfCAGU75nQo8I8mTW71Ht2V/d5rpZ/IZ4K+S7JZkC7pj8NkhT1/Ptn+3pDstd0OSHbkr+E6YvF+n/TzcjW15U5LFSbal+yxO9Vt236MLo69qn8/nAHvfjfX8E/CGtBsd0t188bw2vC3wUeDldDc7HJjkgDbfGrrP1IOGWdZ62hK4rqp+nWRvuvA7jNn2yZbAr+huhLg/sHTS/FMdy9vptnlRkrcAs/VQS3PCoCbNoF2L8hq6i6LX0PUYHEnXIzbZJ+hOr1wFXMxdIWbCIcDl7VTLX9BdgwTdRdrfoAsB3wP+sarOmqKWa4HnAe+m+/LfHfiPaUq/H/ARuou/r2jTT/SGfIzuWrnrk0y1HdP5Mt31ZL9s2/KcFjoBjgIOpLto/UUM7J+q+gld6LisrXOd03dV9VO63ql/oPsdsQOBA6vqN3ejtgknAZ+kO5X1M7qA+pfDzDjE/j0OeAxdmP4a3Y0Kg95FF6yuT3dH72yfh9m8HVgOXABcCJzX2ibX/RvgOXQ3k1xHd4zWqa3dvTjlXYpV9S/A3wCntM/mj4Gnt9EnAl+uqmVV9QvgMOCjSR5QVbfQXdv1H22b95llWevj/wBvTbKWLqieOsxMQ+yTE+iug7yW7rj826RFvA94bro7Qt8PnN6m+X90x/TXzH6pgDQnMvU1y5IkSRo3e9QkSZJ6yqAmSZLUUwY1SZKknjKoSZIk9ZRBTZIkqaeG+sX1Dc22225bS5YsGXcZkiRJszr33HOvrarFU41bkEFtyZIlLF++fNxlSJIkzSrJtI9989SnJElSTxnUJEmSesqgJkmS1FMGNUmSpJ4yqEmSJPWUQU2SJKmnDGqSJEk9ZVCTJEnqKYOaJElSTxnUJEmSesqgJkmS1FML8lmfkqSF4bgcN+4S1svSWjruErRA2KMmSZLUUwY1SZKknjKoSZIk9ZRBTZIkqacMapIkST1lUJMkSeopg5okSVJPGdQkSZJ6yqAmSZLUUwY1SZKknjKoSZIk9dTIglqSnZOcleTiJBclOaq1H5vkqiTnt9cBA/O8IcmKJD9N8rSB9v1b24okx4yqZkmSpD4Z5UPZbweOrqrzkmwJnJvkjDbu76vqPYMTJ9kDeAHwcGAH4BtJfq+N/iDwVGAlcE6S06rq4hHWLkmSNHYjC2pVtQpY1YbXJrkE2HGGWQ4GTqmqW4GfJVkB7N3GraiqywCSnNKmNahJkqQFbV6uUUuyBHg08IPWdGSSC5KclGSb1rYjcOXAbCtb23Ttk9dxeJLlSZavWbNmjrdAkiRp/o08qCXZAvgC8OqquhH4EPBgYE+6Hrf3zsV6qurEqtqrqvZavHjxXCxSkiRprEZ5jRpJNqULaZ+uqi8CVNU1A+M/Any1vb0K2Hlg9p1aGzO0S5IkLVijvOszwMeAS6rq+IH27Qcmezbw4zZ8GvCCJPdOshuwO/BD4Bxg9yS7JdmM7oaD00ZVtyRJUl+MskftccAhwIVJzm9tbwRemGRPoIDLgVcCVNVFSU6lu0ngduCIqvotQJIjgdOBTYCTquqiEdYtSZLUC6O86/M7QKYYtWyGed4BvGOK9mUzzSdJkrQQ+WQCSZKknjKoSZIk9ZRBTZIkqacMapIkST1lUJMkSeopg5okSVJPGdQkSZJ6yqAmSZLUUwY1SZKknjKoSZIk9ZRBTZIkqacMapIkST1lUJMkSeopg5okSVJPGdQkSZJ6yqAmSZLUUwY1SZKknjKoSZIk9ZRBTZIkqacMapIkST1lUJMkSeopg5okSVJPGdQkSZJ6yqAmSZLUUwY1SZKknjKoSZIk9ZRBTZIkqacMapIkST1lUJMkSeopg5okSVJPGdQkSZJ6yqAmSZLUUwY1SZKknjKoSZIk9ZRBTZIkqacMapIkST1lUJMkSeopg5okSVJPGdQkSZJ6amRBLcnOSc5KcnGSi5Ic1drvn+SMJJe2v9u09iR5f5IVSS5I8piBZR3apr80yaGjqlmSJKlPRtmjdjtwdFXtAewDHJFkD+AY4Myq2h04s70HeDqwe3sdDnwIumAHLAUeC+wNLJ0Id5IkSQvZyIJaVa2qqvPa8FrgEmBH4GDg5DbZycCz2vDBwCeq831g6yTbA08Dzqiq66rql8AZwP6jqluSJKkv5uUatSRLgEcDPwC2q6pVbdTPge3a8I7AlQOzrWxt07VLkiQtaCMPakm2AL4AvLqqbhwcV1UF1Byt5/Aky5MsX7NmzVwsUpIkaaxGGtSSbEoX0j5dVV9szde0U5q0v6tb+1XAzgOz79TapmtfR1WdWFV7VdVeixcvntsNkSRJGoNR3vUZ4GPAJVV1/MCo04CJOzcPBb480P6SdvfnPsAN7RTp6cB+SbZpNxHs19okSZIWtEUjXPbjgEOAC5Oc39reCLwbODXJYcAVwPPbuGXAAcAK4BbgpQBVdV2StwHntOneWlXXjbBuSZKkXhhZUKuq7wCZZvSTp5i+gCOmWdZJwElzV50kSVL/+WQCSZKknjKoSZIk9ZRBTZIkqacMapIkST1lUJMkSeopg5okSVJPGdQkSZJ6yqAmSZLUUwY1SZKknjKoSZIk9ZRBTZIkqacMapIkST01VFBL8vgkL23Di5PsNtqyJEmSNGtQS7IUeD3whta0KfCpURYlSZKk4XrUng0cBNwMUFVXA1uOsihJkiQNF9R+U1UFFECS3xltSZIkSYLhgtqpST4MbJ3kFcA3gI+MtixJkiQtmm2CqnpPkqcCNwIPBd5SVWeMvDJJkqSN3KxBDaAFM8OZJEnSPJo1qCVZS7s+bcANwHLg6Kq6bBSFSZIkbeyG6VE7AVgJ/DMQ4AXAg4HzgJOAfUdVnCRJ0sZsmJsJDqqqD1fV2qq6sapOBJ5WVZ8FthlxfZIkSRutYYLaLUmen+Re7fV84Ndt3ORTopIkSZojwwS1FwGHAKuBa9rwi5PcBzhyhLVJkiRt1Ib5eY7LgAOnGf2duS1HkiRJE4a563Nz4DDg4cDmE+1V9bIR1iVJkrTRG+bU5yeB3wWeBnwL2AlYO8qiJEmSNFxQe0hVvRm4uapOBp4BPHa0ZUmSJGmYoHZb+3t9kkcAWwEPHF1JkiRJguF+8PbEJNsAbwZOA7YA3jLSqiRJkjTUXZ8fbYPfAh402nIkSZI0YZi7PrcGXgIsGZy+ql41urIkSZI0zKnPZcD3gQuBO0ZbjiRJkiYME9Q2r6rXjLwSSZIkrWOo31FL8ook2ye5/8Rr5JVJkiRt5IbpUfsN8HfA/+Wuh7AX3lggSZI0UsMEtaPpfvT22lEXI0mSpLsMc+pzBXDLqAuRJEnSuobpUbsZOD/JWcCtE43+PIckSdJoDRPUvtRekiRJmkfDPJng5PVZcJKTgGcCq6vqEa3tWOAVwJo22Ruralkb9wbgMOC3wKuq6vTWvj/wPmAT4KNV9e71qUeSJGlDM21QS3JqVT0/yYXcdbfnnarq92dZ9seBDwCfmNT+91X1nknr2gN4AfBwYAfgG0l+r43+IPBUYCVwTpLTquriWdYtSZK0wZupR+2o9veZ67Pgqvp2kiVDTn4wcEpV3Qr8LMkKYO82bkVVXQaQ5JQ2rUFNkiQteNMGtapa1f5eMcfrPDLJS4DlwNFV9UtgR7rHVE1Y2doArpzU/tg5rkeSJKmXhvl5jrn0IeDBwJ7AKuC9c7XgJIcnWZ5k+Zo1a2afQZIkqefmNahV1TVV9duqugP4CHed3rwK2Hlg0p1a23TtUy37xKraq6r2Wrx48dwXL0mSNM+mDWpJzmx//2auVpZk+4G3zwZ+3IZPA16Q5N5JdgN2B34InAPsnmS3JJvR3XBw2lzVI0mS1Gcz3UywfZI/Bg5qF/FncGRVnTfTgpN8BtgX2DbJSmApsG+SPenuIr0ceGVb1kVJTqW7SeB24Iiq+m1bzpHA6XQ/z3FSVV10dzdSkiRpQzRTUHsL8Ga6043HTxpXwJNmWnBVvXCK5o/NMP07gHdM0b4MWDbTuiRJkhaime76/Dzw+SRvrqq3zWNN0rw7LseNu4T1srSWjrsESdIIDfNkgrclOQh4Yms6u6q+OtqyJEmSNOtdn0neRffjtxe311FJ3jnqwiRJkjZ2wzyU/RnAnu0nNUhyMvAj4I2jLEySJGljN+zvqG09MLzVKAqRJEnSuobpUXsX8KMkZ9H9RMcTgWNGWpUkSZKGupngM0nOBv6wNb2+qn4+0qokSZI0VI/axAPafSKAJEnSPJrvh7JLkiRpSAY1SZKknpoxqCXZJMlP5qsYSZIk3WXGoNYejP7TJLvMUz2SJElqhrmZYBvgoiQ/BG6eaKyqg0ZWlSRJkoYKam8eeRWSJEn6b4b5HbVvJdkV2L2qvpHkvsAmoy9NkiRp4zbMQ9lfAXwe+HBr2hH40iiLkiRJ0nA/z3EE8DjgRoCquhR44CiLkiRJ0nBB7daq+s3EmySLgBpdSZIkSYLhgtq3krwRuE+SpwKfA74y2rIkSZI0TFA7BlgDXAi8ElgGvGmURUmSJGm4uz7vSHIy8AO6U54/rSpPfUqSJI3YrEEtyTOAfwL+CwiwW5JXVtW/jro4SZKkjdkwP3j7XuBPq2oFQJIHA18DDGqSJEkjNMw1amsnQlpzGbB2RPVIkiSpmbZHLclz2uDyJMuAU+muUXsecM481CZJkrRRm+nU54EDw9cAf9KG1wD3GVlFkiRJAmYIalX10vksRJIkSesa5q7P3YC/BJYMTl9VB42uLEmSJA1z1+eXgI/RPY3gjtGWI0mSpAnDBLVfV9X7R16JJEmS1jFMUHtfkqXA14FbJxqr6ryRVSVJkqShgtojgUOAJ3HXqc9q7yVJkjQiwwS15wEPqqrfjLoYSZIk3WWYJxP8GNh61IVIkiRpXcP0qG0N/CTJOax7jZo/zyFJkjRCwwS1pSOvQpIkSf/NrEGtqr41H4VIkiRpXcM8mWAt3V2eAJsBmwI3V9X9RlmYJEnSxm6YHrUtJ4aTBDgY2GeURUmSJGm4uz7vVJ0vAU8bUT2SJElqZg1qSZ4z8HpukncDvx5ivpOSrE7y44G2+yc5I8ml7e82rT1J3p9kRZILkjxmYJ5D2/SXJjl0PbdTkiRpgzNMj9qBA6+nAWvpTn/O5uPA/pPajgHOrKrdgTPbe4CnA7u31+HAh6ALdnR3nT4W2BtYOhHuJEmSFrphrlF76fosuKq+nWTJpOaDgX3b8MnA2cDrW/snqqqA7yfZOsn2bdozquo6gCRn0IW/z6xPTZIkSRuSaYNakrfMMF9V1dvWY33bVdWqNvxzYLs2vCNw5cB0K1vbdO1T1Xs4XW8cu+yyy3qUJkmS1C8znfq8eYoXwGF0vWD3SOs9q1knHH55J1bVXlW11+LFi+dqsZIkSWMzbY9aVb13YjjJlsBRwEuBU4D3TjffLK5Jsn1VrWqnNle39quAnQem26m1XcVdp0on2s9ez3VLkiRtUGa8maDdpfl24AK6UPeYqnp9Va2eab4ZnAZM3Ll5KPDlgfaXtLs/9wFuaKdITwf2S7JNu4lgv9YmSZK04M10jdrfAc8BTgQeWVU33Z0FJ/kMXW/YtklW0t29+W7g1CSHAVcAz2+TLwMOAFYAt9D13FFV1yV5G3BOm+6tEzcWSJIkLXQz3fV5NHAr8Cbg/3YPJQAgdJeYzfgIqap64TSjnjzFtAUcMc1yTgJOmmldkiRJC9FM16jdracWSJIkaW4ZxiRJknrKoCZJktRTBjVJkqSeMqhJkiT1lEFNkiSppwxqkiRJPWVQkyRJ6imDmiRJUk8Z1CRJknrKoCZJktRTBjVJkqSeMqhJkiT1lEFNkiSppwxqkiRJPWVQkyRJ6imDmiRJUk8tGncBkjSXjstx4y5hvS2tpeMuQVLP2KMmSZLUUwY1SZKknjKoSZIk9ZRBTZIkqacMapIkST1lUJMkSeopg5okSVJPGdQkSZJ6yqAmSZLUUwY1SZKknjKoSZIk9ZRBTZIkqacMapIkST1lUJMkSeopg5okSVJPGdQkSZJ6yqAmSZLUUwY1SZKknjKoSZIk9ZRBTZIkqacMapIkST01lqCW5PIkFyY5P8ny1nb/JGckubT93aa1J8n7k6xIckGSx4yjZkmSpPk2zh61P62qPatqr/b+GODMqtodOLO9B3g6sHt7HQ58aN4rlSRJGoM+nfo8GDi5DZ8MPGug/RPV+T6wdZLtx1GgJEnSfBpXUCvg60nOTXJ4a9uuqla14Z8D27XhHYErB+Zd2dokSZIWtEVjWu/jq+qqJA8Ezkjyk8GRVVVJ6u4ssAW+wwF22WWXuatUkiRpTMbSo1ZVV7W/q4F/AfYGrpk4pdn+rm6TXwXsPDD7Tq1t8jJPrKq9qmqvxYsXj7J8SZKkeTHvQS3J7yTZcmIY2A/4MXAacGib7FDgy234NOAl7e7PfYAbBk6RSpIkLVjjOPW5HfAvSSbW/89V9W9JzgFOTXIYcAXw/Db9MuAAYAVwC/DS+S95asfluHGXsF6W1tJxlyBJkoYw70Gtqi4DHjVF+y+AJ0/RXsAR81CaJElSr/Tp5zkkSZI0wKAmSZLUUwY1SZKknjKoSZIk9ZRBTZIkqacMapIkST1lUJMkSeopg5okSVJPGdQkSZJ6yqAmSZLUUwY1SZKknjKoSZIk9ZRBTZIkqacWjbsASZK08B2X48ZdwnpZWkvHun571CRJknrKoCZJktRTBjVJkqSeMqhJkiT1lEFNkiSppwxqkiRJPWVQkyRJ6imDmiRJUk8Z1CRJknrKoCZJktRTBjVJkqSeMqhJkiT1lEFNkiSppwxqkiRJPWVQkyRJ6imDmiRJUk8Z1CRJknrKoCZJktRTBjVJkqSeMqhJkiT1lEFNkiSppwxqkiRJPWVQkyRJ6imDmiRJUk8Z1CRJknrKoCZJktRTG0xQS7J/kp8mWZHkmHHXI0mSNGobRFBLsgnwQeDpwB7AC5PsMd6qJEmSRmuDCGrA3sCKqrqsqn4DnAIcPOaaJEmSRmpDCWo7AlcOvF/Z2iRJkhasVNW4a5hVkucC+1fVy9v7Q4DHVtWRA9McDhze3j4U+Om8Fzq3tgWuHXcRmhMey4XDY7mweDwXjg39WO5aVYunGrFovitZT1cBOw+836m13amqTgROnM+iRinJ8qraa9x16J7zWC4cHsuFxeO5cCzkY7mhnPo8B9g9yW5JNgNeAJw25pokSZJGaoPoUauq25McCZwObAKcVFUXjbksSZKkkdogghpAVS0Dlo27jnm0YE7jymO5gHgsFxaP58KxYI/lBnEzgSRJ0sZoQ7lGTZIkaaNjUOsZH5W1cCQ5KcnqJD8edy26Z5LsnOSsJBcnuSjJUeOuSesnyeZJfpjkP9uxPG7cNemeSbJJkh8l+eq4axkFg1qP+KisBefjwP7jLkJz4nbg6KraA9gHOML/NjdYtwJPqqpHAXsC+yfZZ8w16Z45Crhk3EWMikGtX3xU1gJSVd8Grht3HbrnqmpVVZ3XhtfSfSn4dJQNUHVuam83bS8v1t5AJdkJeAbw0XHXMioGtX7xUVlSzyVZAjwa+MF4K9H6aqfKzgdWA2dUlcdyw3UC8DrgjnEXMioGNUkaUpItgC8Ar66qG8ddj9ZPVf22qvake8rN3kkeMe6adPcleSawuqrOHXcto2RQ65dZH5UlaTySbEoX0j5dVV8cdz2656rqeuAsvJZ0Q/U44KAkl9NdKvSkJJ8ab0lzz6DWLz4qS+qhJAE+BlxSVcePux6tvySLk2zdhu8DPBX4yXir0vqoqjdU1U5VtYTu+/KbVfXiMZc15wxqPVJVtwMTj8q6BDjVR2VtuJJ8Bvge8NAkK5McNu6atN4eBxxC9y/289vrgHEXpfWyPXBWkgvo/nF8RlUtyJ910MLgkwkkSZJ6yh41SZKknjKoSZIk9ZRBTZIkqacMapIkST1lUJMkSeopg5qkjVaSm2af6s5pj03y2lEtX5KmYlCTJEnqKYOaJA1IcmCSHyT5UZJvJNluYPSjknwvyaVJXjEwz18nOSfJBUmOG0PZkhYog5okres7wD5V9Wi65we+bmDc7wNPAv4IeEuSHZLsB+wO7A3sCfxBkifOc82SFqhF4y5AknpmJ+CzSbYHNgN+NjDuy1X1K+BXSc6iC2ePB/YDftSm2YIuuH17/urPhKkAAADKSURBVEqWtFAZ1CRpXf8AHF9VpyXZFzh2YNzkZ+4VEOBdVfXh+SlP0sbEU5+StK6tgKva8KGTxh2cZPMkDwD2pXuo9+nAy5JsAZBkxyQPnK9iJS1s9qhJ2pjdN8nKgffH0/WgfS7JL4FvArsNjL8AOAvYFnhbVV0NXJ3kYcD3kgDcBLwYWD368iUtdKma3JMvSZKkPvDUpyRJUk8Z1CRJknrKoCZJktRTBjVJkqSeMqhJkiT1lEFNkiSppwxqkiRJPWVQkyRJ6qn/Dwm3SI9hXiHPAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0     1336\n",
      "1     3195\n",
      "2     2681\n",
      "3    13184\n",
      "4     2378\n",
      "Name: image_id, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def visualize_class_dis(df, fold, color='maroon'):\n",
    "    x = df.groupby('label').count()\n",
    "    print(x['image_id'])\n",
    "    y = [0,1,2,3,4]\n",
    "\n",
    "    fig = plt.figure(figsize=(10,5))\n",
    "    plt.bar(y, x['image_id'].values, color =color, width = 0.4)\n",
    "    plt.xlabel(\"Label\") \n",
    "    plt.ylabel(\"Number of image\") \n",
    "    plt.title(f\"Class distribution of data fold: {fold}\") \n",
    "\n",
    "    # plt.xticks(x['image_id'])\n",
    "    plt.show()\n",
    "visualize_class_dis(train, 'all data', color='blue')\n",
    "visualize_class_dis(train_external, 'external data', color='purple')\n",
    "visualize_class_dis(train_folds, params['fold'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "if params[\"test_external\"]:\n",
    "    train_folds = merge_data(train_folds, test_external_pseudo)\n",
    "    visualize_class_dis(train_folds, f'fold {fold} add extra pseudo test external data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_data(df, mode=\"undersampling\", val=False):\n",
    "    class_0 = df[df.label==0]\n",
    "    class_1 = df[df.label==1]\n",
    "    class_2 = df[df.label==2]\n",
    "    class_3 = df[df.label==3]\n",
    "    class_4 = df[df.label==4]\n",
    "    if mode == \"undersampling\":\n",
    "        # upsample minority\n",
    "        class_3_downsampled = resample(class_3,\n",
    "                                  replace=True, # sample with replacement\n",
    "                                  n_samples=int(len(class_3)*2/3), # match number in majority class\n",
    "                                  random_state=27) # reproducible results\n",
    "        if val:\n",
    "            return  pd.concat([class_0, class_1, class_2, class_3_downsampled, class_4]) \n",
    "    \n",
    "        class_1_downsampled = resample(class_1,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=int(len(class_1)*0.7), # match number in majority class\n",
    "                          random_state=27) # reproducible results\n",
    "        class_4_upsampled = resample(class_4,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=int(len(class_4)*1.3), # match number in majority class\n",
    "                          random_state=27) # reproducible results\n",
    "        return pd.concat([class_0, class_1_downsampled, class_2, class_3_downsampled, class_4_upsampled]) \n",
    "    else:\n",
    "        class_0_upsampled = resample(class_0,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=int(len(class_3)/4), # match number in majority class\n",
    "                          random_state=27) # reproducible results\n",
    "        class_1_upsampled = resample(class_1,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=int(len(class_3)/3), # match number in majority class\n",
    "                          random_state=27) # reproducible results\n",
    "        class_2_upsampled = resample(class_2,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=int(len(class_3)/3), # match number in majority class\n",
    "                          random_state=27) # reproducible results\n",
    "        class_4_upsampled = resample(class_4,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=int(len(class_3)/3), # match number in majority class\n",
    "                          random_state=27) # reproducible results\n",
    "        return pd.concat([class_0_upsampled, class_1_upsampled, class_2_upsampled, class_3, class_4_upsampled]) \n",
    "\n",
    "    \n",
    "if params[\"balance_data\"]:\n",
    "    train_folds = balance_data(train_folds, mode=\"undersampling\")    \n",
    "    val_folds = balance_data(val_folds, mode=\"undersampling\", val=True)    \n",
    "    \n",
    "#     train_folds = balance_data(train_folds, mode=\"oversamling\")\n",
    "    visualize_class_dis(train_folds, f'fold {fold} after balance data')\n",
    "    visualize_class_dis(val_folds, f'fold {fold} after balance data')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9EtOPkNwh5-8"
   },
   "outputs": [],
   "source": [
    "# Dataset\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, df, transform=None, mosaic_mix = False):\n",
    "        self.df = df\n",
    "        self.file_names = df['image_id'].values\n",
    "        self.labels = df['label'].values\n",
    "        self.transform = transform\n",
    "        self.mosaic_mix = mosaic_mix\n",
    "        self.rand_aug_fn = RandAugment()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_name = self.file_names[idx]\n",
    "        file_path = f'{root}/train_images/{file_name}'\n",
    "        image = cv2.imread(file_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        label = torch.tensor(self.labels[idx]).long()\n",
    "        if params[\"rand_aug\"]:\n",
    "            image = np.array(self.rand_aug_fn(Image.fromarray(image)))\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "\n",
    "        return image, label, file_name\n",
    "    \n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, df, transform=None, valid_test=False, fcrops=False):\n",
    "        self.df = df\n",
    "        self.file_names = df['image_id'].values\n",
    "        self.transform = transform\n",
    "        self.valid_test = valid_test\n",
    "        self.fcrops = fcrops\n",
    "        if self.valid_test:\n",
    "            self.labels = df['label'].values  \n",
    "        else:\n",
    "            assert ValueError(\"Test data does not have annotation, plz check!\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_name = self.file_names[idx]\n",
    "        if self.valid_test:\n",
    "            file_path = f'{root}/train_images/{file_name}'\n",
    "            #file_path = f'{root}/external/extraimages/{file_name}'\n",
    "        else:\n",
    "            file_path = f'{root}/test_images/{file_name}'\n",
    "        image = cv2.imread(file_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        if isinstance(self.transform, list):\n",
    "            outputs = {'images':[],\n",
    "                       'labels':[],\n",
    "                       'image_ids':[]}\n",
    "            if self.fcrops:\n",
    "                for trans in self.transform:\n",
    "                    image_aug = transforms.ToPILImage()(image)\n",
    "                    image_aug = trans(image_aug)\n",
    "                    outputs[\"images\"].append(image_aug)\n",
    "                    del image_aug\n",
    "            else:\n",
    "                for trans in self.transform:\n",
    "                    augmented = trans(image=image)\n",
    "                    image_aug = augmented['image']\n",
    "                    outputs[\"images\"].append(image_aug)\n",
    "                    del image_aug\n",
    "\n",
    "            if self.valid_test:\n",
    "                label = torch.tensor(self.labels[idx]).long()\n",
    "                outputs['labels'] = len(self.transform)*[label]\n",
    "                outputs['image_ids'].append(file_name)\n",
    "                \n",
    "            else:\n",
    "                outputs['labels'] = len(self.transform)*[-1]\n",
    "                \n",
    "            return outputs\n",
    "        else:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image'] \n",
    "        return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Albumentations to define transformation functions for the train and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mm_NP-c4h5-_"
   },
   "outputs": [],
   "source": [
    "train_transform = A.Compose(\n",
    "    [\n",
    "        A.RandomResizedCrop(height=params[\"image_size\"], width=params[\"image_size\"], p=1),\n",
    "        A.OneOf([\n",
    "            A.RandomRotate90(p=0.5),\n",
    "            A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),], p=1.\n",
    "        ),\n",
    "#         A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.IAAAffine(rotate=0.2, shear=0.2,p=0.5),\n",
    "        A.CoarseDropout(max_holes=20, max_height=int(params[\"image_size\"]/15), max_width=int(params[\"image_size\"]/15), p=0.5),\n",
    "#         A.IAAAdditiveGaussianNoise(p=1.),\n",
    "        A.MedianBlur(p=0.5),\n",
    "        A.Equalize(p=0.2),\n",
    "        A.GridDistortion(p=0.2),\n",
    "#         A.RandomGridShuffle(grid=(100, 100), p=0.5),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "val_transform = A.Compose(\n",
    "    [\n",
    "        A.CenterCrop(height=params[\"image_size\"], width=params[\"image_size\"], p=1),\n",
    "        A.Resize(params[\"image_size\"],params[\"image_size\"]),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "if params[\"cutmix\"]:\n",
    "    mixup_fn = Mixup(mixup_alpha=1., cutmix_alpha=1., label_smoothing=params[\"smooth_label\"], num_classes=params[\"num_classes\"])\n",
    "else:\n",
    "    mixup_fn = Mixup(mixup_alpha=1., label_smoothing=params[\"smooth_label\"], num_classes=params[\"num_classes\"])\n",
    "\n",
    "train_dataset = TrainDataset(train_folds, transform=train_transform)\n",
    "val_dataset = TrainDataset(val_folds, transform=val_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also let's define a function that takes a dataset and visualizes different augmentations applied to the same image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_augmentations(dataset, idx=3, samples=10, cols=5):\n",
    "    dataset = copy.deepcopy(dataset)\n",
    "    dataset.transform = A.Compose([t for t in dataset.transform if not isinstance(t, (A.Normalize, ToTensorV2))])\n",
    "    rows = samples // cols\n",
    "    figure, ax = plt.subplots(nrows=rows, ncols=cols, figsize=(12, 6))\n",
    "    for i in range(samples):\n",
    "        image, _, _ = dataset[idx]\n",
    "        ax.ravel()[i].imshow(image)\n",
    "        ax.ravel()[i].set_axis_off()\n",
    "    plt.tight_layout()\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "if params[\"visualize\"]:\n",
    "    random.seed(SEED)\n",
    "    visualize_augmentations(train_dataset, idx=random.randint(1,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnNormalize(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
    "        Returns:\n",
    "            Tensor: Normalized image.\n",
    "        \"\"\"\n",
    "        for t, m, s in zip(tensor, self.mean, self.std):\n",
    "            t.mul_(s).add_(m)\n",
    "            # The normalize code -> t.sub_(m).div_(s)\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Mixup CutMix Aug\n",
    "def visualize_mix_augmentations(dataset, idx=[4,5,1000,3], cols=4):\n",
    "    dataset = copy.deepcopy(dataset)\n",
    "    rows = 1\n",
    "    figure, ax = plt.subplots(nrows=rows, ncols=cols, figsize=(20, 15))\n",
    "    images = []\n",
    "    labels = []\n",
    "    for i in idx:\n",
    "        image, label, _ = dataset[i]\n",
    "        images.append(image)\n",
    "        labels.append(label)\n",
    "        \n",
    "    images, labels = mixup_fn(torch.stack(images).cuda(), torch.stack(labels).float().cuda())\n",
    "    for i, (image,label) in enumerate(zip(images, labels)):\n",
    "        unorm = UnNormalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "        image = (unorm(image).cpu().numpy()*255).astype(int)\n",
    "        ax.ravel()[i].imshow(image.transpose(1,2,0))\n",
    "        ax.ravel()[i].set_title(label, color='GREEN')\n",
    "        ax.ravel()[i].set_axis_off()\n",
    "    plt.tight_layout()\n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "if params[\"visualize\"] and params[\"mix_up\"]:\n",
    "    random.seed(SEED)\n",
    "    visualize_mix_augmentations(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define helpers for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a few helpers for our training pipeline. `calculate_accuracy` takes model predictions and true labels and will return accuracy for those predictions. `MetricMonitor` helps to track metrics such as accuracy or loss during training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bgTqE0eYSjDh"
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(output, target):\n",
    "#     return torch.true_divide((target == output).sum(dim=0), output.size(0)).item()\n",
    "    if params[\"mix_up\"]:\n",
    "        output = torch.argmax(torch.softmax(output, dim=1), dim=1)\n",
    "        return accuracy_score(output.cpu(), target.argmax(1).cpu())\n",
    "    \n",
    "    output = torch.softmax(output, dim=1)\n",
    "    return accuracy_score(output.argmax(1).cpu(), target.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zRhPJiCch5_D"
   },
   "outputs": [],
   "source": [
    "class MetricMonitor:\n",
    "    def __init__(self, float_precision=3):\n",
    "        self.float_precision = float_precision\n",
    "        self.reset()\n",
    "        self.curr_acc = 0.\n",
    "    def reset(self):\n",
    "        self.metrics = defaultdict(lambda: {\"val\": 0, \"count\": 0, \"avg\": 0})\n",
    "\n",
    "    def update(self, metric_name, val):\n",
    "        metric = self.metrics[metric_name]\n",
    "\n",
    "        metric[\"val\"] += val\n",
    "        metric[\"count\"] += 1\n",
    "        metric[\"avg\"] = metric[\"val\"] / metric[\"count\"]\n",
    "        self.curr_acc = metric[\"avg\"]\n",
    "    def __str__(self):\n",
    "        return \" | \".join(\n",
    "            [\n",
    "                \"{metric_name}: {avg:.{float_precision}f}\".format(\n",
    "                    metric_name=metric_name, avg=metric[\"avg\"], float_precision=self.float_precision\n",
    "                )\n",
    "                for (metric_name, metric) in self.metrics.items()\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "if params[\"hard_negative_sample\"]:\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=1, shuffle=True, num_workers=params[\"num_workers\"], pin_memory=True,\n",
    "    )\n",
    "else:\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=params[\"batch_size\"], shuffle=True, num_workers=params[\"num_workers\"], pin_memory=True,\n",
    "    )\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=params[\"batch_size\"], shuffle=False, num_workers=params[\"num_workers\"], pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_hard_sample(train_loader, model, val_criterion, thres):\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(12, 6))\n",
    "    train_loss_list = {'image_id':[],\n",
    "                       'label':[],\n",
    "                       'loss':[],\n",
    "                       'fold':[]}\n",
    "    model.eval()\n",
    "    stream = tqdm(train_loader)\n",
    "    with torch.no_grad():\n",
    "        for i, (images, target, name) in enumerate(stream, start=1):\n",
    "            images = images.to(params[\"device\"], non_blocking=True)\n",
    "            target = target.to(params[\"device\"], non_blocking=True)#.view(-1,params['batch_size'])\n",
    "            output = model(images)\n",
    "            loss = val_criterion(output, target)\n",
    "            if loss > thres:\n",
    "                train_loss_list['image_id'].append(name[0])\n",
    "                train_loss_list['label'].append(int(target[0].cpu().numpy()))\n",
    "                train_loss_list['loss'].append(loss)\n",
    "                train_loss_list['fold'].append(params['fold'])\n",
    "    print(\"Number hard samples:\",len(train_loss_list[\"loss\"]))\n",
    "    #visualize\n",
    "    ax.ravel()[0].plot(train_loss_list['loss'])\n",
    "    ax.ravel()[0].set_title(\"Loss\", color='BLUE')\n",
    "    #ax.ravel()[0].set_axis_off() \n",
    "    \n",
    "    ax.ravel()[1].plot(sorted(train_loss_list['loss']))\n",
    "    ax.ravel()[1].set_title(\"Sort Curve\", color='BLUE')\n",
    "    #ax.ravel()[1].set_axis_off() \n",
    "    \n",
    "    prob = 10 / (abs(sorted(train_loss_list['loss']) - sorted(train_loss_list['loss']).mean()) + 10)\n",
    "    ax.ravel()[2].plot(prob)\n",
    "    ax.ravel()[2].set_title(\"Sort reshape Curve\", color='GREEN')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()  \n",
    "        \n",
    "    return dict(image_id=train_loss_list['image_id'],\n",
    "                label=train_loss_list['label'],\n",
    "                fold=train_loss_list['fold'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define training parameters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a few training parameters such as model architecture, learning rate, batch size, epochs, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"efficientnet\" in params[\"model\"]:\n",
    "    model = timm.create_model(\n",
    "            params[\"model\"],\n",
    "            pretrained=False,\n",
    "            num_classes=params[\"num_classes\"], \n",
    "            drop_rate=params[\"drop_rate\"], \n",
    "            drop_path_rate=0.3)\n",
    "    \n",
    "elif \"skresnet\" in params[\"model\"]:\n",
    "    model = timm.create_model(\n",
    "            params[\"model\"],\n",
    "            pretrained=True,\n",
    "            num_classes=params[\"num_classes\"],\n",
    "            drop_block_rate=params[\"drop_block\"],\n",
    "            drop_path_rate=0.2)\n",
    "else:\n",
    "    model = timm.create_model(\n",
    "            params[\"model\"],\n",
    "            pretrained=True,\n",
    "            num_classes=params[\"num_classes\"],\n",
    "            drop_block_rate=params[\"drop_block\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_features = model.fc.in_features\n",
    "# model.fc = nn.Linear(n_features, params['num_classes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create all required objects and functions for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W7HipbJ8h5_I"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pretrained model:  0.878\n"
     ]
    }
   ],
   "source": [
    "# model = getattr(models, params[\"model\"])(pretrained=False, num_classes=5)\n",
    "model = model.to(params[\"device\"])\n",
    "val_criterion = nn.CrossEntropyLoss().to(params[\"device\"])\n",
    "# criterion = nn.CrossEntropyLoss().to(params[\"device\"])\n",
    "criterion = LabelSmoothingCrossEntropy().to(params[\"device\"])\n",
    "if params[\"mix_up\"]:\n",
    "    criterion = SoftTargetCrossEntropy().to(params[\"device\"])\n",
    "asymetric_criterion = AsymmetricLossSingleLabel().to(params[\"device\"])\n",
    "symetric_criterion = SCELoss(smooth_label=params[\"smooth_label\"]).to(params[\"device\"])\n",
    "\n",
    "if params[\"distributed\"]:\n",
    "    assert ValueError(\"No need to implement in a single machine\")\n",
    "else:\n",
    "    model = torch.nn.DataParallel(model)    \n",
    "if params[\"load_pretrained\"]:\n",
    "    state_dict = torch.load(weights[ckpt_index])\n",
    "    print(\"Load pretrained model: \",state_dict[\"preds\"])\n",
    "    model.load_state_dict(state_dict[\"model\"])\n",
    "    best_acc = state_dict[\"preds\"]\n",
    "    # Hard negative mining based on train data and pretrained model on that data\n",
    "    if params[\"hard_negative_sample\"]:\n",
    "        update_train_data = update_hard_sample(train_loader, model, val_criterion, thres=0.2)\n",
    "        update_train_folds = pd.DataFrame(data=update_train_data)\n",
    "        update_train_folds = pd.concat(5*[update_train_folds])\n",
    "\n",
    "        \n",
    "        #check the update distribution when filter data\n",
    "        print(\"Class distribution for the new data\")\n",
    "        visualize_class_dis(update_train_folds, params[\"fold\"])\n",
    "        \n",
    "        #update the training set\n",
    "        update_train_dataset = TrainDataset(update_train_folds, transform=train_transform)\n",
    "        update_train_loader = DataLoader(\n",
    "            update_train_dataset, batch_size=params[\"batch_size\"], shuffle=True, num_workers=params[\"num_workers\"], pin_memory=True,\n",
    "        )                \n",
    "else:\n",
    "    best_acc = 0.\n",
    "    \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=params[\"lr\"])\n",
    "# scheduler = CosineAnnealingLR(optimizer, T_max=10, eta_min=params[\"lr_min\"], last_epoch=-1)\n",
    "scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=1, eta_min=params[\"lr_min\"], last_epoch=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HNz-5F7Bh5_Y"
   },
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch, params):\n",
    "    metric_monitor = MetricMonitor()\n",
    "    model.train()\n",
    "    if params[\"hard_negative_sample\"]:\n",
    "        stream = tqdm(update_train_loader)\n",
    "    else:\n",
    "        stream = tqdm(train_loader)\n",
    "    for i, (images, target, _) in enumerate(stream, start=1):\n",
    "        images = images.to(params[\"device\"]) #, non_blocking=True)\n",
    "        target = target.to(params[\"device\"]) #, non_blocking=True) #.view(-1,params['batch_size'])\n",
    "        if params[\"mix_up\"]:\n",
    "            images , target = mixup_fn(images, target)\n",
    "        output = model(images)\n",
    "        if isinstance(output, (tuple, list)):\n",
    "            output = output[0]\n",
    "        loss0 = criterion(output, target)\n",
    "#         loss1 = symetric_criterion(output, target)\n",
    "#         loss2 = asymetric_criterion(output, target)\n",
    "        loss = loss0 #+ loss2\n",
    "        if params['gradient_accumulation_steps'] > 1:\n",
    "            loss = loss / params['gradient_accumulation_steps']\n",
    "    \n",
    "        accuracy = calculate_accuracy(output, target)\n",
    "        metric_monitor.update(\"Loss\", loss.item())\n",
    "        metric_monitor.update(\"Accuracy\", accuracy)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "    \n",
    "        optimizer.step()\n",
    "        stream.set_description(\n",
    "            \"Epoch: {epoch}. Train.      {metric_monitor}\".format(epoch=epoch, metric_monitor=metric_monitor)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "chKMqryvh5_a"
   },
   "outputs": [],
   "source": [
    "def validate(val_loader, model, criterion, epoch, params, fold, best_acc):\n",
    "    metric_monitor = MetricMonitor()\n",
    "    model.eval()\n",
    "    stream = tqdm(val_loader)\n",
    "    with torch.no_grad():\n",
    "        for i, (images, target, _) in enumerate(stream, start=1):\n",
    "            images = images.to(params[\"device\"], non_blocking=True)\n",
    "            target = target.to(params[\"device\"], non_blocking=True)#.view(-1,params['batch_size'])\n",
    "            output = model(images)\n",
    "            loss = val_criterion(output, target)\n",
    "            output = torch.softmax(output, dim = 1)\n",
    "            \n",
    "            accuracy = accuracy_score(output.argmax(1).cpu(), target.cpu())\n",
    "\n",
    "            stream.set_description(\n",
    "                \"Epoch: {epoch}. Validation. {metric_monitor}\".format(epoch=epoch, metric_monitor=metric_monitor)\n",
    "            )           \n",
    "            metric_monitor.update(\"Loss\", loss.item())\n",
    "            metric_monitor.update(\"Accuracy\", accuracy)\n",
    "        #to save weight\n",
    "        if (metric_monitor.curr_acc > best_acc): # or epoch == params[\"epochs\"]:\n",
    "            print(f\"Save best weight at acc {round(metric_monitor.curr_acc,4)}, epoch: {epoch}\")\n",
    "            if not os.path.exists(params[\"model\"]):\n",
    "                os.makedirs(params[\"model\"])\n",
    "            torch.save({'model': model.state_dict(), \n",
    "                'loss': loss,\n",
    "                'preds': round(metric_monitor.curr_acc,4)},\n",
    "                 f'weights/{params[\"model\"]}/{params[\"model\"]}_fold{fold}_best_epoch_{epoch}.pth')\n",
    "\n",
    "            best_acc = metric_monitor.curr_acc\n",
    "    return best_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "IiA5Zgfch5_c",
    "outputId": "f4fba281-0997-4202-e9d2-6aa3f93373f0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1. Train.      Loss: 0.965 | Accuracy: 0.772: 100%|██████████| 5694/5694 [13:44<00:00,  6.91it/s]\n",
      "Epoch: 1. Validation. Loss: 0.584 | Accuracy: 0.844: 100%|██████████| 1070/1070 [00:38<00:00, 27.96it/s]\n",
      "  0%|          | 0/5694 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save best weight at acc 0.8438, epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 2. Train.      Loss: 0.961 | Accuracy: 0.778: 100%|██████████| 5694/5694 [13:40<00:00,  6.94it/s]\n",
      "Epoch: 2. Validation. Loss: 0.562 | Accuracy: 0.875: 100%|██████████| 1070/1070 [00:37<00:00, 28.43it/s]\n",
      "  0%|          | 0/5694 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save best weight at acc 0.8745, epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 3. Train.      Loss: 0.957 | Accuracy: 0.780: 100%|██████████| 5694/5694 [13:40<00:00,  6.94it/s]\n",
      "Epoch: 3. Validation. Loss: 0.636 | Accuracy: 0.851: 100%|██████████| 1070/1070 [00:37<00:00, 28.49it/s]\n",
      "Epoch: 4. Train.      Loss: 0.955 | Accuracy: 0.780: 100%|██████████| 5694/5694 [13:40<00:00,  6.94it/s]\n",
      "Epoch: 4. Validation. Loss: 0.530 | Accuracy: 0.875: 100%|██████████| 1070/1070 [00:37<00:00, 28.47it/s]\n",
      "  0%|          | 0/5694 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save best weight at acc 0.8752, epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 5. Train.      Loss: 0.953 | Accuracy: 0.781: 100%|██████████| 5694/5694 [13:40<00:00,  6.94it/s]\n",
      "Epoch: 5. Validation. Loss: 0.541 | Accuracy: 0.859: 100%|██████████| 1070/1070 [00:37<00:00, 28.45it/s]\n",
      "Epoch: 6. Train.      Loss: 0.953 | Accuracy: 0.782: 100%|██████████| 5694/5694 [13:39<00:00,  6.95it/s]\n",
      "Epoch: 6. Validation. Loss: 0.643 | Accuracy: 0.853: 100%|██████████| 1070/1070 [00:37<00:00, 28.47it/s]\n",
      "Epoch: 7. Train.      Loss: 0.947 | Accuracy: 0.785: 100%|██████████| 5694/5694 [13:40<00:00,  6.94it/s]\n",
      "Epoch: 7. Validation. Loss: 0.542 | Accuracy: 0.857: 100%|██████████| 1070/1070 [00:37<00:00, 28.52it/s]\n",
      "Epoch: 8. Train.      Loss: 0.947 | Accuracy: 0.782: 100%|██████████| 5694/5694 [13:40<00:00,  6.94it/s]\n",
      "Epoch: 8. Validation. Loss: 0.495 | Accuracy: 0.870: 100%|██████████| 1070/1070 [00:37<00:00, 28.48it/s]\n",
      "Epoch: 9. Train.      Loss: 0.936 | Accuracy: 0.788: 100%|██████████| 5694/5694 [13:40<00:00,  6.94it/s]\n",
      "Epoch: 9. Validation. Loss: 0.550 | Accuracy: 0.835: 100%|██████████| 1070/1070 [00:37<00:00, 28.50it/s]\n",
      "Epoch: 10. Train.      Loss: 0.945 | Accuracy: 0.785: 100%|██████████| 5694/5694 [13:40<00:00,  6.94it/s]\n",
      "Epoch: 10. Validation. Loss: 0.519 | Accuracy: 0.880: 100%|██████████| 1070/1070 [00:37<00:00, 28.46it/s]\n",
      "  0%|          | 0/5694 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save best weight at acc 0.8801, epoch: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 11. Train.      Loss: 0.941 | Accuracy: 0.784: 100%|██████████| 5694/5694 [14:33<00:00,  6.52it/s]\n",
      "Epoch: 11. Validation. Loss: 0.511 | Accuracy: 0.876: 100%|██████████| 1070/1070 [00:53<00:00, 19.94it/s]\n",
      "Epoch: 12. Train.      Loss: 0.939 | Accuracy: 0.787: 100%|██████████| 5694/5694 [16:57<00:00,  5.60it/s]\n",
      "Epoch: 12. Validation. Loss: 0.687 | Accuracy: 0.822: 100%|██████████| 1070/1070 [00:44<00:00, 23.97it/s]\n",
      "Epoch: 13. Train.      Loss: 0.938 | Accuracy: 0.788: 100%|██████████| 5694/5694 [17:04<00:00,  5.56it/s]\n",
      "Epoch: 13. Validation. Loss: 0.592 | Accuracy: 0.870: 100%|██████████| 1070/1070 [01:15<00:00, 14.11it/s]\n",
      "Epoch: 14. Train.      Loss: 0.941 | Accuracy: 0.788: 100%|██████████| 5694/5694 [24:39<00:00,  3.85it/s]\n",
      "Epoch: 14. Validation. Loss: 0.541 | Accuracy: 0.875: 100%|██████████| 1070/1070 [01:20<00:00, 13.30it/s]\n",
      "Epoch: 15. Train.      Loss: 0.940 | Accuracy: 0.791: 100%|██████████| 5694/5694 [17:11<00:00,  5.52it/s]\n",
      "Epoch: 15. Validation. Loss: 0.559 | Accuracy: 0.877: 100%|██████████| 1070/1070 [00:37<00:00, 28.50it/s]\n",
      "Epoch: 16. Train.      Loss: 0.934 | Accuracy: 0.788: 100%|██████████| 5694/5694 [13:40<00:00,  6.94it/s]\n",
      "Epoch: 16. Validation. Loss: 0.504 | Accuracy: 0.873: 100%|██████████| 1070/1070 [00:37<00:00, 28.48it/s]\n",
      "Epoch: 17. Train.      Loss: 0.929 | Accuracy: 0.792: 100%|██████████| 5694/5694 [13:40<00:00,  6.94it/s]\n",
      "Epoch: 17. Validation. Loss: 0.536 | Accuracy: 0.870: 100%|██████████| 1070/1070 [00:37<00:00, 28.53it/s]\n",
      "Epoch: 18. Train.      Loss: 0.931 | Accuracy: 0.788: 100%|██████████| 5694/5694 [13:41<00:00,  6.93it/s]\n",
      "Epoch: 18. Validation. Loss: 0.611 | Accuracy: 0.830: 100%|██████████| 1070/1070 [00:37<00:00, 28.51it/s]\n",
      "Epoch: 19. Train.      Loss: 0.932 | Accuracy: 0.793: 100%|██████████| 5694/5694 [13:40<00:00,  6.94it/s]\n",
      "Epoch: 19. Validation. Loss: 0.548 | Accuracy: 0.860: 100%|██████████| 1070/1070 [00:37<00:00, 28.51it/s]\n",
      "Epoch: 20. Train.      Loss: 0.926 | Accuracy: 0.796: 100%|██████████| 5694/5694 [13:40<00:00,  6.94it/s]\n",
      "Epoch: 20. Validation. Loss: 0.600 | Accuracy: 0.871: 100%|██████████| 1070/1070 [00:37<00:00, 28.52it/s]\n",
      "Epoch: 21. Train.      Loss: 0.926 | Accuracy: 0.796: 100%|██████████| 5694/5694 [13:40<00:00,  6.94it/s]\n",
      "Epoch: 21. Validation. Loss: 0.486 | Accuracy: 0.875: 100%|██████████| 1070/1070 [00:37<00:00, 28.52it/s]\n",
      "Epoch: 22. Train.      Loss: 0.924 | Accuracy: 0.793: 100%|██████████| 5694/5694 [13:40<00:00,  6.94it/s]\n",
      "Epoch: 22. Validation. Loss: 0.547 | Accuracy: 0.854: 100%|██████████| 1070/1070 [00:37<00:00, 28.57it/s]\n",
      "Epoch: 23. Train.      Loss: 0.922 | Accuracy: 0.795: 100%|██████████| 5694/5694 [13:40<00:00,  6.94it/s]\n",
      "Epoch: 23. Validation. Loss: 0.516 | Accuracy: 0.858: 100%|██████████| 1070/1070 [00:37<00:00, 28.44it/s]\n",
      "Epoch: 24. Train.      Loss: 0.928 | Accuracy: 0.792: 100%|██████████| 5694/5694 [13:40<00:00,  6.94it/s]\n",
      "Epoch: 24. Validation. Loss: 0.561 | Accuracy: 0.862: 100%|██████████| 1070/1070 [00:37<00:00, 28.42it/s]\n",
      "Epoch: 25. Train.      Loss: 0.922 | Accuracy: 0.796: 100%|██████████| 5694/5694 [13:40<00:00,  6.94it/s]\n",
      "Epoch: 25. Validation. Loss: 0.611 | Accuracy: 0.874: 100%|██████████| 1070/1070 [00:37<00:00, 28.48it/s]\n",
      "Epoch: 26. Train.      Loss: 0.923 | Accuracy: 0.794: 100%|██████████| 5694/5694 [13:40<00:00,  6.94it/s]\n",
      "Epoch: 26. Validation. Loss: 0.564 | Accuracy: 0.866: 100%|██████████| 1070/1070 [00:37<00:00, 28.55it/s]\n",
      "Epoch: 27. Train.      Loss: 0.917 | Accuracy: 0.797: 100%|██████████| 5694/5694 [13:41<00:00,  6.94it/s]\n",
      "Epoch: 27. Validation. Loss: 0.540 | Accuracy: 0.844: 100%|██████████| 1070/1070 [00:37<00:00, 28.50it/s]\n",
      "Epoch: 28. Train.      Loss: 0.924 | Accuracy: 0.796: 100%|██████████| 5694/5694 [13:40<00:00,  6.94it/s]\n",
      "Epoch: 28. Validation. Loss: 0.647 | Accuracy: 0.805: 100%|██████████| 1070/1070 [00:37<00:00, 28.48it/s]\n",
      "Epoch: 29. Train.      Loss: 0.915 | Accuracy: 0.798: 100%|██████████| 5694/5694 [13:41<00:00,  6.93it/s]\n",
      "Epoch: 29. Validation. Loss: 0.497 | Accuracy: 0.852: 100%|██████████| 1070/1070 [00:37<00:00, 28.48it/s]\n",
      "Epoch: 30. Train.      Loss: 0.918 | Accuracy: 0.798: 100%|██████████| 5694/5694 [13:40<00:00,  6.94it/s]\n",
      "Epoch: 30. Validation. Loss: 0.662 | Accuracy: 0.839: 100%|██████████| 1070/1070 [00:37<00:00, 28.49it/s]\n"
     ]
    }
   ],
   "source": [
    "best_acc = 0\n",
    "if params[\"train_phase\"]:\n",
    "    for epoch in range(1, params[\"epochs\"] + 1):\n",
    "        train(train_loader, model, criterion, optimizer, epoch, params)\n",
    "        best_acc = validate(val_loader, model, criterion, epoch, params, fold, best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save({'model': model.state_dict(), \n",
    "#     'preds': 0.8},\n",
    "#      f'weights/{params[\"model\"]}_fold{fold}_best_epoch_{epoch}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "classification (1) (1).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "traffic_monitor",
   "language": "python",
   "name": "traffic_monitor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
