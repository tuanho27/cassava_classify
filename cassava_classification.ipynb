{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cassava classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "edyDFX_Ih5-j"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from urllib.request import urlretrieve\n",
    "import pandas as pd\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import resample\n",
    "cudnn.benchmark = True\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n",
    "import timm\n",
    "from timm.loss import LabelSmoothingCrossEntropy, SoftTargetCrossEntropy, JsdCrossEntropy\n",
    "from utils import Mixup, RandAugment, AsymmetricLossSingleLabel, SCELoss\n",
    "from PIL import Image\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(SEED):\n",
    "    random.seed(SEED)\n",
    "    os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] =\"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the root directory dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "waXTuqeVh5-q"
   },
   "outputs": [],
   "source": [
    "root = os.path.join(os.environ[\"HOME\"], \"Workspace/datasets/taiyoyuden/cassava\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split files from the dataset into the train and validation sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some files in the dataset are broken, so we will use only those image files that OpenCV could load correctly. We will use 20000 images for training, 4936 images for validation, and 10 images for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QgLoujrNHY5Z"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test_images',\n",
       " 'label_num_to_disease_map.json',\n",
       " 'train_1_pseudo.csv',\n",
       " 'val_1_pseudo.csv',\n",
       " 'external',\n",
       " 'sample_submission.csv',\n",
       " 'train_images',\n",
       " 'label_num_to_disease_map.json.save',\n",
       " 'train.csv']"
      ]
     },
     "execution_count": 516,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to visualize images and their labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a function that will take a list of images' file paths and their labels and visualize them in a grid. Correct labels are colored green, and incorrectly predicted labels are colored red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000015157.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000201771.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100042118.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000723321.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000812911.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         image_id  label\n",
       "0  1000015157.jpg      0\n",
       "1  1000201771.jpg      3\n",
       "2   100042118.jpg      1\n",
       "3  1000723321.jpg      1\n",
       "4  1000812911.jpg      3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2216849948.jpg</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000015157.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000201771.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         image_id  label\n",
       "0  2216849948.jpg      4\n",
       "1  1000015157.jpg      0\n",
       "2  1000201771.jpg      3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cassava Bacterial Blight (CBB)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cassava Brown Streak Disease (CBSD)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cassava Green Mottle (CGM)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cassava Mosaic Disease (CMD)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Healthy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     0\n",
       "0       Cassava Bacterial Blight (CBB)\n",
       "1  Cassava Brown Streak Disease (CBSD)\n",
       "2           Cassava Green Mottle (CGM)\n",
       "3         Cassava Mosaic Disease (CMD)\n",
       "4                              Healthy"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train = pd.read_csv(f'{root}/train.csv')\n",
    "train_external = pd.read_csv(f'{root}/external/train_external.csv')\n",
    "test_external = pd.read_csv(f'{root}/external/test_external.csv')\n",
    "test_external_pseudo = pd.read_csv(f'{root}/external/test_external_pseudo.csv')\n",
    "test = pd.read_csv(f'{root}/sample_submission.csv')\n",
    "label_map = pd.read_json(f'{root}/label_num_to_disease_map.json', \n",
    "                         orient='index')\n",
    "display(train.head())\n",
    "display(test.head())\n",
    "display(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7320,  910, 5440, 5241, 5784, 6315,  516, 4476, 5628, 8372, 1735,\n",
       "        819, 6999, 2483, 5361, 5101, 6470, 1234, 4605, 3435, 6446, 8716,\n",
       "       9324, 2608, 7899, 2097, 2797, 9217,  239, 2784, 3055, 4708, 1949,\n",
       "       7784, 1317, 1578, 3606, 3940, 8888, 5443, 8842, 8483, 7563, 2662,\n",
       "       7091, 9605, 6285, 5536, 7149, 9720])"
      ]
     },
     "execution_count": 518,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map.iloc[1].values\n",
    "np.random.randint(50, 10000, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ho-BaziAHY5q"
   },
   "outputs": [],
   "source": [
    "def visualize_input_image_grid(filepaths, image_name, labels, cols=4):\n",
    "    rows = 5\n",
    "    figure, ax = plt.subplots(nrows=rows, ncols=cols, figsize=(25, 15))\n",
    "    for i, index in enumerate(np.random.randint(0, len(image_name), 20)):\n",
    "        name = image_name.iloc[index]['image_id']\n",
    "        label =  image_name.iloc[index]['label']\n",
    "        image = cv2.imread(f'{filepaths}/train_images/{name}')\n",
    "        if (i == 0): \n",
    "            print(image.shape)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        ax.ravel()[i].imshow(image)\n",
    "        ax.ravel()[i].set_title(label, color='GREEN')\n",
    "        ax.ravel()[i].set_axis_off()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the parameters of the whole process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_name = [\"resnest26d\",\"resnest50d\",\"tf_efficientnet_b3_ns\", \"skresnet34\" ,\"cspresnet50\", \"vit_base_patch16_384\"]\n",
    "weights = [\n",
    "    \"weights/resnest26d/resnest26d_fold0_best_epoch_28_final_1st.pth\",\n",
    "    \"weights/resnest26d/resnest26d_fold0_best_epoch_13_final_mixup.pth\",\n",
    "    \"weights/resnest26d/resnest26d_fold2_best_epoch_3_final_hnm.pth\",\n",
    "    \"weights/resnest26d/resnest26d_fold4_best_epoch_29_1st.pth\",\n",
    "    \"weights/resnest26d/resnest26d_fold4_best_epoch_26_mix.pth\",\n",
    "    \"weights/resnest26d/resnest26d_fold4_best_epoch_12_cutmix.pth\",\n",
    "    \"weights/resnest26d/resnest26d_fold4_best_epoch_3_external.pth\",\n",
    "    \"weights/resnest26d/resnest26d_fold4_best_epoch_21_final_512.pth\",\n",
    "    \"weights/tf_efficientnet_b3_ns/tf_efficientnet_b3_ns_fold1_best_epoch_19_external.pth\",\n",
    "    \"weights/tf_efficientnet_b3_ns/tf_efficientnet_b3_ns_fold1_best_epoch_26_512.pth\",\n",
    "    \"weights/tf_efficientnet_b3_ns/tf_efficientnet_b3_ns_fold1_best_epoch_1_final_512.pth\",\n",
    "    \"weights/resnest50d/resnest50d_fold1_best_epoch_95_final_1st.pth\",\n",
    "#     \"weights/resnest50d/resnest50d_fold1_best_epoch_9_final_512.pth\"\n",
    "    \"weights/resnest50d/resnest50d_fold1_best_epoch_38_clean_1st.pth\",\n",
    "    \"weights/resnest50d/resnest50d_fold0_best_epoch_25.pth\",\n",
    "    \"./weights/resnest50d/resnest50d_fold2_best_epoch_50_final_1st.pth\",\n",
    "    \"./weights/resnest50d/resnest50d_fold4_best_epoch_39.pth\",\n",
    "    \"weights/resnest50d/resnest50d_fold1_best_epoch_95_final_1st.pth\"\n",
    "]\n",
    "model_index = 1\n",
    "ckpt_index = -1\n",
    "fold_ckpt_index = [11,12]\n",
    "fold_ckpt_weight = [1,1]\n",
    "\n",
    "params = {\n",
    "    \"visualize\": False,\n",
    "    \"fold\": 1,\n",
    "    \"train_external\": True,\n",
    "    \"train_clean_only\": False,\n",
    "    \"test_external\": False,\n",
    "    \"load_pretrained\": True,\n",
    "    \"resume\": False,\n",
    "    \"image_size\": 512,\n",
    "    \"num_classes\": 5,\n",
    "    \"model\": models_name[model_index],\n",
    "    \"device\": \"cuda\",\n",
    "    \"lr\": 1e-4,\n",
    "    \"lr_min\":1e-6,\n",
    "    \"batch_size\": 8,\n",
    "    \"num_workers\": 8,\n",
    "    \"epochs\": 30,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"drop_block\": 0.2,\n",
    "    \"drop_rate\": 0.2,\n",
    "    \"mix_up\": True,\n",
    "    \"cutmix\":True,\n",
    "    \"fmix\":False,\n",
    "    \"smooth_label\": 0.1,\n",
    "    \"rand_aug\": False,\n",
    "    \"local_rank\":0,\n",
    "    \"distributed\": False,\n",
    "    \"hard_negative_sample\": False,\n",
    "    \"tta\": True,\n",
    "    \"train_phase\":True,\n",
    "    \"balance_data\":False,\n",
    "    \"kfold_pred\":False\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define dataset with KFolds strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let visualize the dataset with the defined class, the proceed the split dataset with 5 folds\n",
    "Then, we can add the external dataset to each fold and visualize the class distribution of the dataset\n",
    "\n",
    "The class dataset is defined with transform and random augmentation.\n",
    "\n",
    "`__init__` will receive an optional `transform` argument. It is a transformation function of the Albumentations augmentation pipeline. Then in `__getitem__`, the Dataset class will use that function to augment an image and return it along with the correct label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "if params['visualize']:\n",
    "    visualize_input_image_grid(root, train, label_map)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold  label\n",
      "0     0         218\n",
      "      1         438\n",
      "      2         477\n",
      "      3        2631\n",
      "      4         516\n",
      "1     0         218\n",
      "      1         438\n",
      "      2         477\n",
      "      3        2631\n",
      "      4         516\n",
      "2     0         217\n",
      "      1         438\n",
      "      2         477\n",
      "      3        2632\n",
      "      4         515\n",
      "3     0         217\n",
      "      1         438\n",
      "      2         477\n",
      "      3        2632\n",
      "      4         515\n",
      "4     0         217\n",
      "      1         437\n",
      "      2         478\n",
      "      3        2632\n",
      "      4         515\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "folds = train.copy()\n",
    "Fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "for n, (train_index, val_index) in enumerate(Fold.split(folds, folds['label'])):\n",
    "    folds.loc[val_index, 'fold'] = int(n)\n",
    "folds['fold'] = folds['fold'].astype(int)\n",
    "print(folds.groupby(['fold','label']).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>label</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000015157.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000201771.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100042118.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000812911.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000837476.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17112</th>\n",
       "      <td>997910101.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17113</th>\n",
       "      <td>997973414.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17114</th>\n",
       "      <td>999329392.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17115</th>\n",
       "      <td>999616605.jpg</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17116</th>\n",
       "      <td>999998473.jpg</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17117 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             image_id  label  fold\n",
       "0      1000015157.jpg      0     3\n",
       "1      1000201771.jpg      3     2\n",
       "2       100042118.jpg      1     2\n",
       "3      1000812911.jpg      3     2\n",
       "4      1000837476.jpg      3     2\n",
       "...               ...    ...   ...\n",
       "17112   997910101.jpg      2     3\n",
       "17113   997973414.jpg      1     0\n",
       "17114   999329392.jpg      3     0\n",
       "17115   999616605.jpg      4     2\n",
       "17116   999998473.jpg      4     4\n",
       "\n",
       "[17117 rows x 3 columns]"
      ]
     },
     "execution_count": 523,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold = params[\"fold\"]\n",
    "train_idx = folds[folds['fold'] != fold].index\n",
    "val_idx = folds[folds['fold'] == fold].index\n",
    "\n",
    "train_folds = folds.loc[train_idx].reset_index(drop=True)\n",
    "val_folds = folds.loc[val_idx].reset_index(drop=True)\n",
    "train_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>label</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train-cbsd-663.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train-cgm-560.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train-cmd-1576.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train-cbb-443.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train-cgm-293.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5651</th>\n",
       "      <td>train-cmd-1383.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5652</th>\n",
       "      <td>train-cmd-971.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5653</th>\n",
       "      <td>train-cbb-402.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5654</th>\n",
       "      <td>train-cbsd-1029.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5655</th>\n",
       "      <td>train-cmd-2291.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5656 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 image_id  label  fold\n",
       "0      train-cbsd-663.jpg      1     1\n",
       "1       train-cgm-560.jpg      2     1\n",
       "2      train-cmd-1576.jpg      3     1\n",
       "3       train-cbb-443.jpg      0     1\n",
       "4       train-cgm-293.jpg      2     1\n",
       "...                   ...    ...   ...\n",
       "5651   train-cmd-1383.jpg      3     1\n",
       "5652    train-cmd-971.jpg      3     1\n",
       "5653    train-cbb-402.jpg      0     1\n",
       "5654  train-cbsd-1029.jpg      1     1\n",
       "5655   train-cmd-2291.jpg      3     1\n",
       "\n",
       "[5656 rows x 3 columns]"
      ]
     },
     "execution_count": 524,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_external\n",
    "for idx,label in enumerate(train_external['label']):\n",
    "    train_external.loc[idx,'fold'] = fold\n",
    "train_external['fold'] = train_external['fold'].astype(int)\n",
    "train_external"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_data(df1, df2):\n",
    "    merge_df = pd.concat([df1, df2], axis=0) #,how ='outer', on ='image_id')\n",
    "    return merge_df\n",
    "\n",
    "if params[\"train_external\"]:\n",
    "    train_folds = merge_data(train_folds, train_external)\n",
    "if params[\"train_clean_only\"]:\n",
    "    train_folds = train_external\n",
    "#     train_folds = pd.read_csv(f'{root}/train_{params[\"fold\"]}_pseudo.csv')\n",
    "#     val_folds = pd.read_csv(f'{root}/val_{params[\"fold\"]}_pseudo.csv')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>label</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000015157.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000201771.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100042118.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000812911.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000837476.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5651</th>\n",
       "      <td>train-cmd-1383.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5652</th>\n",
       "      <td>train-cmd-971.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5653</th>\n",
       "      <td>train-cbb-402.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5654</th>\n",
       "      <td>train-cbsd-1029.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5655</th>\n",
       "      <td>train-cmd-2291.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22773 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 image_id  label  fold\n",
       "0          1000015157.jpg      0     3\n",
       "1          1000201771.jpg      3     2\n",
       "2           100042118.jpg      1     2\n",
       "3          1000812911.jpg      3     2\n",
       "4          1000837476.jpg      3     2\n",
       "...                   ...    ...   ...\n",
       "5651   train-cmd-1383.jpg      3     1\n",
       "5652    train-cmd-971.jpg      3     1\n",
       "5653    train-cbb-402.jpg      0     1\n",
       "5654  train-cbsd-1029.jpg      1     1\n",
       "5655   train-cmd-2291.jpg      3     1\n",
       "\n",
       "[22773 rows x 3 columns]"
      ]
     },
     "execution_count": 526,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0     1087\n",
      "1     2189\n",
      "2     2386\n",
      "3    13158\n",
      "4     2577\n",
      "Name: image_id, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0     466\n",
      "1    1443\n",
      "2     773\n",
      "3    2658\n",
      "4     316\n",
      "Name: image_id, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0     1335\n",
      "1     3194\n",
      "2     2682\n",
      "3    13185\n",
      "4     2377\n",
      "Name: image_id, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def visualize_class_dis(df, fold, color='maroon'):\n",
    "    x = df.groupby('label').count()\n",
    "    print(x['image_id'])\n",
    "    y = [0,1,2,3,4]\n",
    "\n",
    "    fig = plt.figure(figsize=(10,5))\n",
    "    plt.bar(y, x['image_id'].values, color =color, width = 0.4)\n",
    "    plt.xlabel(\"Label\") \n",
    "    plt.ylabel(\"Number of image\") \n",
    "    plt.title(f\"Class distribution of data fold: {fold}\") \n",
    "\n",
    "    # plt.xticks(x['image_id'])\n",
    "    plt.show()\n",
    "visualize_class_dis(train, 'all data', color='blue')\n",
    "visualize_class_dis(train_external, 'external data', color='purple')\n",
    "visualize_class_dis(train_folds, params['fold'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "if params[\"test_external\"]:\n",
    "    train_folds = merge_data(train_folds, test_external_pseudo)\n",
    "    visualize_class_dis(train_folds, f'fold {fold} add extra pseudo test external data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_data(df, mode=\"undersampling\", val=False):\n",
    "    class_0 = df[df.label==0]\n",
    "    class_1 = df[df.label==1]\n",
    "    class_2 = df[df.label==2]\n",
    "    class_3 = df[df.label==3]\n",
    "    class_4 = df[df.label==4]\n",
    "    if mode == \"undersampling\":\n",
    "        # upsample minority\n",
    "        class_3_downsampled = resample(class_3,\n",
    "                                  replace=True, # sample with replacement\n",
    "                                  n_samples=int(len(class_3)*2/3), # match number in majority class\n",
    "                                  random_state=27) # reproducible results\n",
    "        if val:\n",
    "            return  pd.concat([class_0, class_1, class_2, class_3_downsampled, class_4]) \n",
    "    \n",
    "        class_1_downsampled = resample(class_1,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=int(len(class_1)*0.7), # match number in majority class\n",
    "                          random_state=27) # reproducible results\n",
    "        class_4_upsampled = resample(class_4,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=int(len(class_4)*1.3), # match number in majority class\n",
    "                          random_state=27) # reproducible results\n",
    "        return pd.concat([class_0, class_1_downsampled, class_2, class_3_downsampled, class_4_upsampled]) \n",
    "    else:\n",
    "        class_0_upsampled = resample(class_0,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=int(len(class_3)/4), # match number in majority class\n",
    "                          random_state=27) # reproducible results\n",
    "        class_1_upsampled = resample(class_1,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=int(len(class_3)/3), # match number in majority class\n",
    "                          random_state=27) # reproducible results\n",
    "        class_2_upsampled = resample(class_2,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=int(len(class_3)/3), # match number in majority class\n",
    "                          random_state=27) # reproducible results\n",
    "        class_4_upsampled = resample(class_4,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=int(len(class_3)/3), # match number in majority class\n",
    "                          random_state=27) # reproducible results\n",
    "        return pd.concat([class_0_upsampled, class_1_upsampled, class_2_upsampled, class_3, class_4_upsampled]) \n",
    "\n",
    "    \n",
    "if params[\"balance_data\"]:\n",
    "    train_folds = balance_data(train_folds, mode=\"undersampling\")    \n",
    "    val_folds = balance_data(val_folds, mode=\"undersampling\", val=True)    \n",
    "    \n",
    "#     train_folds = balance_data(train_folds, mode=\"oversamling\")\n",
    "    visualize_class_dis(train_folds, f'fold {fold} after balance data')\n",
    "    visualize_class_dis(val_folds, f'fold {fold} after balance data')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9EtOPkNwh5-8"
   },
   "outputs": [],
   "source": [
    "# Dataset\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, df, transform=None, mosaic_mix = False):\n",
    "        self.df = df\n",
    "        self.file_names = df['image_id'].values\n",
    "        self.labels = df['label'].values\n",
    "        self.transform = transform\n",
    "        self.mosaic_mix = mosaic_mix\n",
    "        self.rand_aug_fn = RandAugment()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_name = self.file_names[idx]\n",
    "        file_path = f'{root}/train_images/{file_name}'\n",
    "        image = cv2.imread(file_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        label = torch.tensor(self.labels[idx]).long()\n",
    "        if params[\"rand_aug\"]:\n",
    "            image = np.array(self.rand_aug_fn(Image.fromarray(image)))\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "\n",
    "        return image, label, file_name\n",
    "    \n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, df, transform=None, valid_test=False, fcrops=False):\n",
    "        self.df = df\n",
    "        self.file_names = df['image_id'].values\n",
    "        self.transform = transform\n",
    "        self.valid_test = valid_test\n",
    "        self.fcrops = fcrops\n",
    "        if self.valid_test:\n",
    "            self.labels = df['label'].values  \n",
    "        else:\n",
    "            assert ValueError(\"Test data does not have annotation, plz check!\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_name = self.file_names[idx]\n",
    "        if self.valid_test:\n",
    "            file_path = f'{root}/train_images/{file_name}'\n",
    "            #file_path = f'{root}/external/extraimages/{file_name}'\n",
    "        else:\n",
    "            file_path = f'{root}/test_images/{file_name}'\n",
    "        image = cv2.imread(file_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        if isinstance(self.transform, list):\n",
    "            outputs = {'images':[],\n",
    "                       'labels':[],\n",
    "                       'image_ids':[]}\n",
    "            if self.fcrops:\n",
    "                for trans in self.transform:\n",
    "                    image_aug = transforms.ToPILImage()(image)\n",
    "                    image_aug = trans(image_aug)\n",
    "                    outputs[\"images\"].append(image_aug)\n",
    "                    del image_aug\n",
    "            else:\n",
    "                for trans in self.transform:\n",
    "                    augmented = trans(image=image)\n",
    "                    image_aug = augmented['image']\n",
    "                    outputs[\"images\"].append(image_aug)\n",
    "                    del image_aug\n",
    "\n",
    "            if self.valid_test:\n",
    "                label = torch.tensor(self.labels[idx]).long()\n",
    "                outputs['labels'] = len(self.transform)*[label]\n",
    "                outputs['image_ids'].append(file_name)\n",
    "                \n",
    "            else:\n",
    "                outputs['labels'] = len(self.transform)*[-1]\n",
    "                \n",
    "            return outputs\n",
    "        else:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image'] \n",
    "        return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Albumentations to define transformation functions for the train and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mm_NP-c4h5-_"
   },
   "outputs": [],
   "source": [
    "train_transform = A.Compose(\n",
    "    [\n",
    "        A.RandomResizedCrop(height=params[\"image_size\"], width=params[\"image_size\"], p=1),\n",
    "        A.OneOf([\n",
    "            A.RandomRotate90(p=0.5),\n",
    "            A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),], p=1.\n",
    "        ),\n",
    "#         A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.IAAAffine(rotate=0.2, shear=0.2,p=0.5),\n",
    "        A.CoarseDropout(max_holes=20, max_height=int(params[\"image_size\"]/15), max_width=int(params[\"image_size\"]/15), p=0.5),\n",
    "#         A.IAAAdditiveGaussianNoise(p=1.),\n",
    "        A.MedianBlur(p=0.5),\n",
    "        A.Equalize(p=0.2),\n",
    "        A.GridDistortion(p=0.2),\n",
    "#         A.RandomGridShuffle(grid=(100, 100), p=0.5),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "val_transform = A.Compose(\n",
    "    [\n",
    "        A.CenterCrop(height=params[\"image_size\"], width=params[\"image_size\"], p=1),\n",
    "        A.Resize(params[\"image_size\"],params[\"image_size\"]),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "if params[\"cutmix\"]:\n",
    "    mixup_fn = Mixup(mixup_alpha=1., cutmix_alpha=1., label_smoothing=params[\"smooth_label\"], num_classes=params[\"num_classes\"])\n",
    "else:\n",
    "    mixup_fn = Mixup(mixup_alpha=1., label_smoothing=params[\"smooth_label\"], num_classes=params[\"num_classes\"])\n",
    "\n",
    "train_dataset = TrainDataset(train_folds, transform=train_transform)\n",
    "val_dataset = TrainDataset(val_folds, transform=val_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also let's define a function that takes a dataset and visualizes different augmentations applied to the same image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_augmentations(dataset, idx=3, samples=10, cols=5):\n",
    "    dataset = copy.deepcopy(dataset)\n",
    "    dataset.transform = A.Compose([t for t in dataset.transform if not isinstance(t, (A.Normalize, ToTensorV2))])\n",
    "    rows = samples // cols\n",
    "    figure, ax = plt.subplots(nrows=rows, ncols=cols, figsize=(12, 6))\n",
    "    for i in range(samples):\n",
    "        image, _, _ = dataset[idx]\n",
    "        ax.ravel()[i].imshow(image)\n",
    "        ax.ravel()[i].set_axis_off()\n",
    "    plt.tight_layout()\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "if params[\"visualize\"]:\n",
    "    random.seed(SEED)\n",
    "    visualize_augmentations(train_dataset, idx=random.randint(1,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnNormalize(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
    "        Returns:\n",
    "            Tensor: Normalized image.\n",
    "        \"\"\"\n",
    "        for t, m, s in zip(tensor, self.mean, self.std):\n",
    "            t.mul_(s).add_(m)\n",
    "            # The normalize code -> t.sub_(m).div_(s)\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Mixup CutMix Aug\n",
    "def visualize_mix_augmentations(dataset, idx=[4,5,1000,3], cols=4):\n",
    "    dataset = copy.deepcopy(dataset)\n",
    "    rows = 1\n",
    "    figure, ax = plt.subplots(nrows=rows, ncols=cols, figsize=(20, 15))\n",
    "    images = []\n",
    "    labels = []\n",
    "    for i in idx:\n",
    "        image, label, _ = dataset[i]\n",
    "        images.append(image)\n",
    "        labels.append(label)\n",
    "        \n",
    "    images, labels = mixup_fn(torch.stack(images).cuda(), torch.stack(labels).float().cuda())\n",
    "    for i, (image,label) in enumerate(zip(images, labels)):\n",
    "        unorm = UnNormalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "        image = (unorm(image).cpu().numpy()*255).astype(int)\n",
    "        ax.ravel()[i].imshow(image.transpose(1,2,0))\n",
    "        ax.ravel()[i].set_title(label, color='GREEN')\n",
    "        ax.ravel()[i].set_axis_off()\n",
    "    plt.tight_layout()\n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "if params[\"visualize\"] and params[\"mix_up\"]:\n",
    "    random.seed(SEED)\n",
    "    visualize_mix_augmentations(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define helpers for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a few helpers for our training pipeline. `calculate_accuracy` takes model predictions and true labels and will return accuracy for those predictions. `MetricMonitor` helps to track metrics such as accuracy or loss during training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bgTqE0eYSjDh"
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(output, target):\n",
    "#     return torch.true_divide((target == output).sum(dim=0), output.size(0)).item()\n",
    "    if params[\"mix_up\"]:\n",
    "        output = torch.argmax(torch.softmax(output, dim=1), dim=1)\n",
    "        return accuracy_score(output.cpu(), target.argmax(1).cpu())\n",
    "    \n",
    "    output = torch.softmax(output, dim=1)\n",
    "    return accuracy_score(output.argmax(1).cpu(), target.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zRhPJiCch5_D"
   },
   "outputs": [],
   "source": [
    "class MetricMonitor:\n",
    "    def __init__(self, float_precision=3):\n",
    "        self.float_precision = float_precision\n",
    "        self.reset()\n",
    "        self.curr_acc = 0.\n",
    "    def reset(self):\n",
    "        self.metrics = defaultdict(lambda: {\"val\": 0, \"count\": 0, \"avg\": 0})\n",
    "\n",
    "    def update(self, metric_name, val):\n",
    "        metric = self.metrics[metric_name]\n",
    "\n",
    "        metric[\"val\"] += val\n",
    "        metric[\"count\"] += 1\n",
    "        metric[\"avg\"] = metric[\"val\"] / metric[\"count\"]\n",
    "        self.curr_acc = metric[\"avg\"]\n",
    "    def __str__(self):\n",
    "        return \" | \".join(\n",
    "            [\n",
    "                \"{metric_name}: {avg:.{float_precision}f}\".format(\n",
    "                    metric_name=metric_name, avg=metric[\"avg\"], float_precision=self.float_precision\n",
    "                )\n",
    "                for (metric_name, metric) in self.metrics.items()\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "if params[\"hard_negative_sample\"]:\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=1, shuffle=True, num_workers=params[\"num_workers\"], pin_memory=True,\n",
    "    )\n",
    "else:\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=params[\"batch_size\"], shuffle=True, num_workers=params[\"num_workers\"], pin_memory=True,\n",
    "    )\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=params[\"batch_size\"], shuffle=False, num_workers=params[\"num_workers\"], pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_hard_sample(train_loader, model, val_criterion, thres):\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(12, 6))\n",
    "    train_loss_list = {'image_id':[],\n",
    "                       'label':[],\n",
    "                       'loss':[],\n",
    "                       'fold':[]}\n",
    "    model.eval()\n",
    "    stream = tqdm(train_loader)\n",
    "    with torch.no_grad():\n",
    "        for i, (images, target, name) in enumerate(stream, start=1):\n",
    "            images = images.to(params[\"device\"], non_blocking=True)\n",
    "            target = target.to(params[\"device\"], non_blocking=True)#.view(-1,params['batch_size'])\n",
    "            output = model(images)\n",
    "            loss = val_criterion(output, target)\n",
    "            if loss > thres:\n",
    "                train_loss_list['image_id'].append(name[0])\n",
    "                train_loss_list['label'].append(int(target[0].cpu().numpy()))\n",
    "                train_loss_list['loss'].append(loss)\n",
    "                train_loss_list['fold'].append(params['fold'])\n",
    "    print(\"Number hard samples:\",len(train_loss_list[\"loss\"]))\n",
    "    #visualize\n",
    "    ax.ravel()[0].plot(train_loss_list['loss'])\n",
    "    ax.ravel()[0].set_title(\"Loss\", color='BLUE')\n",
    "    #ax.ravel()[0].set_axis_off() \n",
    "    \n",
    "    ax.ravel()[1].plot(sorted(train_loss_list['loss']))\n",
    "    ax.ravel()[1].set_title(\"Sort Curve\", color='BLUE')\n",
    "    #ax.ravel()[1].set_axis_off() \n",
    "    \n",
    "    prob = 10 / (abs(sorted(train_loss_list['loss']) - sorted(train_loss_list['loss']).mean()) + 10)\n",
    "    ax.ravel()[2].plot(prob)\n",
    "    ax.ravel()[2].set_title(\"Sort reshape Curve\", color='GREEN')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()  \n",
    "        \n",
    "    return dict(image_id=train_loss_list['image_id'],\n",
    "                label=train_loss_list['label'],\n",
    "                fold=train_loss_list['fold'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define training parameters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a few training parameters such as model architecture, learning rate, batch size, epochs, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"efficientnet\" in params[\"model\"]:\n",
    "    model = timm.create_model(\n",
    "            params[\"model\"],\n",
    "            pretrained=False,\n",
    "            num_classes=params[\"num_classes\"], \n",
    "            drop_rate=params[\"drop_rate\"], \n",
    "            drop_path_rate=0.3)\n",
    "    \n",
    "elif \"skresnet\" in params[\"model\"]:\n",
    "    model = timm.create_model(\n",
    "            params[\"model\"],\n",
    "            pretrained=True,\n",
    "            num_classes=params[\"num_classes\"],\n",
    "            drop_block_rate=params[\"drop_block\"],\n",
    "            drop_path_rate=0.2)\n",
    "else:\n",
    "    model = timm.create_model(\n",
    "            params[\"model\"],\n",
    "            pretrained=True,\n",
    "            num_classes=params[\"num_classes\"],\n",
    "            drop_block_rate=params[\"drop_block\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_features = model.fc.in_features\n",
    "# model.fc = nn.Linear(n_features, params['num_classes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create all required objects and functions for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W7HipbJ8h5_I"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pretrained model:  0.8967\n"
     ]
    }
   ],
   "source": [
    "# model = getattr(models, params[\"model\"])(pretrained=False, num_classes=5)\n",
    "model = model.to(params[\"device\"])\n",
    "val_criterion = nn.CrossEntropyLoss().to(params[\"device\"])\n",
    "# criterion = nn.CrossEntropyLoss().to(params[\"device\"])\n",
    "criterion = LabelSmoothingCrossEntropy().to(params[\"device\"])\n",
    "if params[\"mix_up\"]:\n",
    "    criterion = SoftTargetCrossEntropy().to(params[\"device\"])\n",
    "asymetric_criterion = AsymmetricLossSingleLabel().to(params[\"device\"])\n",
    "symetric_criterion = SCELoss(smooth_label=params[\"smooth_label\"]).to(params[\"device\"])\n",
    "\n",
    "if params[\"distributed\"]:\n",
    "    assert ValueError(\"No need to implement in a single machine\")\n",
    "else:\n",
    "    model = torch.nn.DataParallel(model)    \n",
    "if params[\"load_pretrained\"]:\n",
    "    state_dict = torch.load(weights[ckpt_index])\n",
    "    print(\"Load pretrained model: \",state_dict[\"preds\"])\n",
    "    model.load_state_dict(state_dict[\"model\"])\n",
    "    best_acc = state_dict[\"preds\"]\n",
    "    # Hard negative mining based on train data and pretrained model on that data\n",
    "    if params[\"hard_negative_sample\"]:\n",
    "        update_train_data = update_hard_sample(train_loader, model, val_criterion, thres=0.2)\n",
    "        update_train_folds = pd.DataFrame(data=update_train_data)\n",
    "        update_train_folds = pd.concat(5*[update_train_folds])\n",
    "\n",
    "        \n",
    "        #check the update distribution when filter data\n",
    "        print(\"Class distribution for the new data\")\n",
    "        visualize_class_dis(update_train_folds, params[\"fold\"])\n",
    "        \n",
    "        #update the training set\n",
    "        update_train_dataset = TrainDataset(update_train_folds, transform=train_transform)\n",
    "        update_train_loader = DataLoader(\n",
    "            update_train_dataset, batch_size=params[\"batch_size\"], shuffle=True, num_workers=params[\"num_workers\"], pin_memory=True,\n",
    "        )                \n",
    "else:\n",
    "    best_acc = 0.\n",
    "    \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=params[\"lr\"])\n",
    "# scheduler = CosineAnnealingLR(optimizer, T_max=10, eta_min=params[\"lr_min\"], last_epoch=-1)\n",
    "scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=1, eta_min=params[\"lr_min\"], last_epoch=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HNz-5F7Bh5_Y"
   },
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch, params):\n",
    "    metric_monitor = MetricMonitor()\n",
    "    model.train()\n",
    "    if params[\"hard_negative_sample\"]:\n",
    "        stream = tqdm(update_train_loader)\n",
    "    else:\n",
    "        stream = tqdm(train_loader)\n",
    "    for i, (images, target, _) in enumerate(stream, start=1):\n",
    "        images = images.to(params[\"device\"]) #, non_blocking=True)\n",
    "        target = target.to(params[\"device\"]) #, non_blocking=True) #.view(-1,params['batch_size'])\n",
    "        if params[\"mix_up\"]:\n",
    "            images , target = mixup_fn(images, target)\n",
    "        output = model(images)\n",
    "        if isinstance(output, (tuple, list)):\n",
    "            output = output[0]\n",
    "        loss0 = criterion(output, target)\n",
    "#         loss1 = symetric_criterion(output, target)\n",
    "#         loss2 = asymetric_criterion(output, target)\n",
    "        loss = loss0 #+ loss2\n",
    "        if params['gradient_accumulation_steps'] > 1:\n",
    "            loss = loss / params['gradient_accumulation_steps']\n",
    "    \n",
    "        accuracy = calculate_accuracy(output, target)\n",
    "        metric_monitor.update(\"Loss\", loss.item())\n",
    "        metric_monitor.update(\"Accuracy\", accuracy)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "    \n",
    "        optimizer.step()\n",
    "        stream.set_description(\n",
    "            \"Epoch: {epoch}. Train.      {metric_monitor}\".format(epoch=epoch, metric_monitor=metric_monitor)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "chKMqryvh5_a"
   },
   "outputs": [],
   "source": [
    "def validate(val_loader, model, criterion, epoch, params, fold, best_acc):\n",
    "    metric_monitor = MetricMonitor()\n",
    "    model.eval()\n",
    "    stream = tqdm(val_loader)\n",
    "    with torch.no_grad():\n",
    "        for i, (images, target, _) in enumerate(stream, start=1):\n",
    "            images = images.to(params[\"device\"], non_blocking=True)\n",
    "            target = target.to(params[\"device\"], non_blocking=True)#.view(-1,params['batch_size'])\n",
    "            output = model(images)\n",
    "            loss = val_criterion(output, target)\n",
    "            output = torch.softmax(output, dim = 1)\n",
    "            \n",
    "            accuracy = accuracy_score(output.argmax(1).cpu(), target.cpu())\n",
    "\n",
    "            stream.set_description(\n",
    "                \"Epoch: {epoch}. Validation. {metric_monitor}\".format(epoch=epoch, metric_monitor=metric_monitor)\n",
    "            )           \n",
    "            metric_monitor.update(\"Loss\", loss.item())\n",
    "            metric_monitor.update(\"Accuracy\", accuracy)\n",
    "        #to save weight\n",
    "        if (metric_monitor.curr_acc > best_acc): # or epoch == params[\"epochs\"]:\n",
    "            print(f\"Save best weight at acc {round(metric_monitor.curr_acc,4)}, epoch: {epoch}\")\n",
    "#             if not os.path.exists(f\"weights/{params[\"model\"]}\"):\n",
    "#                 os.makedirs(f\"weights/{params[\"model\"]}\")\n",
    "            torch.save({'model': model.state_dict(), \n",
    "                'loss': loss,\n",
    "                'preds': round(metric_monitor.curr_acc,4)},\n",
    "                 f'weights/{params[\"model\"]}/{params[\"model\"]}_fold{fold}_best_epoch_{epoch}.pth')\n",
    "\n",
    "            best_acc = metric_monitor.curr_acc\n",
    "    return best_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "IiA5Zgfch5_c",
    "outputId": "f4fba281-0997-4202-e9d2-6aa3f93373f0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2847 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on fold: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1. Train.      Loss: 0.916 | Accuracy: 0.808: 100%|██████████| 2847/2847 [12:03<00:00,  3.94it/s]\n",
      "Epoch: 1. Validation. Loss: 0.509 | Accuracy: 0.886: 100%|██████████| 535/535 [00:36<00:00, 14.73it/s]\n",
      "  0%|          | 0/2847 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save best weight at acc 0.8864, epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 2. Train.      Loss: 0.910 | Accuracy: 0.810: 100%|██████████| 2847/2847 [12:03<00:00,  3.93it/s]\n",
      "Epoch: 2. Validation. Loss: 0.478 | Accuracy: 0.893: 100%|██████████| 535/535 [00:36<00:00, 14.73it/s]\n",
      "  0%|          | 0/2847 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save best weight at acc 0.893, epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 3. Train.      Loss: 0.908 | Accuracy: 0.810: 100%|██████████| 2847/2847 [12:03<00:00,  3.94it/s]\n",
      "Epoch: 3. Validation. Loss: 0.503 | Accuracy: 0.887: 100%|██████████| 535/535 [00:36<00:00, 14.72it/s]\n",
      "Epoch: 4. Train.      Loss: 0.912 | Accuracy: 0.810: 100%|██████████| 2847/2847 [12:03<00:00,  3.94it/s]\n",
      "Epoch: 4. Validation. Loss: 0.470 | Accuracy: 0.892: 100%|██████████| 535/535 [00:36<00:00, 14.71it/s]\n",
      "Epoch: 5. Train.      Loss: 0.910 | Accuracy: 0.811: 100%|██████████| 2847/2847 [12:03<00:00,  3.94it/s]\n",
      "Epoch: 5. Validation. Loss: 0.510 | Accuracy: 0.892: 100%|██████████| 535/535 [00:36<00:00, 14.73it/s]\n",
      "Epoch: 6. Train.      Loss: 0.906 | Accuracy: 0.815: 100%|██████████| 2847/2847 [12:03<00:00,  3.94it/s]\n",
      "Epoch: 6. Validation. Loss: 0.517 | Accuracy: 0.869: 100%|██████████| 535/535 [00:36<00:00, 14.73it/s]\n",
      "Epoch: 7. Train.      Loss: 0.902 | Accuracy: 0.816: 100%|██████████| 2847/2847 [12:03<00:00,  3.94it/s]\n",
      "Epoch: 7. Validation. Loss: 0.456 | Accuracy: 0.896: 100%|██████████| 535/535 [00:36<00:00, 14.74it/s]\n",
      "  0%|          | 0/2847 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save best weight at acc 0.8958, epoch: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 8. Train.      Loss: 0.908 | Accuracy: 0.814: 100%|██████████| 2847/2847 [12:03<00:00,  3.94it/s]\n",
      "Epoch: 8. Validation. Loss: 0.481 | Accuracy: 0.892: 100%|██████████| 535/535 [00:36<00:00, 14.74it/s]\n",
      "Epoch: 9. Train.      Loss: 0.903 | Accuracy: 0.817: 100%|██████████| 2847/2847 [12:03<00:00,  3.93it/s]\n",
      "Epoch: 9. Validation. Loss: 0.525 | Accuracy: 0.885: 100%|██████████| 535/535 [00:36<00:00, 14.74it/s]\n",
      "Epoch: 10. Train.      Loss: 0.904 | Accuracy: 0.815: 100%|██████████| 2847/2847 [12:03<00:00,  3.94it/s]\n",
      "Epoch: 10. Validation. Loss: 0.487 | Accuracy: 0.886: 100%|██████████| 535/535 [00:36<00:00, 14.74it/s]\n",
      "Epoch: 11. Train.      Loss: 0.899 | Accuracy: 0.816: 100%|██████████| 2847/2847 [12:03<00:00,  3.94it/s]\n",
      "Epoch: 11. Validation. Loss: 0.488 | Accuracy: 0.889: 100%|██████████| 535/535 [00:36<00:00, 14.72it/s]\n",
      "Epoch: 12. Train.      Loss: 0.903 | Accuracy: 0.814: 100%|██████████| 2847/2847 [12:03<00:00,  3.94it/s]\n",
      "Epoch: 12. Validation. Loss: 0.491 | Accuracy: 0.886: 100%|██████████| 535/535 [00:36<00:00, 14.72it/s]\n",
      "Epoch: 13. Train.      Loss: 0.893 | Accuracy: 0.821: 100%|██████████| 2847/2847 [12:03<00:00,  3.94it/s]\n",
      "Epoch: 13. Validation. Loss: 0.466 | Accuracy: 0.890: 100%|██████████| 535/535 [00:36<00:00, 14.74it/s]\n",
      "Epoch: 14. Train.      Loss: 0.899 | Accuracy: 0.821: 100%|██████████| 2847/2847 [12:03<00:00,  3.94it/s]\n",
      "Epoch: 14. Validation. Loss: 0.469 | Accuracy: 0.894: 100%|██████████| 535/535 [00:36<00:00, 14.74it/s]\n",
      "Epoch: 15. Train.      Loss: 0.896 | Accuracy: 0.818: 100%|██████████| 2847/2847 [12:03<00:00,  3.94it/s]\n",
      "Epoch: 15. Validation. Loss: 0.458 | Accuracy: 0.887: 100%|██████████| 535/535 [00:36<00:00, 14.72it/s]\n",
      "Epoch: 16. Train.      Loss: 0.893 | Accuracy: 0.818: 100%|██████████| 2847/2847 [12:03<00:00,  3.94it/s]\n",
      "Epoch: 16. Validation. Loss: 0.453 | Accuracy: 0.894: 100%|██████████| 535/535 [00:36<00:00, 14.71it/s]\n",
      "Epoch: 17. Train.      Loss: 0.894 | Accuracy: 0.819: 100%|██████████| 2847/2847 [12:03<00:00,  3.93it/s]\n",
      "Epoch: 17. Validation. Loss: 0.466 | Accuracy: 0.899: 100%|██████████| 535/535 [00:36<00:00, 14.74it/s]\n",
      "  0%|          | 0/2847 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save best weight at acc 0.8993, epoch: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 18. Train.      Loss: 0.896 | Accuracy: 0.819: 100%|██████████| 2847/2847 [12:03<00:00,  3.94it/s]\n",
      "Epoch: 18. Validation. Loss: 0.510 | Accuracy: 0.890: 100%|██████████| 535/535 [00:36<00:00, 14.71it/s]\n",
      "Epoch: 19. Train.      Loss: 0.892 | Accuracy: 0.820: 100%|██████████| 2847/2847 [12:03<00:00,  3.94it/s]\n",
      "Epoch: 19. Validation. Loss: 0.464 | Accuracy: 0.899: 100%|██████████| 535/535 [00:36<00:00, 14.72it/s]\n",
      "Epoch: 20. Train.      Loss: 0.898 | Accuracy: 0.819: 100%|██████████| 2847/2847 [12:03<00:00,  3.93it/s]\n",
      "Epoch: 20. Validation. Loss: 0.452 | Accuracy: 0.890: 100%|██████████| 535/535 [00:36<00:00, 14.72it/s]\n",
      "Epoch: 21. Train.      Loss: 0.897 | Accuracy: 0.818: 100%|██████████| 2847/2847 [12:03<00:00,  3.93it/s]\n",
      "Epoch: 21. Validation. Loss: 0.449 | Accuracy: 0.890: 100%|██████████| 535/535 [00:36<00:00, 14.72it/s]\n",
      "Epoch: 22. Train.      Loss: 0.894 | Accuracy: 0.817: 100%|██████████| 2847/2847 [12:03<00:00,  3.94it/s]\n",
      "Epoch: 22. Validation. Loss: 0.456 | Accuracy: 0.889: 100%|██████████| 535/535 [00:36<00:00, 14.72it/s]\n",
      "Epoch: 23. Train.      Loss: 0.891 | Accuracy: 0.820: 100%|██████████| 2847/2847 [12:00<00:00,  3.95it/s]\n",
      "Epoch: 23. Validation. Loss: 0.484 | Accuracy: 0.893: 100%|██████████| 535/535 [00:36<00:00, 14.80it/s]\n",
      "Epoch: 24. Train.      Loss: 0.885 | Accuracy: 0.822: 100%|██████████| 2847/2847 [12:00<00:00,  3.95it/s]\n",
      "Epoch: 24. Validation. Loss: 0.521 | Accuracy: 0.888: 100%|██████████| 535/535 [00:36<00:00, 14.79it/s]\n",
      "Epoch: 25. Train.      Loss: 0.890 | Accuracy: 0.822: 100%|██████████| 2847/2847 [12:01<00:00,  3.94it/s]\n",
      "Epoch: 25. Validation. Loss: 0.488 | Accuracy: 0.882: 100%|██████████| 535/535 [00:36<00:00, 14.72it/s]\n",
      "Epoch: 26. Train.      Loss: 0.895 | Accuracy: 0.819: 100%|██████████| 2847/2847 [12:02<00:00,  3.94it/s]\n",
      "Epoch: 26. Validation. Loss: 0.505 | Accuracy: 0.887: 100%|██████████| 535/535 [00:36<00:00, 14.73it/s]\n",
      "Epoch: 27. Train.      Loss: 0.893 | Accuracy: 0.824: 100%|██████████| 2847/2847 [12:03<00:00,  3.94it/s]\n",
      "Epoch: 27. Validation. Loss: 0.452 | Accuracy: 0.894: 100%|██████████| 535/535 [00:36<00:00, 14.75it/s]\n",
      "Epoch: 28. Train.      Loss: 0.891 | Accuracy: 0.819: 100%|██████████| 2847/2847 [12:02<00:00,  3.94it/s]\n",
      "Epoch: 28. Validation. Loss: 0.519 | Accuracy: 0.854: 100%|██████████| 535/535 [00:36<00:00, 14.75it/s]\n",
      "Epoch: 29. Train.      Loss: 0.888 | Accuracy: 0.821: 100%|██████████| 2847/2847 [12:02<00:00,  3.94it/s]\n",
      "Epoch: 29. Validation. Loss: 0.503 | Accuracy: 0.889: 100%|██████████| 535/535 [00:36<00:00, 14.74it/s]\n",
      "Epoch: 30. Train.      Loss: 0.890 | Accuracy: 0.821: 100%|██████████| 2847/2847 [12:02<00:00,  3.94it/s]\n",
      "Epoch: 30. Validation. Loss: 0.497 | Accuracy: 0.892: 100%|██████████| 535/535 [00:36<00:00, 14.73it/s]\n"
     ]
    }
   ],
   "source": [
    "best_acc = 0\n",
    "print(f\"Train on fold: {fold}\")\n",
    "if params[\"train_phase\"]:\n",
    "    for epoch in range(1, params[\"epochs\"] + 1):\n",
    "        train(train_loader, model, criterion, optimizer, epoch, params)\n",
    "        best_acc = validate(val_loader, model, criterion, epoch, params, fold, best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save({'model': model.state_dict(), \n",
    "#     'preds': 0.8},\n",
    "#      f'weights/{params[\"model\"]}_fold{fold}_best_epoch_{epoch}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "classification (1) (1).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "traffic_monitor",
   "language": "python",
   "name": "traffic_monitor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
