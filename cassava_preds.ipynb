{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cassava classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "edyDFX_Ih5-j"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from urllib.request import urlretrieve\n",
    "import pandas as pd\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import resample\n",
    "cudnn.benchmark = True\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n",
    "import timm\n",
    "from timm.loss import LabelSmoothingCrossEntropy, SoftTargetCrossEntropy, JsdCrossEntropy\n",
    "from utils import Mixup, RandAugment, RAdam\n",
    "from PIL import Image\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(SEED):\n",
    "    random.seed(SEED)\n",
    "    os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] =\"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the root directory dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "waXTuqeVh5-q"
   },
   "outputs": [],
   "source": [
    "root = os.path.join(os.environ[\"HOME\"], \"Workspace/datasets/taiyoyuden/cassava\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split files from the dataset into the train and validation sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some files in the dataset are broken, so we will use only those image files that OpenCV could load correctly. We will use 20000 images for training, 4936 images for validation, and 10 images for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QgLoujrNHY5Z"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test_images',\n",
       " 'label_num_to_disease_map.json',\n",
       " 'train_1_pseudo.csv',\n",
       " 'val_1_pseudo.csv',\n",
       " 'external',\n",
       " 'sample_submission.csv',\n",
       " 'train_images',\n",
       " 'label_num_to_disease_map.json.save',\n",
       " 'train.csv']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to visualize images and their labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a function that will take a list of images' file paths and their labels and visualize them in a grid. Correct labels are colored green, and incorrectly predicted labels are colored red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000015157.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000201771.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100042118.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000723321.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000812911.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         image_id  label\n",
       "0  1000015157.jpg      0\n",
       "1  1000201771.jpg      3\n",
       "2   100042118.jpg      1\n",
       "3  1000723321.jpg      1\n",
       "4  1000812911.jpg      3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2216849948.jpg</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000015157.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000201771.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         image_id  label\n",
       "0  2216849948.jpg      4\n",
       "1  1000015157.jpg      0\n",
       "2  1000201771.jpg      3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cassava Bacterial Blight (CBB)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cassava Brown Streak Disease (CBSD)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cassava Green Mottle (CGM)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cassava Mosaic Disease (CMD)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Healthy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     0\n",
       "0       Cassava Bacterial Blight (CBB)\n",
       "1  Cassava Brown Streak Disease (CBSD)\n",
       "2           Cassava Green Mottle (CGM)\n",
       "3         Cassava Mosaic Disease (CMD)\n",
       "4                              Healthy"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train = pd.read_csv(f'{root}/train.csv')\n",
    "train_external = pd.read_csv(f'{root}/external/train_external.csv')\n",
    "test_external = pd.read_csv(f'{root}/external/test_external.csv')\n",
    "test_external_pseudo = pd.read_csv(f'{root}/external/test_external_pseudo.csv')\n",
    "test = pd.read_csv(f'{root}/sample_submission.csv')\n",
    "label_map = pd.read_json(f'{root}/label_num_to_disease_map.json', \n",
    "                         orient='index')\n",
    "display(train.head())\n",
    "display(test.head())\n",
    "display(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7320,  910, 5440, 5241, 5784, 6315,  516, 4476, 5628, 8372, 1735,\n",
       "        819, 6999, 2483, 5361, 5101, 6470, 1234, 4605, 3435, 6446, 8716,\n",
       "       9324, 2608, 7899, 2097, 2797, 9217,  239, 2784, 3055, 4708, 1949,\n",
       "       7784, 1317, 1578, 3606, 3940, 8888, 5443, 8842, 8483, 7563, 2662,\n",
       "       7091, 9605, 6285, 5536, 7149, 9720])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map.iloc[1].values\n",
    "np.random.randint(50, 10000, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ho-BaziAHY5q"
   },
   "outputs": [],
   "source": [
    "def visualize_input_image_grid(filepaths, image_name, labels, cols=4):\n",
    "    rows = 5\n",
    "    figure, ax = plt.subplots(nrows=rows, ncols=cols, figsize=(25, 15))\n",
    "    for i, index in enumerate(np.random.randint(0, len(image_name), 20)):\n",
    "        name = image_name.iloc[index]['image_id']\n",
    "        label =  image_name.iloc[index]['label']\n",
    "        image = cv2.imread(f'{filepaths}/train_images/{name}')\n",
    "        if (i == 0): \n",
    "            print(image.shape)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        ax.ravel()[i].imshow(image)\n",
    "        ax.ravel()[i].set_title(label, color='GREEN')\n",
    "        ax.ravel()[i].set_axis_off()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the parameters of the whole process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_name = [\"resnest26d\",\"resnest50d\",\"tf_efficientnet_b3_ns\", \"skresnet34\" ,\"cspresnet50\", \"vit_base_patch16_384\"]\n",
    "WEIGHTS = [\n",
    "    \"weights/resnest26d/resnest26d_fold0_best_epoch_28_final_1st.pth\", #0\n",
    "    \"weights/resnest26d/resnest26d_fold0_best_epoch_13_final_mixup.pth\",\n",
    "    \"weights/resnest26d/resnest26d_fold1_best_epoch_7_finale_ex_hnm.pth\",\n",
    "    \"weights/resnest26d/resnest26d_fold1_best_epoch_27_final_mix.pth\",\n",
    "    \"weights/resnest26d/resnest26d_fold2_best_epoch_3_final_hnm.pth\",\n",
    "    \"weights/resnest26d/resnest26d_fold4_best_epoch_29_1st.pth\", #5\n",
    "    \"weights/resnest26d/resnest26d_fold4_best_epoch_26_mix.pth\", \n",
    "    \"weights/resnest26d/resnest26d_fold4_best_epoch_12_cutmix.pth\", \n",
    "    \"weights/resnest26d/resnest26d_fold4_best_epoch_3_external.pth\",\n",
    "    \"weights/resnest26d/resnest26d_fold4_best_epoch_21_final_512.pth\",\n",
    "    \"weights/tf_efficientnet_b3_ns/tf_efficientnet_b3_ns_fold1_best_epoch_19_external.pth\", #10\n",
    "    \"weights/tf_efficientnet_b3_ns/tf_efficientnet_b3_ns_fold1_best_epoch_26_512.pth\", \n",
    "    \"weights/tf_efficientnet_b3_ns/tf_efficientnet_b3_ns_fold1_best_epoch_1_final_512.pth\", \n",
    "    \"weights/resnest50d/resnest50d_fold1_best_epoch_95_final_1st.pth\",\n",
    "    \"weights/resnest50d/resnest50d_fold1_best_epoch_9_final_512.pth\",\n",
    "    \"weights/resnest50d/resnest50d_fold1_best_epoch_82.pth\", #15 # test balance data (0.86)\n",
    "    \"weights/cspresnet50/cspresnet50_fold1_best_epoch_24_final.pth\", \n",
    "        \n",
    "]\n",
    "# Single model\n",
    "model_index = 0\n",
    "ckpt_index = 2\n",
    "\n",
    "# kfold\n",
    "fold_model_index = 0\n",
    "fold_ckpt_index = [2,3]\n",
    "fold_ckpt_weight = [1,1]\n",
    "\n",
    "# Ensemble\n",
    "ensemble_models_name = [\"resnest26d\" ,\"tf_efficientnet_b3_ns\", \"resnest50d\"]\n",
    "ensemble_ckpt_index = [2, 12, 13]\n",
    "ensemble_ckpt_weight = [1, 1, 1]\n",
    "\n",
    "params = {\n",
    "    \"visualize\": False,\n",
    "    \"fold\": 1,\n",
    "    \"train_external\": True,\n",
    "    \"test_external\": True,\n",
    "    \"load_pretrained\": True,\n",
    "    \"resume\": False,\n",
    "    \"image_size\": 512,\n",
    "    \"num_classes\": 5,\n",
    "    \"model\": models_name[model_index],\n",
    "    \"device\": \"cuda\",\n",
    "    \"lr\": 5e-5,\n",
    "    \"lr_min\":1e-6,\n",
    "    \"batch_size\": 8,\n",
    "    \"num_workers\": 8,\n",
    "    \"epochs\": 100,\n",
    "    \"gradient_accumulation_steps\": 8,\n",
    "    \"drop_block\": 0.3,\n",
    "    \"drop_rate\": 0.3,\n",
    "    \"mix_up\": True,\n",
    "    \"cutmix\":True,\n",
    "    \"rand_aug\": False,\n",
    "    \"local_rank\":0,\n",
    "    \"distributed\": False,\n",
    "    \"hard_negative_sample\": False,\n",
    "    \"tta\": True,\n",
    "    \"crops_tta\":False,\n",
    "    \"train_phase\":False,\n",
    "    \"balance_data\":False,\n",
    "    \"kfold_pred\":True,\n",
    "    \"ensemble\": True,\n",
    "    \"error_analysis\":True,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define dataset with KFolds strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = train.copy()\n",
    "Fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "for n, (train_index, val_index) in enumerate(Fold.split(folds, folds['label'])):\n",
    "    folds.loc[val_index, 'fold'] = int(n)\n",
    "folds['fold'] = folds['fold'].astype(int)\n",
    "###\n",
    "fold = params[\"fold\"]\n",
    "train_idx = folds[folds['fold'] != fold].index\n",
    "val_idx = folds[folds['fold'] == fold].index\n",
    "\n",
    "train_folds = folds.loc[train_idx].reset_index(drop=True)\n",
    "val_folds = folds.loc[val_idx].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, df, transform=None, mosaic_mix = False):\n",
    "        self.df = df\n",
    "        self.file_names = df['image_id'].values\n",
    "        self.labels = df['label'].values\n",
    "        self.transform = transform\n",
    "        self.mosaic_mix = mosaic_mix\n",
    "        self.rand_aug_fn = RandAugment()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_name = self.file_names[idx]\n",
    "        file_path = f'{root}/train_images/{file_name}'\n",
    "        image = cv2.imread(file_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        label = torch.tensor(self.labels[idx]).long()\n",
    "        if params[\"rand_aug\"]:\n",
    "            image = np.array(self.rand_aug_fn(Image.fromarray(image)))\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "\n",
    "        return image, label, file_name\n",
    "    \n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, df, transform=None, valid_test=False, fcrops=False):\n",
    "        self.df = df\n",
    "        self.file_names = df['image_id'].values\n",
    "        self.transform = transform\n",
    "        self.valid_test = valid_test\n",
    "        self.fcrops = fcrops\n",
    "        if self.valid_test:\n",
    "            self.labels = df['label'].values  \n",
    "        else:\n",
    "            assert ValueError(\"Test data does not have annotation, plz check!\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_name = self.file_names[idx]\n",
    "        if self.valid_test:\n",
    "            file_path = f'{root}/train_images/{file_name}'\n",
    "            #file_path = f'{root}/external/extraimages/{file_name}'\n",
    "        else:\n",
    "            file_path = f'{root}/test_images/{file_name}'\n",
    "        image = cv2.imread(file_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        if isinstance(self.transform, list):\n",
    "            outputs = {'images':[],\n",
    "                       'labels':[],\n",
    "                       'image_ids':[]}\n",
    "            if self.fcrops:\n",
    "                for trans in self.transform:\n",
    "                    image_aug = Image.open(file_path)\n",
    "                    image_aug = trans(image_aug)\n",
    "                    outputs[\"images\"].append(image_aug)\n",
    "                    del image_aug\n",
    "            else:\n",
    "                for trans in self.transform:\n",
    "                    augmented = trans(image=image)\n",
    "                    image_aug = augmented['image']\n",
    "                    outputs[\"images\"].append(image_aug)\n",
    "                    del image_aug\n",
    "\n",
    "            if self.valid_test:\n",
    "                label = torch.tensor(self.labels[idx]).long()\n",
    "                outputs['labels'] = len(self.transform)*[label]\n",
    "                outputs['image_ids'].append(file_name)\n",
    "                \n",
    "            else:\n",
    "                outputs['labels'] = len(self.transform)*[-1]\n",
    "                \n",
    "            return outputs\n",
    "        else:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image'] \n",
    "        return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Albumentations to define transformation functions for the train and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mm_NP-c4h5-_"
   },
   "outputs": [],
   "source": [
    "train_transform = A.Compose(\n",
    "    [\n",
    "        A.RandomResizedCrop(height=params[\"image_size\"], width=params[\"image_size\"], p=1),\n",
    "        A.OneOf([\n",
    "            A.RandomRotate90(p=0.5),\n",
    "            A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),], p=1.\n",
    "        ),\n",
    "#         A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.IAAAffine(rotate=0.2, shear=0.2,p=0.5),\n",
    "        A.CoarseDropout(max_holes=20, max_height=int(params[\"image_size\"]/15), max_width=int(params[\"image_size\"]/15), p=0.5),\n",
    "#         A.IAAAdditiveGaussianNoise(p=1.),\n",
    "        A.MedianBlur(p=0.5),\n",
    "        A.Equalize(p=0.2),\n",
    "        A.GridDistortion(p=0.2),\n",
    "#         A.RandomGridShuffle(p=0.5),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "val_transform = A.Compose(\n",
    "    [\n",
    "        A.CenterCrop(height=params[\"image_size\"], width=params[\"image_size\"], p=1),\n",
    "        A.Resize(params[\"image_size\"],params[\"image_size\"]),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "if params[\"cutmix\"]:\n",
    "    mixup_fn = Mixup(mixup_alpha=1., cutmix_alpha=1., label_smoothing=0.1, num_classes=params[\"num_classes\"])\n",
    "else:\n",
    "    mixup_fn = Mixup(mixup_alpha=1., label_smoothing=0.1, num_classes=params[\"num_classes\"])\n",
    "\n",
    "val_dataset = TrainDataset(val_folds, transform=val_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_tta0 = A.Compose(\n",
    "    [\n",
    "     A.CenterCrop(height=params[\"image_size\"], width=params[\"image_size\"], p=1),    \n",
    "     A.Resize(height=params[\"image_size\"], width=params[\"image_size\"], p=1),\n",
    "     A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),   \n",
    "     ToTensorV2()\n",
    "    ]\n",
    ")\n",
    "\n",
    "transform_tta1 = A.Compose(\n",
    "    [\n",
    "     A.CenterCrop(height=params[\"image_size\"], width=params[\"image_size\"], p=1),\n",
    "     A.Resize(height=params[\"image_size\"], width=params[\"image_size\"], p=1),\n",
    "     A.HorizontalFlip(p=1.),\n",
    "     A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),   \n",
    "     ToTensorV2()\n",
    "    ]\n",
    ")\n",
    "transform_tta2 = A.Compose(\n",
    "    [\n",
    "     A.CenterCrop(height=params[\"image_size\"], width=params[\"image_size\"], p=1),\n",
    "     A.Resize(height=params[\"image_size\"], width=params[\"image_size\"], p=1),\n",
    "     A.VerticalFlip(p=1.),\n",
    "     A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "     ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "transform_tta3 = A.Compose(\n",
    "    [\n",
    "     A.CenterCrop(height=params[\"image_size\"], width=params[\"image_size\"], p=1),\n",
    "     A.Resize(height=params[\"image_size\"], width=params[\"image_size\"], p=1),\n",
    "     A.RandomRotate90(p=1.),\n",
    "     A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),     \n",
    "     ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "##### Test TTA with Five Crops\n",
    "transform_crop_tta0 = T.Compose([\n",
    "                T.FiveCrop(params[\"image_size\"]),\n",
    "                T.Lambda(lambda crops: ([T.ToTensor()(crop) for crop in crops])),\n",
    "                T.Lambda(lambda norms: torch.stack([T.Normalize(mean=[0.5], std=[0.5])(norm) for norm in norms]))\n",
    "        ]\n",
    ")\n",
    "    \n",
    "transform_crop_tta1 = T.Compose([\n",
    "                T.FiveCrop(params[\"image_size\"]),\n",
    "                T.Lambda(lambda crops: ([T.ToTensor()(crop) for crop in crops])),\n",
    "                T.Lambda(lambda flips: ([T.RandomHorizontalFlip(p=1.)(flip) for flip in flips])),\n",
    "                T.Lambda(lambda norms: torch.stack([T.Normalize(mean=[0.5], std=[0.5])(norm) for norm in norms]))\n",
    "    ]\n",
    ")\n",
    "transform_crop_tta2 = T.Compose([\n",
    "                T.FiveCrop(params[\"image_size\"]),\n",
    "                T.Lambda(lambda crops: ([T.ToTensor()(crop) for crop in crops])),\n",
    "                T.Lambda(lambda flips: ([T.RandomVerticalFlip(p=1.)(flip) for flip in flips])),\n",
    "                T.Lambda(lambda norms: torch.stack([T.Normalize(mean=[0.5], std=[0.5])(norm) for norm in norms]))\n",
    "    ]\n",
    ")\n",
    "test_transform_tta = [transform_tta0, transform_tta1, transform_tta2, transform_tta3, transform_tta3]\n",
    "test_transform_tta_crops = [transform_crop_tta0, transform_crop_tta1, transform_crop_tta2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also let's define a function that takes a dataset and visualizes different augmentations applied to the same image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bgTqE0eYSjDh"
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(output, target):\n",
    "#     return torch.true_divide((target == output).sum(dim=0), output.size(0)).item()\n",
    "    if params[\"mix_up\"]:\n",
    "        output = torch.argmax(torch.softmax(output, dim=1), dim=1)\n",
    "        return accuracy_score(output.cpu(), target.argmax(1).cpu())\n",
    "    \n",
    "    output = torch.softmax(output, dim=1)\n",
    "    return accuracy_score(output.argmax(1).cpu(), target.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zRhPJiCch5_D"
   },
   "outputs": [],
   "source": [
    "class MetricMonitor:\n",
    "    def __init__(self, float_precision=3):\n",
    "        self.float_precision = float_precision\n",
    "        self.reset()\n",
    "        self.curr_acc = 0.\n",
    "    def reset(self):\n",
    "        self.metrics = defaultdict(lambda: {\"val\": 0, \"count\": 0, \"avg\": 0})\n",
    "\n",
    "    def update(self, metric_name, val):\n",
    "        metric = self.metrics[metric_name]\n",
    "\n",
    "        metric[\"val\"] += val\n",
    "        metric[\"count\"] += 1\n",
    "        metric[\"avg\"] = metric[\"val\"] / metric[\"count\"]\n",
    "        self.curr_acc = metric[\"avg\"]\n",
    "    def __str__(self):\n",
    "        return \" | \".join(\n",
    "            [\n",
    "                \"{metric_name}: {avg:.{float_precision}f}\".format(\n",
    "                    metric_name=metric_name, avg=metric[\"avg\"], float_precision=self.float_precision\n",
    "                )\n",
    "                for (metric_name, metric) in self.metrics.items()\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pretrained model: resnest26d  0.8997\n"
     ]
    }
   ],
   "source": [
    "def declare_model(name, index=None):\n",
    "    model = timm.create_model(name,\n",
    "            pretrained=False,\n",
    "            num_classes=params[\"num_classes\"],\n",
    "            drop_rate=params[\"drop_rate\"])\n",
    "    model = model.to(params[\"device\"])\n",
    "    \n",
    "    if params[\"distributed\"]:\n",
    "        assert ValueError(\"No need to implement in a single machine\")\n",
    "    else:\n",
    "        model = torch.nn.DataParallel(model) \n",
    "        \n",
    "    if params[\"load_pretrained\"]:\n",
    "        if index == None: \n",
    "            return model\n",
    "        state_dict = torch.load(WEIGHTS[index])\n",
    "        print(f\"Load pretrained model: {name} \",state_dict[\"preds\"])\n",
    "        model.load_state_dict(state_dict[\"model\"])\n",
    "        best_acc = state_dict[\"preds\"]   \n",
    "    return model\n",
    "\n",
    "model = declare_model(params[\"model\"], ckpt_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_features = model.fc.in_features\n",
    "# model.fc = nn.Linear(n_features, params['num_classes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create all required objects and functions for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W7HipbJ8h5_I"
   },
   "outputs": [],
   "source": [
    "val_criterion = nn.CrossEntropyLoss().to(params[\"device\"])\n",
    "# criterion = nn.CrossEntropyLoss().to(params[\"device\"])\n",
    "criterion = LabelSmoothingCrossEntropy().to(params[\"device\"])\n",
    "if params[\"mix_up\"]:\n",
    "    criterion = SoftTargetCrossEntropy().to(params[\"device\"])         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------\n",
    "## Make validation prediction w/TTA on validation and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TestDataset(train_folds, transform=test_transform_tta, valid_test=True)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=params[\"batch_size\"], shuffle=True, num_workers=params[\"num_workers\"], pin_memory=True,\n",
    ")\n",
    "if params[\"tta\"]:\n",
    "    val_pred_dataset = TestDataset(val_folds, transform=test_transform_tta, valid_test=True)\n",
    "    test_pred_dataset = TestDataset(test, transform=test_transform_tta)\n",
    "else:\n",
    "    val_pred_dataset = TestDataset(val_folds, transform=val_transform, valid_test=True)\n",
    "    test_pred_dataset = TestDataset(test, transform=val_transform)\n",
    "\n",
    "val_pred_dataset_crops = TestDataset(val_folds, transform=test_transform_tta_crops,valid_test=True, fcrops=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred_loader = DataLoader(\n",
    "    val_pred_dataset, batch_size=params['batch_size'], shuffle=False, num_workers=2, pin_memory=True,\n",
    ")\n",
    "val_pred_loader_crop = DataLoader(\n",
    "    val_pred_dataset_crops, batch_size=params['batch_size'], shuffle=False, num_workers=2, pin_memory=True,\n",
    ")\n",
    "test_pred_loader = DataLoader(\n",
    "    test_pred_dataset, batch_size=params['batch_size'], shuffle=False, num_workers=2, pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_tta(data, pred):     \n",
    "    unorm = UnNormalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "    if params[\"tta\"]:\n",
    "        figure, ax = plt.subplots(nrows=1, ncols=num_tta, figsize=(12, 6))\n",
    "        for i, image in enumerate(data[\"images\"]):\n",
    "            image = (unorm(image[0]).cpu().numpy()*255).astype(int)\n",
    "            ax.ravel()[i].imshow(image.transpose(2,1,0))\n",
    "            ax.ravel()[i].set_title(str(pred), color='GREEN')\n",
    "            ax.ravel()[i].set_axis_off()\n",
    "        plt.tight_layout()\n",
    "        plt.show() \n",
    "    else:\n",
    "        image = (unorm(data).cpu().numpy()*255).astype(int)\n",
    "        imgplot = plt.imshow(image[0].transpose(2,1,0))\n",
    "        imgplot = plt.title(str(pred), color='GREEN')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, model, criterion, epoch, params, fold, best_acc):\n",
    "    metric_monitor = MetricMonitor()\n",
    "    model.eval()\n",
    "    stream = tqdm(val_loader)\n",
    "    with torch.no_grad():\n",
    "        for i, (images, target, _) in enumerate(stream, start=1):\n",
    "            images = images.to(params[\"device\"], non_blocking=True)\n",
    "            target = target.to(params[\"device\"], non_blocking=True)#.view(-1,params['batch_size'])\n",
    "            output = model(images)\n",
    "            loss = val_criterion(output, target)\n",
    "            output = torch.softmax(output, dim = 1)\n",
    "            \n",
    "            accuracy = accuracy_score(output.argmax(1).cpu(), target.cpu())\n",
    "\n",
    "            stream.set_description(\n",
    "                \"Epoch: {epoch}. Validation. {metric_monitor}\".format(epoch=epoch, metric_monitor=metric_monitor)\n",
    "            )           \n",
    "            metric_monitor.update(\"Loss\", loss.item())\n",
    "            metric_monitor.update(\"Accuracy\", accuracy)\n",
    "        #to save weight\n",
    "        if (metric_monitor.curr_acc > best_acc): # or epoch == params[\"epochs\"]:\n",
    "            print(f\"Save best weight at acc {round(metric_monitor.curr_acc,4)}, epoch: {epoch}\")\n",
    "            torch.save({'model': model.state_dict(), \n",
    "                'preds': round(metric_monitor.curr_acc,4)},\n",
    "                 f'weights/{params[\"model\"]}_fold{fold}_best_epoch_{epoch}.pth')\n",
    "\n",
    "            best_acc = metric_monitor.curr_acc\n",
    "    return best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TTA Validation. Accuracy: 0.943:   4%|▍         | 22/535 [00:11<04:12,  2.03it/s]"
     ]
    }
   ],
   "source": [
    "def tta_validate(loader, model, params, valid_test=False):\n",
    "    num_tta = len(test_transform_tta)\n",
    "    incorrect_pred_list = {'image_id':[],\n",
    "                       'label':[],\n",
    "                       'pred':[],\n",
    "                       'prob':[]}\n",
    "    correct_pred_list = {'image_id':[],\n",
    "                       'label':[],\n",
    "                       'pred':[],\n",
    "                       'prob':[]}    \n",
    "    metric_monitor = MetricMonitor()\n",
    "    model.eval()\n",
    "    stream = tqdm(loader)\n",
    "    count_change = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(stream, start=1):\n",
    "            if params[\"visualize\"]:\n",
    "                visualize_tta(data, pred)\n",
    "            tta_output = []   \n",
    "            for i, image in enumerate(data[\"images\"]):\n",
    "                tta_output.append(model(image))\n",
    "            tta_output = torch.stack(tta_output, dim=0).mean(dim=0)\n",
    "            output = torch.softmax(tta_output, dim=1)\n",
    "            pred = output.argmax(1)\n",
    "            if params[\"error_analysis\"]:\n",
    "                topk_output, topk_ids = torch.topk(output, params[\"num_classes\"])\n",
    "                for i in range(len(data[\"labels\"][0])):\n",
    "                    ## adjust the output prediction\n",
    "                    max_1st = topk_ids[i][0]\n",
    "                    max_2nd = topk_ids[i][1]\n",
    "                    if  max_1st == 0 and max_2nd == 4 and output[i][max_2nd] > 0.35:\n",
    "#                     if  max_1st == 0 and max_2nd == 4 and output[i][max_1st] < 0.49:\n",
    "                        pred[i] = max_2nd\n",
    "                        count_change+=1\n",
    "                    if max_1st == 3 and max_2nd == 2 and output[i][max_2nd] > 0.35:\n",
    "#                     if max_1st == 3 and max_2nd == 2 and output[i][max_1st] < 0.49:\n",
    "                        \n",
    "                        pred[i] = max_2nd\n",
    "                        count_change+=1\n",
    "                    \n",
    "                    if data[\"labels\"][0][i] != output.argmax(1).cpu()[i]:\n",
    "                        incorrect_pred_list['image_id'].append(data[\"image_ids\"][0][i])                        \n",
    "                        incorrect_pred_list['label'].append(data[\"labels\"][0][i].cpu().numpy())\n",
    "                        incorrect_pred_list['pred'].append(output.argmax(1).cpu()[i].cpu().numpy())\n",
    "                        incorrect_pred_list['prob'].append(output[i].cpu().numpy())\n",
    "                    else:\n",
    "                        correct_pred_list['image_id'].append(data[\"image_ids\"][0][i])                        \n",
    "                        correct_pred_list['label'].append(data[\"labels\"][0][i].cpu().numpy())\n",
    "                        correct_pred_list['pred'].append(output.argmax(1).cpu()[i].cpu().numpy())\n",
    "                        correct_pred_list['prob'].append(output[i].cpu().numpy())   \n",
    "            accuracy = accuracy_score(pred.cpu(), data[\"labels\"][0].cpu())\n",
    "            metric_monitor.update(\"Accuracy\", accuracy)            \n",
    "            stream.set_description(\n",
    "                \"TTA Validation. {metric_monitor}\".format(metric_monitor=metric_monitor)\n",
    "            )\n",
    "    print(f\"Total count change: {count_change}\")\n",
    "    return incorrect_pred_list, correct_pred_list\n",
    "if params[\"tta\"]:\n",
    "    incorrect_val_list, correct_val_list = tta_validate(val_pred_loader, model, params, valid_test=True)\n",
    "\n",
    "#     if params[\"error_analysis\"]:\n",
    "#         incorrect_val = pd.DataFrame(incorrect_val_list)\n",
    "#         incorrect_val.to_csv(f'val_{params[\"model\"]}_{params[\"fold\"]}_incorrect.csv' ,index=False)\n",
    "#         correct_val = pd.DataFrame(correct_val_list)\n",
    "#         correct_val.to_csv(f'val_{params[\"model\"]}_{params[\"fold\"]}_correct.csv' ,index=False)\n",
    "        \n",
    "#         incorrect_train_list, correct_train_list =  tta_validate(train_loader, model, params, valid_test=True)\n",
    "#         incorrect_train = pd.DataFrame(incorrect_train_list)\n",
    "#         incorrect_train.to_csv(f'train_{params[\"model\"]}_{params[\"fold\"]}_incorrect.csv' ,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[0.1,0.2,0.3],\n",
    "                  [0.3,0.5,0.4],\n",
    "                  [0.8,0.2,0.9]])\n",
    "top = torch.topk(x,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([[0.3000, 0.2000, 0.1000],\n",
       "        [0.5000, 0.4000, 0.3000],\n",
       "        [0.9000, 0.8000, 0.2000]]),\n",
       "indices=tensor([[2, 1, 0],\n",
       "        [1, 2, 0],\n",
       "        [2, 0, 1]]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def visualize_class_dis(df, fold, color='maroon'):\n",
    "    x = df.groupby('label').count()\n",
    "    x_p = df.groupby('pred').count()\n",
    "    y = [0,1,2,3,4]\n",
    "\n",
    "    fig = plt.figure(figsize=(10,5))\n",
    "    plt.bar(y, x['image_id'].values, color = color, width = 0.4)\n",
    "    plt.xlabel(\"Label\") \n",
    "    plt.ylabel(\"Number of image\") \n",
    "    plt.title(f\"Class distribution of data fold: {fold}\") \n",
    "    \n",
    "    fig = plt.figure(figsize=(10,5))\n",
    "    plt.bar(y, x_p['image_id'].values, color = 'maroon', width = 0.4)\n",
    "    plt.xlabel(\"Label\") \n",
    "    plt.ylabel(\"Number of image\") \n",
    "    plt.title(f\"Class distribution of data fold: {fold}\") \n",
    "\n",
    "    # plt.xticks(x['image_id'])\n",
    "    plt.show()\n",
    "if params[\"error_analysis\"]:\n",
    "    incorrect_val = pd.read_csv(f'val_{params[\"model\"]}_{params[\"fold\"]}_incorrect.csv')\n",
    "    incorrect_train = pd.read_csv(f'train_{params[\"model\"]}_{params[\"fold\"]}_incorrect.csv')\n",
    "    visualize_class_dis(incorrect_val, 'incorrect', color='blue')\n",
    "    visualize_class_dis(incorrect_train, 'correct', color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tta_crop = len(test_transform_tta_crops)\n",
    "def tta_crops_validate(loader, model, params, valid_test=False):\n",
    "    metric_monitor = MetricMonitor()\n",
    "    model.eval()\n",
    "    stream = tqdm(loader)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(stream, start=1):\n",
    "            if params[\"visualize\"]:\n",
    "                visualize_tta(data, pred)\n",
    "            tta_output = []   \n",
    "            for i, image in enumerate(data[\"images\"]):\n",
    "                batch, ncrops, c, h, w = image.size()\n",
    "                image = image.view(-1, c, h, w)\n",
    "                output = model(image)\n",
    "                tta_output.append(output.view(batch, ncrops, -1).mean(1))\n",
    "            tta_output = torch.stack(tta_output, dim=0).mean(dim=0)\n",
    "            output = torch.softmax(tta_output, dim=1)\n",
    "            accuracy = accuracy_score(output.argmax(1).cpu(), data[\"labels\"][0].cpu())\n",
    "                \n",
    "            metric_monitor.update(\"Accuracy\", accuracy)            \n",
    "            stream.set_description(\n",
    "                \"Crops TTA Validation. {metric_monitor}\".format(metric_monitor=metric_monitor)\n",
    "            )   \n",
    "## Re test the validation accuracy with TTA\n",
    "if params[\"crops_tta\"]:\n",
    "    tta_crops_validate(val_pred_loader_crop, model, params, valid_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/535 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pretrained model: weights/resnest26d/resnest26d_fold1_best_epoch_7_finale_ex_hnm.pth with acc: 0.8997\n",
      "Load pretrained model: weights/resnest26d/resnest26d_fold1_best_epoch_27_final_mix.pth with acc: 0.8972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Folds TTA Validation. Accuracy: 0.910:  14%|█▍        | 74/535 [00:32<03:22,  2.28it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-2cae932035a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m             ) \n\u001b[1;32m     31\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tta\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"fold\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mkfold_tta_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_pred_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-28-2cae932035a4>\u001b[0m in \u001b[0;36mkfold_tta_validate\u001b[0;34m(loader, params, valid_test)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mtta_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtta_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtta_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mmetric_monitor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# CV w/wo fold predict on validation set\n",
    "def kfold_tta_validate(loader, params, valid_test=False):\n",
    "    model = declare_model(models_name[fold_model_index])\n",
    "    metric_monitor = MetricMonitor()\n",
    "    model.eval()\n",
    "    stream = tqdm(loader)\n",
    "    models = []\n",
    "    for ckpt, weight in zip(fold_ckpt_index, fold_ckpt_weight):\n",
    "        state_dict = torch.load(WEIGHTS[ckpt])\n",
    "        fold_acc = state_dict[\"preds\"]\n",
    "        print(f\"Load pretrained model: {WEIGHTS[ckpt]} with acc: {fold_acc}\")\n",
    "        model.load_state_dict(state_dict[\"model\"])\n",
    "        models.append(model)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(stream, start=1):\n",
    "            if params[\"visualize\"]:\n",
    "                visualize_tta(data, pred)\n",
    "            tta_output = []   \n",
    "            for i, image in enumerate(data[\"images\"]):\n",
    "                kfout = [model(image) for model in models]\n",
    "                tta_output.append(torch.stack(kfout, dim=0).mean(dim=0))\n",
    "            tta_output = torch.stack(tta_output, dim=0).mean(dim=0)\n",
    "            output = torch.softmax(tta_output, dim=1)\n",
    "            accuracy = accuracy_score(output.argmax(1).cpu(), data[\"labels\"][0].cpu())\n",
    "                \n",
    "            metric_monitor.update(\"Accuracy\", accuracy)            \n",
    "            stream.set_description(\n",
    "                \"Folds TTA Validation. {metric_monitor}\".format(metric_monitor=metric_monitor)\n",
    "            ) \n",
    "if params[\"tta\"] and params[\"fold\"]:\n",
    "    kfold_tta_validate(val_pred_loader, params, valid_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble CV w/ fold predict on validation set\n",
    "def ensemble_w_kfold_tta_validate(loader, models, params, valid_test=False):\n",
    "    metric_monitor = MetricMonitor()\n",
    "    stream = tqdm(loader)\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(stream, start=1):\n",
    "            if params[\"visualize\"]:\n",
    "                visualize_tta(data, pred)\n",
    "            tta_output = []   \n",
    "            for i, image in enumerate(data[\"images\"]):\n",
    "                kfout = [model(image) for model in models]\n",
    "                tta_output.append(torch.stack(kfout, dim=0).mean(dim=0))\n",
    "            tta_output = torch.stack(tta_output, dim=0).mean(dim=0)\n",
    "            output = torch.softmax(tta_output, dim=1)\n",
    "            accuracy = accuracy_score(output.argmax(1).cpu(), data[\"labels\"][0].cpu())\n",
    "            metric_monitor.update(\"Accuracy\", accuracy)            \n",
    "            stream.set_description(\n",
    "                \"Ensemble Validation. {metric_monitor}\".format(metric_monitor=metric_monitor)\n",
    "            ) \n",
    "if params[\"ensemble\"] and params[\"tta\"] and params[\"fold\"]:\n",
    "    models = []\n",
    "    print(f\"Ensemble below models: {ensemble_models_name}\")\n",
    "    for name, ckpt, weight in zip(ensemble_models_name, ensemble_ckpt_index, ensemble_ckpt_weight):\n",
    "        print(ckpt)\n",
    "        m = declare_model(name, ckpt)  \n",
    "        models.append(m)\n",
    "        del m\n",
    "    ensemble_w_kfold_tta_validate(val_pred_loader, models, params, valid_test=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------\n",
    "## Make Test prediction w/wo Kfold check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tta = len(test_transform_tta)\n",
    "def predict(loader, model, params, valid_test=False):\n",
    "    outputs = []\n",
    "    preds = []\n",
    "    model.eval()\n",
    "    stream = tqdm(loader)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(stream, start=1):\n",
    "            if params[\"tta\"]:\n",
    "                ## TTA output\n",
    "                tta_output = []   \n",
    "                for i, image in enumerate(data[\"images\"]):\n",
    "                    tta_output.append(model(image))\n",
    "                tta_output = torch.stack(tta_output, dim=0).mean(dim=0)\n",
    "                output = torch.softmax(tta_output, dim=1)\n",
    "                outputs.append(output)\n",
    "            else:\n",
    "                data = data.to(params[\"device\"], non_blocking=True)\n",
    "                output = model(data)\n",
    "                output = torch.softmax(output, dim = 1)\n",
    "                outputs.append(output)\n",
    "                \n",
    "            pred = torch.argmax(output, dim=1).cpu().numpy()\n",
    "            preds.extend(pred)\n",
    "            ## For visualize the TTA \n",
    "            if params[\"visualize\"]:\n",
    "                visualize_tta(data, pred)\n",
    "    if params[\"kfold_pred\"]:\n",
    "        return torch.stack(outputs, dim=0)\n",
    "    else:\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV w/wo fold predict\n",
    "if params[\"kfold_pred\"]:\n",
    "    fold_preds = []\n",
    "    for ckpt, weight in zip(fold_ckpt_index, fold_ckpt_weight):\n",
    "        state_dict = torch.load(WEIGHTS[ckpt])\n",
    "        fold_acc = state_dict[\"preds\"]\n",
    "        print(f\"Load pretrained model: {weights[ckpt]} with acc: {fold_acc}\")\n",
    "        model.load_state_dict(state_dict[\"model\"])\n",
    "        best_acc = state_dict[\"preds\"]\n",
    "        out = predict(test_pred_loader, model, params, valid_test=True).mul(weight)\n",
    "        fold_preds.append(out)\n",
    "        del(out)\n",
    "    final_out_fold = torch.softmax(torch.stack(fold_preds).mean(dim=0), dim=2).view(-1,5).argmax(1)\n",
    "    final_out = final_out_fold.cpu().numpy()\n",
    "else:\n",
    "    final_out = predict(test_pred_loader, model, params, valid_test=True)\n",
    "    \n",
    "test['label'] = final_out \n",
    "test.to_csv('submission.csv', index=False)\n",
    "test.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV w/wo fold predict\n",
    "def ensemble_w_kfold_tta_pred(loader, models, params, valid_test=False):\n",
    "    metric_monitor = MetricMonitor()\n",
    "    stream = tqdm(loader)\n",
    "    outputs = []\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(stream, start=1):\n",
    "            if params[\"visualize\"]:\n",
    "                visualize_tta(data, pred)\n",
    "            tta_output = []   \n",
    "            for i, image in enumerate(data[\"images\"]):\n",
    "                kfout = [model(image) for model in models]\n",
    "                tta_output.append(torch.stack(kfout, dim=0).mean(dim=0))\n",
    "            tta_output = torch.stack(tta_output, dim=0).mean(dim=0)\n",
    "            output = torch.softmax(tta_output, dim=1)\n",
    "            outputs.append(output.argmax(1).cpu().numpy())\n",
    "    return outputs\n",
    "\n",
    "if params[\"ensemble\"]:\n",
    "    models = []\n",
    "    print(f\"Ensemble below models: {ensemble_models_name}\")\n",
    "    for name, ckpt, weight in zip(ensemble_models_name, ensemble_ckpt_index, ensemble_ckpt_weight):\n",
    "        m = declare_model(name, ckpt)  \n",
    "        models.append(m)\n",
    "        del m\n",
    "    final_out_ensemble = ensemble_w_kfold_tta_pred(test_pred_loader, models, params, valid_test=True)[0]\n",
    "    \n",
    "test['label'] = final_out_ensemble \n",
    "test.to_csv('submission.csv', index=False)\n",
    "test.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params[\"tta\"] = True\n",
    "create_pseudo = False\n",
    "num_tta = len(test_transform_tta)\n",
    "def pseudo_predict(loader, model, params, valid_test=False):\n",
    "    preds = {'image_id':[],\n",
    "             'label': []}\n",
    "    model.eval()\n",
    "    stream = tqdm(loader)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(stream, start=1):\n",
    "            ## TTA output\n",
    "            tta_output = []   \n",
    "            for i, image in enumerate(data[\"images\"]):\n",
    "                tta_output.append(model(image))\n",
    "            tta_output = torch.stack(tta_output, dim=0).mean(dim=0)\n",
    "            output = torch.softmax(tta_output, dim=1)\n",
    "            pred = torch.argmax(output, dim=1).cpu().numpy()\n",
    "            for j,p in enumerate(pred):\n",
    "                if output[j][p] > 0.7:\n",
    "                    preds['image_id'].append(data[\"image_ids\"][0][j])\n",
    "                    preds['label'].append(p)\n",
    "    return preds\n",
    "if create_pseudo:\n",
    "    ## add external data\n",
    "    test_external_dataset = TestDataset(test_external, transform=test_transform_tta, valid_test=True)\n",
    "    test_external_loader = DataLoader(\n",
    "        test_external_dataset, batch_size=params['batch_size'], shuffle=False, num_workers=4, pin_memory=True,\n",
    "    )  \n",
    "    pred_test_external = pseudo_predict(test_external_loader, model, params, valid_test=True)\n",
    "    pseudo = pd.DataFrame(pred_test_external)\n",
    "    pseudo.to_csv(f'{root}/external/test_external_pseudo.csv' ,index=False)\n",
    "    visualize_class_dis(pseudo, 'external pseudo data')\n",
    "\n",
    "    \n",
    "pseudo_val_dataset = TestDataset(val_folds, transform=test_transform_tta, valid_test=True)\n",
    "pseudo_val_loader = DataLoader(\n",
    "    pseudo_val_dataset, batch_size=params['batch_size'], shuffle=False, num_workers=4, pin_memory=True,\n",
    ")  \n",
    "pred_val_pseudo = pseudo_predict(pseudo_val_loader, model, params, valid_test=True)\n",
    "pseudo = pd.DataFrame(pred_val_pseudo)\n",
    "pseudo.to_csv(f'{root}/val_{params[\"fold\"]}_pseudo.csv' ,index=False)\n",
    "\n",
    "    \n",
    "pseudo_train_dataset = TestDataset(train_folds, transform=test_transform_tta, valid_test=True)\n",
    "pseudo_train_loader = DataLoader(\n",
    "    pseudo_train_dataset, batch_size=params['batch_size'], shuffle=False, num_workers=4, pin_memory=True,\n",
    ")  \n",
    "pred_train_pseudo = pseudo_predict(pseudo_train_loader, model, params, valid_test=True)\n",
    "pseudo = pd.DataFrame(pred_train_pseudo)\n",
    "pseudo.to_csv(f'{root}/train_{params[\"fold\"]}_pseudo.csv' ,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "classification (1) (1).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "traffic_monitor",
   "language": "python",
   "name": "traffic_monitor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
