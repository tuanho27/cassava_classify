{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cassava classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "edyDFX_Ih5-j"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from urllib.request import urlretrieve\n",
    "import pandas as pd\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import resample\n",
    "cudnn.benchmark = True\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n",
    "import timm\n",
    "from timm.loss import LabelSmoothingCrossEntropy, SoftTargetCrossEntropy, JsdCrossEntropy\n",
    "from utils import Mixup, RandAugment, RAdam\n",
    "from PIL import Image\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(SEED):\n",
    "    random.seed(SEED)\n",
    "    os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] =\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the root directory dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "waXTuqeVh5-q"
   },
   "outputs": [],
   "source": [
    "root = os.path.join(os.environ[\"HOME\"], \"Workspace/datasets/taiyoyuden/cassava\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split files from the dataset into the train and validation sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some files in the dataset are broken, so we will use only those image files that OpenCV could load correctly. We will use 20000 images for training, 4936 images for validation, and 10 images for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QgLoujrNHY5Z"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test_images',\n",
       " 'label_num_to_disease_map.json',\n",
       " 'external',\n",
       " 'sample_submission.csv',\n",
       " 'train_images',\n",
       " 'label_num_to_disease_map.json.save',\n",
       " 'train.csv']"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to visualize images and their labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a function that will take a list of images' file paths and their labels and visualize them in a grid. Correct labels are colored green, and incorrectly predicted labels are colored red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000015157.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000201771.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100042118.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000723321.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000812911.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         image_id  label\n",
       "0  1000015157.jpg      0\n",
       "1  1000201771.jpg      3\n",
       "2   100042118.jpg      1\n",
       "3  1000723321.jpg      1\n",
       "4  1000812911.jpg      3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2216849948.jpg</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000015157.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000201771.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         image_id  label\n",
       "0  2216849948.jpg      4\n",
       "1  1000015157.jpg      0\n",
       "2  1000201771.jpg      3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cassava Bacterial Blight (CBB)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cassava Brown Streak Disease (CBSD)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cassava Green Mottle (CGM)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cassava Mosaic Disease (CMD)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Healthy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     0\n",
       "0       Cassava Bacterial Blight (CBB)\n",
       "1  Cassava Brown Streak Disease (CBSD)\n",
       "2           Cassava Green Mottle (CGM)\n",
       "3         Cassava Mosaic Disease (CMD)\n",
       "4                              Healthy"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train = pd.read_csv(f'{root}/train.csv')\n",
    "train_external = pd.read_csv(f'{root}/external/train_external.csv')\n",
    "test_external = pd.read_csv(f'{root}/external/test_external.csv')\n",
    "test_external_pseudo = pd.read_csv(f'{root}/external/test_external_pseudo.csv')\n",
    "test = pd.read_csv(f'{root}/sample_submission.csv')\n",
    "label_map = pd.read_json(f'{root}/label_num_to_disease_map.json', \n",
    "                         orient='index')\n",
    "display(train.head())\n",
    "display(test.head())\n",
    "display(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7320,  910, 5440, 5241, 5784, 6315,  516, 4476, 5628, 8372, 1735,\n",
       "        819, 6999, 2483, 5361, 5101, 6470, 1234, 4605, 3435, 6446, 8716,\n",
       "       9324, 2608, 7899, 2097, 2797, 9217,  239, 2784, 3055, 4708, 1949,\n",
       "       7784, 1317, 1578, 3606, 3940, 8888, 5443, 8842, 8483, 7563, 2662,\n",
       "       7091, 9605, 6285, 5536, 7149, 9720])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map.iloc[1].values\n",
    "np.random.randint(50, 10000, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ho-BaziAHY5q"
   },
   "outputs": [],
   "source": [
    "def visualize_input_image_grid(filepaths, image_name, labels, cols=4):\n",
    "    rows = 5\n",
    "    figure, ax = plt.subplots(nrows=rows, ncols=cols, figsize=(25, 15))\n",
    "    for i, index in enumerate(np.random.randint(0, len(image_name), 20)):\n",
    "        name = image_name.iloc[index]['image_id']\n",
    "        label =  image_name.iloc[index]['label']\n",
    "        image = cv2.imread(f'{filepaths}/train_images/{name}')\n",
    "        if (i == 0): \n",
    "            print(image.shape)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        ax.ravel()[i].imshow(image)\n",
    "        ax.ravel()[i].set_title(label, color='GREEN')\n",
    "        ax.ravel()[i].set_axis_off()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the parameters of the whole process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_name = [\"resnest26d\",\"resnest50d\",\"tf_efficientnet_b3_ns\", \"skresnet34\" ,\"cspresnet50\", \"vit_base_patch16_384\"]\n",
    "WEIGHTS = [\n",
    "    \"weights/resnest26d_fold0_best_epoch_28_final_1st.pth\", #0\n",
    "    \"weights/resnest26d_fold0_best_epoch_13_final_mixup.pth\",\n",
    "    \"weights/resnest26d_fold1_best_epoch_7_finale_ex_hnm.pth\",\n",
    "    \"weights/resnest26d_fold2_best_epoch_3_final_hnm.pth\",\n",
    "    \"weights/resnest26d_fold4_best_epoch_29_1st.pth\",\n",
    "    \"weights/resnest26d_fold4_best_epoch_26_mix.pth\", #5\n",
    "    \"weights/resnest26d_fold4_best_epoch_12_cutmix.pth\", \n",
    "    \"weights/resnest26d_fold4_best_epoch_3_external.pth\",\n",
    "    \"weights/resnest26d_fold4_best_epoch_21_final_512.pth\",\n",
    "    \"weights/tf_efficientnet_b3_ns_fold1_best_epoch_19_external.pth\",\n",
    "    \"weights/tf_efficientnet_b3_ns_fold1_best_epoch_26_512.pth\", #10\n",
    "    \"weights/tf_efficientnet_b3_ns_fold1_best_epoch_1_final_512.pth\", \n",
    "    \"weights/resnest50d_fold1_best_epoch_95_final_1st.pth\",\n",
    "    \"weights/resnest50d_fold1_best_epoch_9_final_512.pth\",\n",
    "    \"weights/resnest50d_fold1_best_epoch_82.pth\", # test balance data (0.86)\n",
    "    \"weights/cspresnet50_fold1_best_epoch_24_final.pth\", #15\n",
    "        \n",
    "]\n",
    "# Single model\n",
    "model_index = 1\n",
    "ckpt_index = 12\n",
    "\n",
    "# kfold\n",
    "fold_model_index = 1\n",
    "fold_ckpt_index = [12,13,14]\n",
    "fold_ckpt_weight = [1,1,1]\n",
    "\n",
    "# Ensemble\n",
    "ensemble_models_name = [\"resnest26d\" ,\"tf_efficientnet_b3_ns\", \"resnest50d\", \"cspresnet50\"]\n",
    "ensemble_ckpt_index = [2, 11, 12, 15]\n",
    "ensemble_ckpt_weight = [1, 1, 1, 1]\n",
    "\n",
    "params = {\n",
    "    \"visualize\": False,\n",
    "    \"fold\": 1,\n",
    "    \"train_external\": True,\n",
    "    \"test_external\": True,\n",
    "    \"load_pretrained\": True,\n",
    "    \"resume\": False,\n",
    "    \"image_size\": 512,\n",
    "    \"num_classes\": 5,\n",
    "    \"model\": models_name[model_index],\n",
    "    \"device\": \"cuda\",\n",
    "    \"lr\": 5e-5,\n",
    "    \"lr_min\":1e-6,\n",
    "    \"batch_size\": 8,\n",
    "    \"num_workers\": 8,\n",
    "    \"epochs\": 100,\n",
    "    \"gradient_accumulation_steps\": 8,\n",
    "    \"drop_block\": 0.3,\n",
    "    \"drop_rate\": 0.3,\n",
    "    \"mix_up\": True,\n",
    "    \"cutmix\":True,\n",
    "    \"rand_aug\": False,\n",
    "    \"local_rank\":0,\n",
    "    \"distributed\": False,\n",
    "    \"hard_negative_sample\": False,\n",
    "    \"tta\": True,\n",
    "    \"crops_tta\":False,\n",
    "    \"train_phase\":False,\n",
    "    \"balance_data\":False,\n",
    "    \"kfold_pred\":True,\n",
    "    \"ensemble\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define dataset with KFolds strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = train.copy()\n",
    "Fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "for n, (train_index, val_index) in enumerate(Fold.split(folds, folds['label'])):\n",
    "    folds.loc[val_index, 'fold'] = int(n)\n",
    "folds['fold'] = folds['fold'].astype(int)\n",
    "###\n",
    "fold = params[\"fold\"]\n",
    "train_idx = folds[folds['fold'] != fold].index\n",
    "val_idx = folds[folds['fold'] == fold].index\n",
    "\n",
    "train_folds = folds.loc[train_idx].reset_index(drop=True)\n",
    "val_folds = folds.loc[val_idx].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, df, transform=None, mosaic_mix = False):\n",
    "        self.df = df\n",
    "        self.file_names = df['image_id'].values\n",
    "        self.labels = df['label'].values\n",
    "        self.transform = transform\n",
    "        self.mosaic_mix = mosaic_mix\n",
    "        self.rand_aug_fn = RandAugment()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_name = self.file_names[idx]\n",
    "        file_path = f'{root}/train_images/{file_name}'\n",
    "        image = cv2.imread(file_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        label = torch.tensor(self.labels[idx]).long()\n",
    "        if params[\"rand_aug\"]:\n",
    "            image = np.array(self.rand_aug_fn(Image.fromarray(image)))\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "\n",
    "        return image, label, file_name\n",
    "    \n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, df, transform=None, valid_test=False, fcrops=False):\n",
    "        self.df = df\n",
    "        self.file_names = df['image_id'].values\n",
    "        self.transform = transform\n",
    "        self.valid_test = valid_test\n",
    "        self.fcrops = fcrops\n",
    "        if self.valid_test:\n",
    "            self.labels = df['label'].values  \n",
    "        else:\n",
    "            assert ValueError(\"Test data does not have annotation, plz check!\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_name = self.file_names[idx]\n",
    "        if self.valid_test:\n",
    "            file_path = f'{root}/train_images/{file_name}'\n",
    "            #file_path = f'{root}/external/extraimages/{file_name}'\n",
    "        else:\n",
    "            file_path = f'{root}/test_images/{file_name}'\n",
    "        image = cv2.imread(file_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        if isinstance(self.transform, list):\n",
    "            outputs = {'images':[],\n",
    "                       'labels':[],\n",
    "                       'image_ids':[]}\n",
    "            if self.fcrops:\n",
    "                for trans in self.transform:\n",
    "                    image_aug = Image.open(file_path)\n",
    "                    image_aug = trans(image_aug)\n",
    "                    outputs[\"images\"].append(image_aug)\n",
    "                    del image_aug\n",
    "            else:\n",
    "                for trans in self.transform:\n",
    "                    augmented = trans(image=image)\n",
    "                    image_aug = augmented['image']\n",
    "                    outputs[\"images\"].append(image_aug)\n",
    "                    del image_aug\n",
    "\n",
    "            if self.valid_test:\n",
    "                label = torch.tensor(self.labels[idx]).long()\n",
    "                outputs['labels'] = len(self.transform)*[label]\n",
    "                outputs['image_ids'].append(file_name)\n",
    "                \n",
    "            else:\n",
    "                outputs['labels'] = len(self.transform)*[-1]\n",
    "                \n",
    "            return outputs\n",
    "        else:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image'] \n",
    "        return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Albumentations to define transformation functions for the train and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mm_NP-c4h5-_"
   },
   "outputs": [],
   "source": [
    "train_transform = A.Compose(\n",
    "    [\n",
    "        A.RandomResizedCrop(height=params[\"image_size\"], width=params[\"image_size\"], p=1),\n",
    "        A.OneOf([\n",
    "            A.RandomRotate90(p=0.5),\n",
    "            A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),], p=1.\n",
    "        ),\n",
    "#         A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.IAAAffine(rotate=0.2, shear=0.2,p=0.5),\n",
    "        A.CoarseDropout(max_holes=20, max_height=int(params[\"image_size\"]/15), max_width=int(params[\"image_size\"]/15), p=0.5),\n",
    "#         A.IAAAdditiveGaussianNoise(p=1.),\n",
    "        A.MedianBlur(p=0.5),\n",
    "        A.Equalize(p=0.2),\n",
    "        A.GridDistortion(p=0.2),\n",
    "#         A.RandomGridShuffle(p=0.5),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "val_transform = A.Compose(\n",
    "    [\n",
    "        A.CenterCrop(height=params[\"image_size\"], width=params[\"image_size\"], p=1),\n",
    "        A.Resize(params[\"image_size\"],params[\"image_size\"]),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "if params[\"cutmix\"]:\n",
    "    mixup_fn = Mixup(mixup_alpha=1., cutmix_alpha=1., label_smoothing=0.1, num_classes=params[\"num_classes\"])\n",
    "else:\n",
    "    mixup_fn = Mixup(mixup_alpha=1., label_smoothing=0.1, num_classes=params[\"num_classes\"])\n",
    "\n",
    "val_dataset = TrainDataset(val_folds, transform=val_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_tta0 = A.Compose(\n",
    "    [\n",
    "     A.CenterCrop(height=params[\"image_size\"], width=params[\"image_size\"], p=1),    \n",
    "     A.Resize(height=params[\"image_size\"], width=params[\"image_size\"], p=1),\n",
    "     A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),   \n",
    "     ToTensorV2()\n",
    "    ]\n",
    ")\n",
    "\n",
    "transform_tta1 = A.Compose(\n",
    "    [\n",
    "     A.CenterCrop(height=params[\"image_size\"], width=params[\"image_size\"], p=1),\n",
    "     A.Resize(height=params[\"image_size\"], width=params[\"image_size\"], p=1),\n",
    "     A.HorizontalFlip(p=1.),\n",
    "     A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),   \n",
    "     ToTensorV2()\n",
    "    ]\n",
    ")\n",
    "transform_tta2 = A.Compose(\n",
    "    [\n",
    "     A.CenterCrop(height=params[\"image_size\"], width=params[\"image_size\"], p=1),\n",
    "     A.Resize(height=params[\"image_size\"], width=params[\"image_size\"], p=1),\n",
    "     A.VerticalFlip(p=1.),\n",
    "     A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "     ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "transform_tta3 = A.Compose(\n",
    "    [\n",
    "     A.CenterCrop(height=params[\"image_size\"], width=params[\"image_size\"], p=1),\n",
    "     A.Resize(height=params[\"image_size\"], width=params[\"image_size\"], p=1),\n",
    "     A.RandomRotate90(p=1.),\n",
    "     A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),     \n",
    "     ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "transform_tta4 = A.Compose(\n",
    "    [\n",
    "     A.CenterCrop(height=params[\"image_size\"], width=params[\"image_size\"], p=1),\n",
    "     A.Resize(height=params[\"image_size\"], width=params[\"image_size\"], p=1),\n",
    "     A.RandomRotate90(p=1.),\n",
    "     A.RandomRotate90(p=1.),\n",
    "     A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),     \n",
    "     ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "##### Test TTA with Five Crops\n",
    "transform_crop_tta0 = T.Compose([\n",
    "                T.FiveCrop(params[\"image_size\"]),\n",
    "                T.Lambda(lambda crops: ([T.ToTensor()(crop) for crop in crops])),\n",
    "                T.Lambda(lambda norms: torch.stack([T.Normalize(mean=[0.5], std=[0.5])(norm) for norm in norms]))\n",
    "        ]\n",
    ")\n",
    "    \n",
    "transform_crop_tta1 = T.Compose([\n",
    "                T.FiveCrop(params[\"image_size\"]),\n",
    "                T.Lambda(lambda crops: ([T.ToTensor()(crop) for crop in crops])),\n",
    "                T.Lambda(lambda flips: ([T.RandomHorizontalFlip(p=1.)(flip) for flip in flips])),\n",
    "                T.Lambda(lambda norms: torch.stack([T.Normalize(mean=[0.5], std=[0.5])(norm) for norm in norms]))\n",
    "    ]\n",
    ")\n",
    "transform_crop_tta2 = T.Compose([\n",
    "                T.FiveCrop(params[\"image_size\"]),\n",
    "                T.Lambda(lambda crops: ([T.ToTensor()(crop) for crop in crops])),\n",
    "                T.Lambda(lambda flips: ([T.RandomVerticalFlip(p=1.)(flip) for flip in flips])),\n",
    "                T.Lambda(lambda norms: torch.stack([T.Normalize(mean=[0.5], std=[0.5])(norm) for norm in norms]))\n",
    "    ]\n",
    ")\n",
    "test_transform_tta = [transform_tta0, transform_tta1, transform_tta2, transform_tta3, transform_tta3, transform_tta4]\n",
    "test_transform_tta_crops = [transform_crop_tta0, transform_crop_tta1, transform_crop_tta2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also let's define a function that takes a dataset and visualizes different augmentations applied to the same image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bgTqE0eYSjDh"
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(output, target):\n",
    "#     return torch.true_divide((target == output).sum(dim=0), output.size(0)).item()\n",
    "    if params[\"mix_up\"]:\n",
    "        output = torch.argmax(torch.softmax(output, dim=1), dim=1)\n",
    "        return accuracy_score(output.cpu(), target.argmax(1).cpu())\n",
    "    \n",
    "    output = torch.softmax(output, dim=1)\n",
    "    return accuracy_score(output.argmax(1).cpu(), target.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zRhPJiCch5_D"
   },
   "outputs": [],
   "source": [
    "class MetricMonitor:\n",
    "    def __init__(self, float_precision=3):\n",
    "        self.float_precision = float_precision\n",
    "        self.reset()\n",
    "        self.curr_acc = 0.\n",
    "    def reset(self):\n",
    "        self.metrics = defaultdict(lambda: {\"val\": 0, \"count\": 0, \"avg\": 0})\n",
    "\n",
    "    def update(self, metric_name, val):\n",
    "        metric = self.metrics[metric_name]\n",
    "\n",
    "        metric[\"val\"] += val\n",
    "        metric[\"count\"] += 1\n",
    "        metric[\"avg\"] = metric[\"val\"] / metric[\"count\"]\n",
    "        self.curr_acc = metric[\"avg\"]\n",
    "    def __str__(self):\n",
    "        return \" | \".join(\n",
    "            [\n",
    "                \"{metric_name}: {avg:.{float_precision}f}\".format(\n",
    "                    metric_name=metric_name, avg=metric[\"avg\"], float_precision=self.float_precision\n",
    "                )\n",
    "                for (metric_name, metric) in self.metrics.items()\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights/resnest50d_fold1_best_epoch_95_final_1st.pth resnest50d\n",
      "Load pretrained model: resnest50d  0.8967\n"
     ]
    }
   ],
   "source": [
    "def declare_model(name, index=None):\n",
    "    model = timm.create_model(name,\n",
    "            pretrained=False,\n",
    "            num_classes=params[\"num_classes\"],\n",
    "            drop_rate=params[\"drop_rate\"])\n",
    "    model = model.to(params[\"device\"])\n",
    "    \n",
    "    if params[\"distributed\"]:\n",
    "        assert ValueError(\"No need to implement in a single machine\")\n",
    "    else:\n",
    "        model = torch.nn.DataParallel(model) \n",
    "        \n",
    "    if params[\"load_pretrained\"]:\n",
    "        if index == None: \n",
    "            return model\n",
    "        state_dict = torch.load(WEIGHTS[index])\n",
    "        print(f\"Load pretrained model: {name} \",state_dict[\"preds\"])\n",
    "        model.load_state_dict(state_dict[\"model\"])\n",
    "        best_acc = state_dict[\"preds\"]   \n",
    "    return model\n",
    "\n",
    "model = declare_model(params[\"model\"], ckpt_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_features = model.fc.in_features\n",
    "# model.fc = nn.Linear(n_features, params['num_classes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create all required objects and functions for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W7HipbJ8h5_I"
   },
   "outputs": [],
   "source": [
    "val_criterion = nn.CrossEntropyLoss().to(params[\"device\"])\n",
    "# criterion = nn.CrossEntropyLoss().to(params[\"device\"])\n",
    "criterion = LabelSmoothingCrossEntropy().to(params[\"device\"])\n",
    "if params[\"mix_up\"]:\n",
    "    criterion = SoftTargetCrossEntropy().to(params[\"device\"])         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------\n",
    "## Make validation prediction w/TTA on validation and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "if params[\"tta\"]:\n",
    "    val_pred_dataset = TestDataset(val_folds, transform=test_transform_tta, valid_test=True)\n",
    "    test_pred_dataset = TestDataset(test, transform=test_transform_tta)\n",
    "else:\n",
    "    val_pred_dataset = TestDataset(val_folds, transform=val_transform, valid_test=True)\n",
    "    test_pred_dataset = TestDataset(test, transform=val_transform)\n",
    "\n",
    "val_pred_dataset_crops = TestDataset(val_folds, transform=test_transform_tta_crops,valid_test=True, fcrops=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred_loader = DataLoader(\n",
    "    val_pred_dataset, batch_size=params['batch_size'], shuffle=False, num_workers=2, pin_memory=True,\n",
    ")\n",
    "val_pred_loader_crop = DataLoader(\n",
    "    val_pred_dataset_crops, batch_size=params['batch_size'], shuffle=False, num_workers=2, pin_memory=True,\n",
    ")\n",
    "test_pred_loader = DataLoader(\n",
    "    test_pred_dataset, batch_size=params['batch_size'], shuffle=False, num_workers=2, pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_tta(data, pred):     \n",
    "    unorm = UnNormalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "    if params[\"tta\"]:\n",
    "        figure, ax = plt.subplots(nrows=1, ncols=num_tta, figsize=(12, 6))\n",
    "        for i, image in enumerate(data[\"images\"]):\n",
    "            image = (unorm(image[0]).cpu().numpy()*255).astype(int)\n",
    "            ax.ravel()[i].imshow(image.transpose(2,1,0))\n",
    "            ax.ravel()[i].set_title(str(pred), color='GREEN')\n",
    "            ax.ravel()[i].set_axis_off()\n",
    "        plt.tight_layout()\n",
    "        plt.show() \n",
    "    else:\n",
    "        image = (unorm(data).cpu().numpy()*255).astype(int)\n",
    "        imgplot = plt.imshow(image[0].transpose(2,1,0))\n",
    "        imgplot = plt.title(str(pred), color='GREEN')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, model, criterion, epoch, params, fold, best_acc):\n",
    "    metric_monitor = MetricMonitor()\n",
    "    model.eval()\n",
    "    stream = tqdm(val_loader)\n",
    "    with torch.no_grad():\n",
    "        for i, (images, target, _) in enumerate(stream, start=1):\n",
    "            images = images.to(params[\"device\"], non_blocking=True)\n",
    "            target = target.to(params[\"device\"], non_blocking=True)#.view(-1,params['batch_size'])\n",
    "            output = model(images)\n",
    "            loss = val_criterion(output, target)\n",
    "            output = torch.softmax(output, dim = 1)\n",
    "            \n",
    "            accuracy = accuracy_score(output.argmax(1).cpu(), target.cpu())\n",
    "\n",
    "            stream.set_description(\n",
    "                \"Epoch: {epoch}. Validation. {metric_monitor}\".format(epoch=epoch, metric_monitor=metric_monitor)\n",
    "            )           \n",
    "            metric_monitor.update(\"Loss\", loss.item())\n",
    "            metric_monitor.update(\"Accuracy\", accuracy)\n",
    "        #to save weight\n",
    "        if (metric_monitor.curr_acc > best_acc): # or epoch == params[\"epochs\"]:\n",
    "            print(f\"Save best weight at acc {round(metric_monitor.curr_acc,4)}, epoch: {epoch}\")\n",
    "            torch.save({'model': model.state_dict(), \n",
    "                'preds': round(metric_monitor.curr_acc,4)},\n",
    "                 f'weights/{params[\"model\"]}_fold{fold}_best_epoch_{epoch}.pth')\n",
    "\n",
    "            best_acc = metric_monitor.curr_acc\n",
    "    return best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TTA Validation. Accuracy: 0.902: 100%|██████████| 535/535 [03:26<00:00,  2.59it/s]\n"
     ]
    }
   ],
   "source": [
    "num_tta = len(test_transform_tta)\n",
    "incorrect_pred_list = {'image_id':[],\n",
    "                       'label':[],\n",
    "                       'pred':[]}\n",
    "def tta_validate(loader, model, params, valid_test=False):\n",
    "    metric_monitor = MetricMonitor()\n",
    "    model.eval()\n",
    "    stream = tqdm(loader)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(stream, start=1):\n",
    "            if params[\"visualize\"]:\n",
    "                visualize_tta(data, pred)\n",
    "            tta_output = []   \n",
    "            for i, image in enumerate(data[\"images\"]):\n",
    "                tta_output.append(model(image))\n",
    "            tta_output = torch.stack(tta_output, dim=0).mean(dim=0)\n",
    "            output = torch.softmax(tta_output, dim=1)\n",
    "            for i in  range(len(data[\"labels\"][0])):\n",
    "                if data[\"labels\"][0][i] != output.argmax(1).cpu()[i]:\n",
    "                    incorrect_pred_list['label'].append(data[\"labels\"][0][i])\n",
    "                    incorrect_pred_list['pred'].append(output.argmax(1).cpu()[i])\n",
    "                    incorrect_pred_list['label'].append(data[\"image_ids\"][0][i])                        \n",
    "            accuracy = accuracy_score(output.argmax(1).cpu(), data[\"labels\"][0].cpu())\n",
    "                \n",
    "            metric_monitor.update(\"Accuracy\", accuracy)            \n",
    "            stream.set_description(\n",
    "                \"TTA Validation. {metric_monitor}\".format(metric_monitor=metric_monitor)\n",
    "            )   \n",
    "num_tta_crop = len(test_transform_tta_crops)\n",
    "if params[\"tta\"]:\n",
    "    tta_validate(val_pred_loader, model, params, valid_test=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tta_crops_validate(loader, model, params, valid_test=False):\n",
    "    metric_monitor = MetricMonitor()\n",
    "    model.eval()\n",
    "    stream = tqdm(loader)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(stream, start=1):\n",
    "            if params[\"visualize\"]:\n",
    "                visualize_tta(data, pred)\n",
    "            tta_output = []   \n",
    "            for i, image in enumerate(data[\"images\"]):\n",
    "                batch, ncrops, c, h, w = image.size()\n",
    "                image = image.view(-1, c, h, w)\n",
    "                output = model(image)\n",
    "                tta_output.append(output.view(batch, ncrops, -1).mean(1))\n",
    "            tta_output = torch.stack(tta_output, dim=0).mean(dim=0)\n",
    "            output = torch.softmax(tta_output, dim=1)\n",
    "            accuracy = accuracy_score(output.argmax(1).cpu(), data[\"labels\"][0].cpu())\n",
    "                \n",
    "            metric_monitor.update(\"Accuracy\", accuracy)            \n",
    "            stream.set_description(\n",
    "                \"Crops TTA Validation. {metric_monitor}\".format(metric_monitor=metric_monitor)\n",
    "            )   \n",
    "## Re test the validation accuracy with TTA\n",
    "if params[\"crops_tta\"]:\n",
    "    tta_crops_validate(val_pred_loader_crop, model, params, valid_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/535 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pretrained model: weights/resnest50d_fold1_best_epoch_95_final_1st.pth with acc: 0.8967\n",
      "Load pretrained model: weights/resnest50d_fold1_best_epoch_9_final_512.pth with acc: 0.8998\n",
      "Load pretrained model: weights/resnest50d_fold1_best_epoch_82.pth with acc: 0.8666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Folds TTA Validation. Accuracy: 0.892: 100%|██████████| 535/535 [10:44<00:00,  1.20s/it]\n"
     ]
    }
   ],
   "source": [
    "# CV w/wo fold predict on validation set\n",
    "def kfold_tta_validate(loader, params, valid_test=False):\n",
    "    model = declare_model(models_name[fold_model_index])\n",
    "    metric_monitor = MetricMonitor()\n",
    "    model.eval()\n",
    "    stream = tqdm(loader)\n",
    "    models = []\n",
    "    for ckpt, weight in zip(fold_ckpt_index, fold_ckpt_weight):\n",
    "        state_dict = torch.load(WEIGHTS[ckpt])\n",
    "        fold_acc = state_dict[\"preds\"]\n",
    "        print(f\"Load pretrained model: {WEIGHTS[ckpt]} with acc: {fold_acc}\")\n",
    "        model.load_state_dict(state_dict[\"model\"])\n",
    "        models.append(model)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(stream, start=1):\n",
    "            if params[\"visualize\"]:\n",
    "                visualize_tta(data, pred)\n",
    "            tta_output = []   \n",
    "            for i, image in enumerate(data[\"images\"]):\n",
    "                kfout = [model(image) for model in models]\n",
    "                tta_output.append(torch.stack(kfout, dim=0).mean(dim=0))\n",
    "            tta_output = torch.stack(tta_output, dim=0).mean(dim=0)\n",
    "            output = torch.softmax(tta_output, dim=1)\n",
    "            accuracy = accuracy_score(output.argmax(1).cpu(), data[\"labels\"][0].cpu())\n",
    "                \n",
    "            metric_monitor.update(\"Accuracy\", accuracy)            \n",
    "            stream.set_description(\n",
    "                \"Folds TTA Validation. {metric_monitor}\".format(metric_monitor=metric_monitor)\n",
    "            ) \n",
    "if params[\"tta\"] and params[\"fold\"]:\n",
    "    kfold_tta_validate(val_pred_loader, params, valid_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble below models: ['resnest26d', 'tf_efficientnet_b3_ns', 'resnest50d', 'cspresnet50']\n",
      "2\n",
      "weights/resnest26d_fold1_best_epoch_7_finale_ex_hnm.pth resnest26d\n",
      "Load pretrained model: resnest26d  0.8997\n",
      "11\n",
      "weights/tf_efficientnet_b3_ns_fold1_best_epoch_1_final_512.pth tf_efficientnet_b3_ns\n",
      "Load pretrained model: tf_efficientnet_b3_ns  0.897\n",
      "12\n",
      "weights/resnest50d_fold1_best_epoch_95_final_1st.pth resnest50d\n",
      "Load pretrained model: resnest50d  0.8967\n",
      "15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/535 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights/cspresnet50_fold1_best_epoch_24_final.pth cspresnet50\n",
      "Load pretrained model: cspresnet50  0.8883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ensemble Validation. Accuracy: 0.896: 100%|██████████| 535/535 [11:04<00:00,  1.24s/it]\n"
     ]
    }
   ],
   "source": [
    "# Ensemble CV w/ fold predict on validation set\n",
    "def ensemble_w_kfold_tta_validate(loader, models, params, valid_test=False):\n",
    "    metric_monitor = MetricMonitor()\n",
    "    stream = tqdm(loader)\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(stream, start=1):\n",
    "            if params[\"visualize\"]:\n",
    "                visualize_tta(data, pred)\n",
    "            tta_output = []   \n",
    "            for i, image in enumerate(data[\"images\"]):\n",
    "                kfout = [model(image) for model in models]\n",
    "                tta_output.append(torch.stack(kfout, dim=0).mean(dim=0))\n",
    "            tta_output = torch.stack(tta_output, dim=0).mean(dim=0)\n",
    "            output = torch.softmax(tta_output, dim=1)\n",
    "            accuracy = accuracy_score(output.argmax(1).cpu(), data[\"labels\"][0].cpu())\n",
    "                \n",
    "            metric_monitor.update(\"Accuracy\", accuracy)            \n",
    "            stream.set_description(\n",
    "                \"Ensemble Validation. {metric_monitor}\".format(metric_monitor=metric_monitor)\n",
    "            ) \n",
    "if params[\"ensemble\"] and params[\"tta\"] and params[\"fold\"]:\n",
    "    models = []\n",
    "    print(f\"Ensemble below models: {ensemble_models_name}\")\n",
    "    for name, ckpt, weight in zip(ensemble_models_name, ensemble_ckpt_index, ensemble_ckpt_weight):\n",
    "        print(ckpt)\n",
    "        m = declare_model(name, ckpt)  \n",
    "        models.append(m)\n",
    "        del m\n",
    "    ensemble_w_kfold_tta_validate(val_pred_loader, models, params, valid_test=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------\n",
    "## Make Test prediction w/wo Kfold check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tta = len(test_transform_tta)\n",
    "def predict(loader, model, params, valid_test=False):\n",
    "    outputs = []\n",
    "    preds = []\n",
    "    model.eval()\n",
    "    stream = tqdm(loader)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(stream, start=1):\n",
    "            if params[\"tta\"]:\n",
    "                ## TTA output\n",
    "                tta_output = []   \n",
    "                for i, image in enumerate(data[\"images\"]):\n",
    "                    tta_output.append(model(image))\n",
    "                tta_output = torch.stack(tta_output, dim=0).mean(dim=0)\n",
    "                output = torch.softmax(tta_output, dim=1)\n",
    "                outputs.append(output)\n",
    "            else:\n",
    "                data = data.to(params[\"device\"], non_blocking=True)\n",
    "                output = model(data)\n",
    "                output = torch.softmax(output, dim = 1)\n",
    "                outputs.append(output)\n",
    "                \n",
    "            pred = torch.argmax(output, dim=1).cpu().numpy()\n",
    "            preds.extend(pred)\n",
    "            ## For visualize the TTA \n",
    "            if params[\"visualize\"]:\n",
    "                visualize_tta(data, pred)\n",
    "    if params[\"kfold_pred\"]:\n",
    "        return torch.stack(outputs, dim=0)\n",
    "    else:\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pretrained model: weights/resnest50d_fold1_best_epoch_9_final_512.pth with acc: 0.8967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.33it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pretrained model: weights/resnest50d_fold1_best_epoch_78.pth with acc: 0.8998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.29it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pretrained model: weights/cspresnet50_fold1_best_epoch_24_final.pth with acc: 0.8666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.32it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2216849948.jpg</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000015157.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000201771.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         image_id  label\n",
       "0  2216849948.jpg      4\n",
       "1  1000015157.jpg      2\n",
       "2  1000201771.jpg      3"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CV w/wo fold predict\n",
    "if params[\"kfold_pred\"]:\n",
    "    fold_preds = []\n",
    "    for ckpt, weight in zip(fold_ckpt_index, fold_ckpt_weight):\n",
    "        state_dict = torch.load(WEIGHTS[ckpt])\n",
    "        fold_acc = state_dict[\"preds\"]\n",
    "        print(f\"Load pretrained model: {weights[ckpt]} with acc: {fold_acc}\")\n",
    "        model.load_state_dict(state_dict[\"model\"])\n",
    "        best_acc = state_dict[\"preds\"]\n",
    "        out = predict(test_pred_loader, model, params, valid_test=True).mul(weight)\n",
    "        fold_preds.append(out)\n",
    "        del(out)\n",
    "    final_out_fold = torch.softmax(torch.stack(fold_preds).mean(dim=0), dim=2).view(-1,5).argmax(1)\n",
    "    final_out = final_out_fold.cpu().numpy()\n",
    "else:\n",
    "    final_out = predict(test_pred_loader, model, params, valid_test=True)\n",
    "    \n",
    "test['label'] = final_out \n",
    "test.to_csv('submission.csv', index=False)\n",
    "test.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble below models: ['resnest26d', 'tf_efficientnet_b3_ns', 'resnest50d', 'cspresnet50']\n",
      "weights/resnest26d_fold1_best_epoch_7_finale_ex_hnm.pth resnest26d\n",
      "Load pretrained model: resnest26d  0.8997\n",
      "weights/tf_efficientnet_b3_ns_fold1_best_epoch_1_final_512.pth tf_efficientnet_b3_ns\n",
      "Load pretrained model: tf_efficientnet_b3_ns  0.897\n",
      "weights/resnest50d_fold1_best_epoch_95_final_1st.pth resnest50d\n",
      "Load pretrained model: resnest50d  0.8967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights/cspresnet50_fold1_best_epoch_24_final.pth cspresnet50\n",
      "Load pretrained model: cspresnet50  0.8883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.28it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2216849948.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000015157.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000201771.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         image_id  label\n",
       "0  2216849948.jpg      2\n",
       "1  1000015157.jpg      1\n",
       "2  1000201771.jpg      3"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CV w/wo fold predict\n",
    "def ensemble_w_kfold_tta_pred(loader, models, params, valid_test=False):\n",
    "    metric_monitor = MetricMonitor()\n",
    "    stream = tqdm(loader)\n",
    "    outputs = []\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(stream, start=1):\n",
    "            if params[\"visualize\"]:\n",
    "                visualize_tta(data, pred)\n",
    "            tta_output = []   \n",
    "            for i, image in enumerate(data[\"images\"]):\n",
    "                kfout = [model(image) for model in models]\n",
    "                tta_output.append(torch.stack(kfout, dim=0).mean(dim=0))\n",
    "            tta_output = torch.stack(tta_output, dim=0).mean(dim=0)\n",
    "            output = torch.softmax(tta_output, dim=1)\n",
    "            outputs.append(output.argmax(1).cpu().numpy())\n",
    "    return outputs\n",
    "\n",
    "if params[\"ensemble\"]:\n",
    "    models = []\n",
    "    print(f\"Ensemble below models: {ensemble_models_name}\")\n",
    "    for name, ckpt, weight in zip(ensemble_models_name, ensemble_ckpt_index, ensemble_ckpt_weight):\n",
    "        m = declare_model(name, ckpt)  \n",
    "        models.append(m)\n",
    "        del m\n",
    "    final_out_ensemble = ensemble_w_kfold_tta_pred(test_pred_loader, models, params, valid_test=True)[0]\n",
    "    \n",
    "test['label'] = final_out_ensemble \n",
    "test.to_csv('submission.csv', index=False)\n",
    "test.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([2, 1, 3])]"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_out_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params[\"tta\"] = True\n",
    "create_pseudo = False\n",
    "num_tta = len(test_transform_tta)\n",
    "def pseudo_predict(loader, model, params, valid_test=False):\n",
    "    preds = {'image_id':[],\n",
    "             'label': []}\n",
    "    model.eval()\n",
    "    stream = tqdm(loader)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(stream, start=1):\n",
    "            ## TTA output\n",
    "            tta_output = []   \n",
    "            for i, image in enumerate(data[\"images\"]):\n",
    "                tta_output.append(model(image))\n",
    "            tta_output = torch.stack(tta_output, dim=0).mean(dim=0)\n",
    "            output = torch.softmax(tta_output, dim=1)\n",
    "            pred = torch.argmax(output, dim=1).cpu().numpy()\n",
    "            for j,p in enumerate(pred):\n",
    "                if output[j][p] > 0.7:\n",
    "                    preds['image_id'].append(data[\"image_ids\"][0][j])\n",
    "                    preds['label'].append(p)\n",
    "    return preds\n",
    "if create_pseudo:\n",
    "    ## add external data\n",
    "    test_external_dataset = TestDataset(test_external, transform=test_transform_tta, valid_test=True)\n",
    "    test_external_loader = DataLoader(\n",
    "        test_external_dataset, batch_size=params['batch_size'], shuffle=False, num_workers=4, pin_memory=True,\n",
    "    )  \n",
    "    pred_test_external = pseudo_predict(test_external_loader, model, params, valid_test=True)\n",
    "    pseudo = pd.DataFrame(pred_test_external)\n",
    "    pseudo.to_csv(f'{root}/external/test_external_pseudo.csv' ,index=False)\n",
    "    visualize_class_dis(pseudo, 'external pseudo data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "classification (1) (1).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "traffic_monitor",
   "language": "python",
   "name": "traffic_monitor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
